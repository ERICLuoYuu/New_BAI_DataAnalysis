<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/New_BAI_DataAnalysis/assets/css/just-the-docs-default.css"> <link rel="stylesheet" href="/New_BAI_DataAnalysis/assets/css/just-the-docs-head-nav.css" id="jtd-head-nav-stylesheet"> <style id="jtd-nav-activation"> .site-nav > ul.nav-list:first-child > li:not(:nth-child(5)) > a, .site-nav > ul.nav-list:first-child > li > ul > li a { background-image: none; } .site-nav > ul.nav-list:not(:first-child) a, .site-nav li.external a { background-image: none; } .site-nav > ul.nav-list:first-child > li:nth-child(5) > a { font-weight: 600; text-decoration: none; }.site-nav > ul.nav-list:first-child > li:nth-child(5) > button svg { transform: rotate(-90deg); }.site-nav > ul.nav-list:first-child > li.nav-list-item:nth-child(5) > ul.nav-list { display: block; } </style> <script src="/New_BAI_DataAnalysis/assets/js/vendor/lunr.min.js"></script> <script src="/New_BAI_DataAnalysis/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>REGRESSION | Data Analysis for Ecologist</title> <meta name="generator" content="Jekyll v3.10.0" /> <meta property="og:title" content="REGRESSION" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="A tutorial for starters learning how to use python to munipulate climate data!" /> <meta property="og:description" content="A tutorial for starters learning how to use python to munipulate climate data!" /> <link rel="canonical" href="https://ericluoyuu.github.io/New_BAI_DataAnalysis/python_3_regression.html" /> <meta property="og:url" content="https://ericluoyuu.github.io/New_BAI_DataAnalysis/python_3_regression.html" /> <meta property="og:site_name" content="Data Analysis for Ecologist" /> <meta property="og:type" content="website" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="REGRESSION" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","description":"A tutorial for starters learning how to use python to munipulate climate data!","headline":"REGRESSION","url":"https://ericluoyuu.github.io/New_BAI_DataAnalysis/python_3_regression.html"}</script> <!-- End Jekyll SEO tag --> </head> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']], displayMath: [ ['$$', '$$'] ], processEscapes: true, processEnvironments: true } }); </script> <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"> </script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"> </script> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/New_BAI_DataAnalysis/" class="site-title lh-tight"> Data Analysis for Ecologist </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </button> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><a href="/New_BAI_DataAnalysis/" class="nav-list-link">Home</a></li><li class="nav-list-item"><a href="/New_BAI_DataAnalysis/python_0_installation.html" class="nav-list-link">0. INSTALLATION</a></li><li class="nav-list-item"><a href="/New_BAI_DataAnalysis/python_1_basics.html" class="nav-list-link">1. BASICS OF PYTHON</a></li><li class="nav-list-item"><a href="/New_BAI_DataAnalysis/python_2_data_visualization.html" class="nav-list-link">2. DATA HANDLING AND VISUALIZATION</a></li><li class="nav-list-item"><a href="/New_BAI_DataAnalysis/python_3_regression.html" class="nav-list-link">3. REGRESSION</a></li><li class="nav-list-item"><a href="/New_BAI_DataAnalysis/python_4_flux_calculation.html" class="nav-list-link">4. FLUX CALCULATION</a></li></ul> </nav> <footer class="site-footer"> This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll. </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Data Analysis for Ecologist" aria-label="Search Data Analysis for Ecologist" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> </div> <div class="main-content-wrap"> <div id="main-content" class="main-content"> <main> <h1 id="regression"> <a href="#regression" class="anchor-heading" aria-labelledby="regression"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <strong>Regression</strong> </h1> <h2 id="table-of-contents"> <a href="#table-of-contents" class="anchor-heading" aria-labelledby="table-of-contents"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Table of Contents </h2> <ol> <li><a href="#1-whats-regression-all-about">What’s Regression All About?</a></li> <li><a href="#2-simple-linear-regression">Simple Linear Regression</a></li> <li><a href="#3-multiple-regression">Multiple Regression</a></li> <li><a href="#4-machine-learning-with-random-forests">Machine Learning with Random Forests</a></li> <li><a href="#5-gap-filling-in-time-series">Gap-filling in Time Series</a></li> </ol> <p>Welcome! This tutorial will walk you through regression analysis - one of the most useful tools you’ll encounter for making sense of ecological data. We’ll start from the basics and work our way up to more advanced machine learning methods. Don’t worry if statistics isn’t your strong suit. We’ll take it step by step, and by the end you should feel comfortable applying these techniques to your own data.</p><hr /> <h1 id="1-whats-regression-all-about"> <a href="#1-whats-regression-all-about" class="anchor-heading" aria-labelledby="1-whats-regression-all-about"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <strong>1. What’s Regression All About?</strong> </h1> <h2 id="the-basic-idea"> <a href="#the-basic-idea" class="anchor-heading" aria-labelledby="the-basic-idea"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> The Basic Idea </h2> <p>Here’s the thing: as ecologists, we’re constantly trying to figure out what drives the patterns we observe. Why are there more species in some places than others? What makes trees grow faster? How does temperature affect animal behavior?</p> <p>Regression gives us a way to quantify these relationships. Instead of just saying “warmer temperatures seem to increase growth,” we can say “for every 1°C increase in temperature, tree ring width increases by 0.15 mm.” That’s powerful stuff.</p> <p>At its core, regression asks: <strong>how does one thing change when another thing changes?</strong></p> <h2 id="a-quick-example"> <a href="#a-quick-example" class="anchor-heading" aria-labelledby="a-quick-example"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> A Quick Example </h2> <p>Let’s say you’re studying tree growth across a temperature gradient. You measure tree ring widths at different sites:</p> <div class="table-wrapper"><table> <thead> <tr> <th>Mean Annual Temperature (°C)</th> <th>Tree Ring Width (mm)</th> </tr> </thead> <tbody> <tr> <td>8</td> <td>1.2</td> </tr> <tr> <td>10</td> <td>1.8</td> </tr> <tr> <td>12</td> <td>2.4</td> </tr> <tr> <td>14</td> <td>2.9</td> </tr> <tr> <td>16</td> <td>3.2</td> </tr> </tbody> </table></div> <p>You can see there’s a pattern - warmer sites have wider rings. But how strong is this relationship? Can we predict growth at a site with 11°C mean temperature? Regression helps us answer these questions.</p> <h2 id="what-can-regression-achieve"> <a href="#what-can-regression-achieve" class="anchor-heading" aria-labelledby="what-can-regression-achieve"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> What Can Regression Achieve? </h2> <p>In ecological research, regression is useful for:</p> <p><strong>Making predictions</strong> - You’ve measured carbon flux at 20 sites, but you want to estimate it across the whole landscape. Regression lets you predict values at unmeasured locations based on environmental variables you can get from satellite data.</p> <p><strong>Understanding relationships</strong> - Does nitrogen addition actually increase plant biomass? By how much? Is the effect statistically significant or could it just be noise?</p> <p><strong>Figuring out what matters</strong> - When you have 15 environmental variables that might explain species richness, regression helps you sort out which ones are actually important.</p> <p><strong>Supporting management decisions</strong> - If you know how much habitat area affects population size, you can make informed recommendations about reserve design.</p> <h2 id="terminology"> <a href="#terminology" class="anchor-heading" aria-labelledby="terminology"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Terminology </h2> <p>Before we dive in, let’s get our vocabulary straight. Different fields use different terms for the same things, which can be confusing.</p> <p><strong>Target variable</strong></p> <ul> <li>You can also call it response variable, dependent variable, outcome</li> <li>The variable you’re trying to predict or explain</li> <li>We usually call it <strong>y</strong></li> <li>Examples: species richness, biomass, survival rate, carbon flux</li> </ul> <p><strong>independent variables</strong></p> <ul> <li>Can also be termed predictors, features, independent variables, explanatory variables</li> <li>The variable you use to make predictions or explain target variables</li> <li>We call these <strong>x</strong> (or x₁, x₂, etc. when there are several)</li> <li>Examples: temperature, precipitation, soil pH, elevation</li> </ul> <p><strong>The model:</strong></p> <ul> <li>This is the mathematical equation that describes how x relates to y</li> <li>General form: y = f(x) + error</li> <li>The “error” part is important - it acknowledges that our model won’t be perfect</li> </ul> <p><strong>Coefficients:</strong></p> <ul> <li>These are the numbers in our model that define the relationship</li> <li>In a simple model like y = 3 + 2x, the “3” is the intercept and “2” is the slope</li> <li>We estimate these from our data</li> </ul> <p><strong>Residuals:</strong></p> <ul> <li>The difference between what we observed and what our model predicted</li> <li>Small residuals = good model fit</li> <li>Patterns in residuals = something’s wrong with our model</li> </ul> <h2 id="how-does-regression-actually-work"> <a href="#how-does-regression-actually-work" class="anchor-heading" aria-labelledby="how-does-regression-actually-work"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> How Does Regression Actually Work? </h2> <p>The basic process goes like this:</p> <ol> <li> <p><strong>Choose a model type.</strong> Are you assuming a straight line relationship? A curve? Multiple predictors?</p> </li> <li> <p><strong>Fit the model to your data.</strong> This means finding the coefficient values that make your predictions as close to the observations as possible.</p> </li> <li> <p><strong>Check if it worked.</strong> Look at how well the model fits, whether the assumptions are met, and whether the results make ecological sense.</p> </li> </ol> <p>The most common approach for step 2 is called “least squares” - we find the coefficients that minimize the sum of squared differences between observed and predicted values. We square the differences so that positive and negative errors don’t cancel out.</p> <h2 id="evaluating-your-model"> <a href="#evaluating-your-model" class="anchor-heading" aria-labelledby="evaluating-your-model"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Evaluating Your Model </h2> <p>How do you know if your model is any good? A few key metrics:</p> <p><strong>R² (R-squared)</strong>: This tells you what fraction of the variation in your data is explained by the model. An R² of 0.7 means your model explains 70% of the variance. What’s “good” depends entirely on your system - in controlled experiments 0.9 might be expected, while in field ecology 0.3 might be excellent.</p> <div>$$ R^2 = 1 - \frac{SS_{res}}{SS_{tot}} = 1 - \frac{\sum_{i=1}^{n}(y_i - \hat{y}_i)^2}{\sum_{i=1}^{n}(y_i - \bar{y})^2} $$</div> <p>Where:</p> <p>$y_i$ is the observed value</p> <p>$\hat{y}_i$ is the predicted value</p> <p>$\bar{y}$ is the mean of observed values</p> <p>$SS_{res}$ is the sum of squared residuals</p> <p>$SS_{tot}$ is the total sum of squares</p> <p><strong>RMSE (Root Mean Square Error)</strong>: This is the average size of your prediction errors, in the same units as your response variable. An RMSE of 2.5°C for a temperature model means your predictions are typically off by about 2.5 degrees.</p> <div>$$ RMSE = \sqrt{\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2} $$</div> <p><strong>MAE (Mean Absolute Error)</strong>: Similar to RMSE but less sensitive to outliers. Useful when you have some weird extreme values in your data.</p> <div>$$ MAE = \frac{1}{n}\sum_{i=1}^{n}|y_i - \hat{y}_i| $$</div> <p>The difference between RMSE and MAE? RMSE penalizes large errors more heavily because of the squaring. If you have a few really bad predictions, RMSE will be much higher than MAE. This can be useful for detecting outliers or problematic predictions.</p><hr /> <h1 id="2-simple-linear-regression"> <a href="#2-simple-linear-regression" class="anchor-heading" aria-labelledby="2-simple-linear-regression"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <strong>2. Simple Linear Regression</strong> </h1> <p>Alright, let’s actually do some regression. We’ll start with the simplest case: one predictor, one response, straight line relationship.</p> <h2 id="the-model"> <a href="#the-model" class="anchor-heading" aria-labelledby="the-model"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> The Model </h2> <p>Simple linear regression fits this equation:</p> <p><strong>ŷ = β₀ + β₁x</strong></p> <p>Where:</p> <ul> <li><strong>ŷ</strong> (y-hat) is our <em>predicted</em> value of the response variable</li> <li><strong>β₀</strong> (beta-zero) is the intercept (value of y when x is zero)</li> <li><strong>β₁</strong> (beta-one) is the slope (how much y changes for each unit increase in x)</li> <li><strong>x</strong> is our predictor variable</li> </ul> <p>That’s it. We’re just fitting a line through our data points.</p> <h3 id="whats-actually-happening"> <a href="#whats-actually-happening" class="anchor-heading" aria-labelledby="whats-actually-happening"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> What’s Actually Happening? </h3> <p>When we fit a regression, we’re looking for the line that minimizes the total squared distance between our observed data points and the line. These distances are called <strong>residuals</strong> - the difference between what we actually observed and what our model predicted.</p> <p>The full model, including the error, is:</p> <p><strong>y = β₀ + β₁x + ε</strong></p> <p>Where <strong>ε</strong> (epsilon) represents the residual error - all the variation in y that our model doesn’t capture. In a perfect world with a perfect model, ε would be zero. In reality, it never is.</p> <p><strong>Why squared distances?</strong> Squaring does two things: (1) it treats positive and negative errors equally (a prediction 10g too high is just as bad as 10g too low), and (2) it penalizes large errors more heavily than small ones. This method is called <strong>Ordinary Least Squares (OLS)</strong>.</p> <h2 id="lets-try-it-penguin-body-mass-and-flipper-length"> <a href="#lets-try-it-penguin-body-mass-and-flipper-length" class="anchor-heading" aria-labelledby="lets-try-it-penguin-body-mass-and-flipper-length"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Let’s Try It: Penguin Body Mass and Flipper Length </h2> <p>We’ll use the Palmer Penguins dataset - real measurements collected by Dr. Kristen Gorman at Palmer Station, Antarctica.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">plotly.express</span> <span class="k">as</span> <span class="n">px</span>
<span class="kn">import</span> <span class="nn">plotly.graph_objects</span> <span class="k">as</span> <span class="n">go</span>

<span class="c1"># Load the penguins dataset
# You can install it with: pip install palmerpenguins
</span><span class="kn">from</span> <span class="nn">palmerpenguins</span> <span class="kn">import</span> <span class="n">load_penguins</span>
<span class="n">penguins</span> <span class="o">=</span> <span class="n">load_penguins</span><span class="p">()</span>

<span class="c1"># Take a look at what we have
</span><span class="k">print</span><span class="p">(</span><span class="s">"Dataset shape:"</span><span class="p">,</span> <span class="n">penguins</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">First few rows:"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">penguins</span><span class="p">.</span><span class="n">head</span><span class="p">())</span>

<span class="c1"># Check for missing values and drop them for now
</span><span class="n">penguins_clean</span> <span class="o">=</span> <span class="n">penguins</span><span class="p">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s">'flipper_length_mm'</span><span class="p">,</span> <span class="s">'body_mass_g'</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Complete cases: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">penguins_clean</span><span class="p">)</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div> <p>Now let’s explore the relationship between flipper length and body mass:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Quick visualization
</span><span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">penguins_clean</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">'flipper_length_mm'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'body_mass_g'</span><span class="p">,</span>
                 <span class="n">color</span><span class="o">=</span><span class="s">'species'</span><span class="p">,</span>
                 <span class="n">title</span><span class="o">=</span><span class="s">'Penguin Body Mass vs Flipper Length'</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">update_layout</span><span class="p">(</span><span class="n">template</span><span class="o">=</span><span class="s">'simple_white'</span><span class="p">,</span> <span class="n">font_size</span> <span class="o">=</span> <span class="mi">36</span><span class="p">,)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div> <p>You’ll see there’s clearly a positive relationship - longer flippers go with heavier birds. Let’s quantify it.</p> <h3 id="fitting-the-regression"> <a href="#fitting-the-regression" class="anchor-heading" aria-labelledby="fitting-the-regression"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Fitting the Regression </h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="c1"># Prepare the data
# sklearn expects X to be a 2D array (rows = samples, columns = features)
# Even with one feature, we need shape (n_samples, 1), not (n_samples,)
# That's why we use [['flipper_length_mm']] (double brackets) instead of ['flipper_length_mm']
</span><span class="n">X</span> <span class="o">=</span> <span class="n">penguins_clean</span><span class="p">[[</span><span class="s">'flipper_length_mm'</span><span class="p">]].</span><span class="n">values</span>  
<span class="n">y</span> <span class="o">=</span> <span class="n">penguins_clean</span><span class="p">[</span><span class="s">'body_mass_g'</span><span class="p">].</span><span class="n">values</span>

<span class="c1"># Fit the model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Slope: </span><span class="si">{</span><span class="n">model</span><span class="p">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> g per mm"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Intercept: </span><span class="si">{</span><span class="n">model</span><span class="p">.</span><span class="n">intercept_</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> g"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"R-squared: </span><span class="si">{</span><span class="n">model</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div> <h3 id="visualizing-the-fit"> <a href="#visualizing-the-fit" class="anchor-heading" aria-labelledby="visualizing-the-fit"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Visualizing the Fit </h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Get predictions for the regression line
# We create a sequence of x values spanning our data range
# reshape(-1, 1) converts the 1D array to 2D (required by sklearn)
</span><span class="n">X_line</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">penguins_clean</span><span class="p">[</span><span class="s">'flipper_length_mm'</span><span class="p">].</span><span class="nb">min</span><span class="p">(),</span> 
                     <span class="n">penguins_clean</span><span class="p">[</span><span class="s">'flipper_length_mm'</span><span class="p">].</span><span class="nb">max</span><span class="p">(),</span> <span class="mi">100</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y_line</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_line</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">go</span><span class="p">.</span><span class="n">Figure</span><span class="p">()</span>

<span class="c1"># Data points
</span><span class="n">fig</span><span class="p">.</span><span class="n">add_trace</span><span class="p">(</span><span class="n">go</span><span class="p">.</span><span class="n">Scatter</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">penguins_clean</span><span class="p">[</span><span class="s">'flipper_length_mm'</span><span class="p">],</span> 
    <span class="n">y</span><span class="o">=</span><span class="n">penguins_clean</span><span class="p">[</span><span class="s">'body_mass_g'</span><span class="p">],</span>
    <span class="n">mode</span><span class="o">=</span><span class="s">'markers'</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">'Observations'</span><span class="p">,</span>
    <span class="n">marker</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">24</span><span class="p">,</span> <span class="n">opacity</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="p">))</span>

<span class="c1"># Regression line
</span><span class="n">fig</span><span class="p">.</span><span class="n">add_trace</span><span class="p">(</span><span class="n">go</span><span class="p">.</span><span class="n">Scatter</span><span class="p">(</span>
    <span class="n">x</span><span class="o">=</span><span class="n">X_line</span><span class="p">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">y</span><span class="o">=</span><span class="n">y_line</span><span class="p">,</span>
    <span class="n">mode</span><span class="o">=</span><span class="s">'lines'</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">'Regression line'</span><span class="p">,</span>
    <span class="n">line</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">color</span><span class="o">=</span><span class="s">'red'</span><span class="p">,</span> <span class="n">width</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="p">))</span>

<span class="n">fig</span><span class="p">.</span><span class="n">update_layout</span><span class="p">(</span>
    <span class="n">title</span><span class="o">=</span><span class="s">'Penguin Body Mass vs Flipper Length'</span><span class="p">,</span>
    <span class="n">xaxis_title</span><span class="o">=</span><span class="s">'Flipper Length (mm)'</span><span class="p">,</span>
    <span class="n">yaxis_title</span><span class="o">=</span><span class="s">'Body Mass (g)'</span><span class="p">,</span>
    <span class="n">template</span><span class="o">=</span><span class="s">'simple_white'</span><span class="p">,</span>
    <span class="n">font_size</span> <span class="o">=</span> <span class="mi">36</span>
<span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div> <h2 id="what-do-these-numbers-mean"> <a href="#what-do-these-numbers-mean" class="anchor-heading" aria-labelledby="what-do-these-numbers-mean"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> What Do These Numbers Mean? </h2> <p>With real data, you should get something like:</p> <ul> <li> <p><strong>Slope ≈ 49.7 g/mm</strong>: For every 1 mm increase in flipper length, body mass increases by about 50 grams. This is our β₁.</p> </li> <li> <p><strong>Intercept ≈ -5781 g</strong>: This would be the predicted mass at flipper length = 0, which makes no biological sense (negative mass!), but it’s needed mathematically to position the line correctly within the range of our actual data. This is our β₀.</p> </li> <li> <p><strong>R² = 0.76</strong> means flipper length explains about 76% of the variation in body mass. The remaining 24% is unexplained variation (our ε).</p> </li> </ul> <h3 id="making-predictions"> <a href="#making-predictions" class="anchor-heading" aria-labelledby="making-predictions"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Making Predictions </h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># What's the predicted mass for a penguin with 200mm flippers?
</span><span class="n">new_flipper</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">200</span><span class="p">]])</span>  <span class="c1"># Note: 2D array for sklearn
</span><span class="n">predicted_mass</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">new_flipper</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Predicted mass for 200mm flipper: </span><span class="si">{</span><span class="n">predicted_mass</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">0</span><span class="n">f</span><span class="si">}</span><span class="s"> g"</span><span class="p">)</span>

<span class="c1"># What about 180mm?
</span><span class="n">new_flipper</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">180</span><span class="p">]])</span>
<span class="n">predicted_mass</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">new_flipper</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Predicted mass for 180mm flipper: </span><span class="si">{</span><span class="n">predicted_mass</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">0</span><span class="n">f</span><span class="si">}</span><span class="s"> g"</span><span class="p">)</span>
</code></pre></div></div> <h3 id="a-note-on-extrapolation"> <a href="#a-note-on-extrapolation" class="anchor-heading" aria-labelledby="a-note-on-extrapolation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> A Note on Extrapolation </h3> <p>Be careful about predicting outside the range of your training data! Our model was built on penguins with flippers roughly 170-230mm. If you try to predict mass for a 100mm flipper or a 300mm flipper, you’re <strong>extrapolating</strong> - assuming the linear relationship continues outside the observed range. This is often dangerous because:</p> <ol> <li>Relationships may be non-linear at extremes</li> <li>You have no data to validate predictions in that range</li> <li>Biological constraints may make extrapolations impossible (like our negative-mass intercept)</li> </ol> <p><strong>Stick to interpolation</strong> (predicting within your data range) whenever possible.</p><hr /> <div style="background-color: #f5f5f5; padding: 10px; border-radius: 5px; margin-bottom: 5px;"> <div class="notice--primary"> <h3> <a href="#a-note-on-extrapolation" class="anchor-heading" aria-labelledby="a-note-on-extrapolation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Try It Yourself </h3> <p>Using the Palmer Penguins dataset, fit a simple regression predicting bill length from bill depth. What do you find? Is the relationship positive or negative? How does R² compare to the flipper-mass relationship?</p> <details> <summary>Solution!</summary> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">palmerpenguins</span> <span class="kn">import</span> <span class="n">load_penguins</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="n">penguins</span> <span class="o">=</span> <span class="n">load_penguins</span><span class="p">().</span><span class="n">dropna</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s">'bill_length_mm'</span><span class="p">,</span> <span class="s">'bill_depth_mm'</span><span class="p">])</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">penguins</span><span class="p">[[</span><span class="s">'bill_depth_mm'</span><span class="p">]].</span><span class="n">values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">penguins</span><span class="p">[</span><span class="s">'bill_length_mm'</span><span class="p">].</span><span class="n">values</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Slope: </span><span class="si">{</span><span class="n">model</span><span class="p">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Intercept: </span><span class="si">{</span><span class="n">model</span><span class="p">.</span><span class="n">intercept_</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"R-squared: </span><span class="si">{</span><span class="n">model</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="c1"># Surprise! You'll find a NEGATIVE relationship and very low R²
# This is Simpson's paradox - when you combine the species,
# the overall trend is negative, but within each species 
# the relationship is positive. This is because Gentoo penguins
# have long bills but shallow depth, while Adelie have 
# shorter bills but deeper depth.
</span>
<span class="c1"># Try it by species:
</span><span class="k">for</span> <span class="n">species</span> <span class="ow">in</span> <span class="n">penguins</span><span class="p">[</span><span class="s">'species'</span><span class="p">].</span><span class="n">unique</span><span class="p">():</span>
    <span class="n">subset</span> <span class="o">=</span> <span class="n">penguins</span><span class="p">[</span><span class="n">penguins</span><span class="p">[</span><span class="s">'species'</span><span class="p">]</span> <span class="o">==</span> <span class="n">species</span><span class="p">]</span>
    <span class="n">X_sp</span> <span class="o">=</span> <span class="n">subset</span><span class="p">[[</span><span class="s">'bill_depth_mm'</span><span class="p">]].</span><span class="n">values</span>
    <span class="n">y_sp</span> <span class="o">=</span> <span class="n">subset</span><span class="p">[</span><span class="s">'bill_length_mm'</span><span class="p">].</span><span class="n">values</span>
    <span class="n">model_sp</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">().</span><span class="n">fit</span><span class="p">(</span><span class="n">X_sp</span><span class="p">,</span> <span class="n">y_sp</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">species</span><span class="si">}</span><span class="s">: slope = </span><span class="si">{</span><span class="n">model_sp</span><span class="p">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">, R² = </span><span class="si">{</span><span class="n">model_sp</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_sp</span><span class="p">,</span> <span class="n">y_sp</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div> </div> </details> </div> </div><hr /> <h2 id="limitation-simpsons-paradox"> <a href="#limitation-simpsons-paradox" class="anchor-heading" aria-labelledby="limitation-simpsons-paradox"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Limitation: Simpson’s Paradox </h2> <p>The exercise above reveals something important: simple regression can be misleading when you have groups in your data. This phenomenon is called <strong>Simpson’s Paradox</strong> - when a trend appears in several groups of data but disappears or reverses when the groups are combined.</p> <p>In our case:</p> <ul> <li><strong>Within each species</strong>: deeper bills → longer bills (positive relationship)</li> <li><strong>Across all species combined</strong>: deeper bills → shorter bills (negative relationship!)</li> </ul> <p>How can this be? It’s because species differ systematically in both variables. Gentoo penguins have long but shallow bills; Adelie penguins have shorter but deeper bills. When you ignore species, these group differences create an artificial negative trend.</p> <p>The flipper-mass relationship also differs among species - Gentoo penguins are bigger than Adelie and Chinstrap overall.</p> <p>This is one reason we need multiple regression - to account for additional factors that might be confounding our results.</p><hr /> <h1 id="3-multiple-regression"> <a href="#3-multiple-regression" class="anchor-heading" aria-labelledby="3-multiple-regression"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <strong>3. Multiple Regression</strong> </h1> <p>In the real world, ecological responses depend on many factors simultaneously. Multiple regression lets us include all of them in one model.</p> <h2 id="why-go-multiple"> <a href="#why-go-multiple" class="anchor-heading" aria-labelledby="why-go-multiple"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Why Go Multiple? </h2> <p>Looking at the penguin data, body mass depends on more than just flipper length:</p> <ul> <li>Species differ in overall body size</li> <li>Males are larger than females</li> <li>Bill dimensions correlate with mass too</li> </ul> <p>If we only model mass against flipper length, we’re missing important information. Multiple regression lets us ask: <strong>“What’s the effect of flipper length, <em>after accounting for</em> species and sex?”</strong></p> <p>This is fundamentally different from simple regression. We’re no longer asking “how does y change with x?” but rather “how does y change with x₁, <em>holding x₂, x₃, etc. constant</em>?”</p> <h2 id="the-model-1"> <a href="#the-model-1" class="anchor-heading" aria-labelledby="the-model-1"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> The Model </h2> <p>We extend our simple model to include multiple predictors:</p> <p><strong>ŷ = β₀ + β₁x₁ + β₂x₂ + β₃x₃ + …</strong></p> <p>Each coefficient now tells us the <strong>partial effect</strong> of that variable - its effect while holding the others constant. This is crucial for interpretation and is what makes multiple regression so powerful for teasing apart confounded relationships.</p> <h2 id="example-predicting-penguin-body-mass"> <a href="#example-predicting-penguin-body-mass" class="anchor-heading" aria-labelledby="example-predicting-penguin-body-mass"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Example: Predicting Penguin Body Mass </h2> <p>Let’s build a more complete model for penguin body mass.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">palmerpenguins</span> <span class="kn">import</span> <span class="n">load_penguins</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span>
<span class="kn">import</span> <span class="nn">sklearn.metrics</span> <span class="k">as</span> <span class="n">metrics</span>

<span class="c1"># Load and prepare data
</span><span class="n">penguins</span> <span class="o">=</span> <span class="n">load_penguins</span><span class="p">().</span><span class="n">dropna</span><span class="p">()</span>

<span class="c1"># Encode categorical variables (species and sex) as numbers
# LabelEncoder converts text categories to integers: 0, 1, 2, etc.
</span><span class="n">le_species</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">le_sex</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">penguins</span><span class="p">[</span><span class="s">'species_code'</span><span class="p">]</span> <span class="o">=</span> <span class="n">le_species</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">penguins</span><span class="p">[</span><span class="s">'species'</span><span class="p">])</span>
<span class="n">penguins</span><span class="p">[</span><span class="s">'sex_code'</span><span class="p">]</span> <span class="o">=</span> <span class="n">le_sex</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">penguins</span><span class="p">[</span><span class="s">'sex'</span><span class="p">])</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Species encoding:"</span><span class="p">,</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">le_species</span><span class="p">.</span><span class="n">classes_</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">le_species</span><span class="p">.</span><span class="n">classes_</span><span class="p">)))))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Sex encoding:"</span><span class="p">,</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">le_sex</span><span class="p">.</span><span class="n">classes_</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">le_sex</span><span class="p">.</span><span class="n">classes_</span><span class="p">)))))</span>

<span class="c1"># Check what we have
</span><span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Dataset: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">penguins</span><span class="p">)</span><span class="si">}</span><span class="s"> penguins"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">penguins</span><span class="p">[[</span><span class="s">'species'</span><span class="p">,</span> <span class="s">'sex'</span><span class="p">,</span> <span class="s">'flipper_length_mm'</span><span class="p">,</span> <span class="s">'bill_length_mm'</span><span class="p">,</span> 
                <span class="s">'bill_depth_mm'</span><span class="p">,</span> <span class="s">'body_mass_g'</span><span class="p">]].</span><span class="n">head</span><span class="p">())</span>
</code></pre></div></div> <h3 id="a-note-on-encoding-categorical-variables"> <a href="#a-note-on-encoding-categorical-variables" class="anchor-heading" aria-labelledby="a-note-on-encoding-categorical-variables"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> A Note on Encoding Categorical Variables </h3> <p>We just converted species (Adelie, Chinstrap, Gentoo) to numbers (0, 1, 2). This is called <strong>label encoding</strong> and it has a subtle problem: it implies an ordering. The model might think Gentoo (2) is “more” than Adelie (0) in some way.</p> <p>For binary variables like sex (male/female → 0/1), this is fine - we’re just measuring the difference between two groups.</p> <p>For multi-category variables, a better approach is <strong>one-hot encoding</strong> (also called dummy variables), where each category gets its own 0/1 column. We’ll keep label encoding here for simplicity, but be aware this is a simplification. In a real analysis, you’d want to use one-hot encoding or a statistical framework that handles categories properly.</p> <h3 id="check-correlations-first"> <a href="#check-correlations-first" class="anchor-heading" aria-labelledby="check-correlations-first"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Check Correlations First </h3> <p>Before building a model, it’s good practice to explore how your variables relate to each other:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Which variables correlate with body mass?
</span><span class="n">numeric_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s">'flipper_length_mm'</span><span class="p">,</span> <span class="s">'bill_length_mm'</span><span class="p">,</span> <span class="s">'bill_depth_mm'</span><span class="p">,</span> 
                <span class="s">'body_mass_g'</span><span class="p">,</span> <span class="s">'species_code'</span><span class="p">,</span> <span class="s">'sex_code'</span><span class="p">]</span>
                
<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">Correlations with body mass:"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">penguins</span><span class="p">[</span><span class="n">numeric_cols</span><span class="p">].</span><span class="n">corr</span><span class="p">()[</span><span class="s">'body_mass_g'</span><span class="p">].</span><span class="n">sort_values</span><span class="p">(</span><span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span>
</code></pre></div></div> <p><strong>Why check correlations?</strong></p> <ol> <li>It gives you a preview of which variables might be useful predictors</li> <li>It reveals potential <strong>multicollinearity</strong> - when predictors are highly correlated with <em>each other</em></li> </ol> <p><strong>Multicollinearity</strong> is a problem because if two predictors are highly correlated, it becomes hard to separate their individual effects. The model can’t tell if it’s variable A or variable B causing the effect. You’ll get unstable coefficient estimates. We’ll keep an eye on this.</p> <h3 id="why-split-into-training-and-test-sets"> <a href="#why-split-into-training-and-test-sets" class="anchor-heading" aria-labelledby="why-split-into-training-and-test-sets"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Why Split Into Training and Test Sets? </h3> <p>This is a fundamental concept in predictive modeling:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Prepare features and target
</span><span class="n">X</span> <span class="o">=</span> <span class="n">penguins</span><span class="p">[[</span><span class="s">'flipper_length_mm'</span><span class="p">,</span> <span class="s">'bill_length_mm'</span><span class="p">,</span> <span class="s">'bill_depth_mm'</span><span class="p">,</span> 
              <span class="s">'species_code'</span><span class="p">,</span> <span class="s">'sex_code'</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">penguins</span><span class="p">[</span><span class="s">'body_mass_g'</span><span class="p">]</span>

<span class="c1"># Split into training and test sets
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>
<span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Training set: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span><span class="si">}</span><span class="s"> penguins"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Test set: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span><span class="si">}</span><span class="s"> penguins"</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Why do we split the data?</strong></p> <p>If we train our model on all the data and then evaluate it on the same data, we’re essentially asking “how well did you memorize this?” A model could achieve a perfect score by memorizing every data point without learning any real patterns.</p> <p>What we actually want to know is: “how well will this model perform on <em>new, unseen data</em>?”</p> <p>By holding out a <strong>test set</strong> that the model never sees during training, we can get an honest estimate of how well the model will generalize to new penguins.</p> <ul> <li><strong>Training set (70%)</strong>: Used to fit the model</li> <li><strong>Test set (30%)</strong>: Used only for evaluation, never for fitting</li> </ul> <p>The <code class="language-plaintext highlighter-rouge">random_state=42</code> ensures we get the same split every time we run the code (for reproducibility).</p> <h3 id="fitting-the-multiple-regression"> <a href="#fitting-the-multiple-regression" class="anchor-heading" aria-labelledby="fitting-the-multiple-regression"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Fitting the Multiple Regression </h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Fit the model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Look at coefficients
</span><span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">Model coefficients:"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  Intercept: </span><span class="si">{</span><span class="n">model</span><span class="p">.</span><span class="n">intercept_</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">coef</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">coef_</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">coef</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div> <h2 id="interpreting-the-results"> <a href="#interpreting-the-results" class="anchor-heading" aria-labelledby="interpreting-the-results"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Interpreting the Results </h2> <p>What do these coefficients actually mean? This is where multiple regression differs fundamentally from simple regression.</p> <p><strong>flipper_length_mm</strong>: Each additional mm of flipper length adds about 17g to body mass, <em>after controlling for</em> other variables. Note this is smaller than in simple regression (~50g). Why? Because some of that apparent flipper effect was actually due to species differences—Gentoo penguins have both longer flippers AND higher mass. Once we account for species, the “pure” flipper effect is smaller.</p> <p><strong>bill_length_mm</strong>: Longer bills are associated with slightly higher mass, holding other variables constant.</p> <p><strong>bill_depth_mm</strong>: Deeper bills are associated with higher mass. This makes sense—it’s a measure of overall head size.</p> <p><strong>species_code</strong>: The coefficient shows average difference between species (encoded as 0, 1, 2). Interpretation is tricky with encoded categories because we’re treating it as a continuous variable. This is a limitation of our simple encoding approach.</p> <p><strong>sex_code</strong>: Males (coded as 1) are heavier than females (coded as 0) by about this many grams, controlling for body measurements. This is the easiest to interpret—it’s the male-female difference in mass after accounting for differences in flipper length, bill size, etc.</p> <h3 id="the-key-insight-controlling-for-other-variables"> <a href="#the-key-insight-controlling-for-other-variables" class="anchor-heading" aria-labelledby="the-key-insight-controlling-for-other-variables"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> The Key Insight: “Controlling For” Other Variables </h3> <p>The phrase “controlling for” or “holding constant” is crucial in multiple regression. Here’s what it means:</p> <p>Imagine you could magically find two penguins that are:</p> <ul> <li>The same species</li> <li>The same sex</li> <li>Have the same bill length</li> <li>Have the same bill depth</li> <li>But differ in flipper length by 1mm</li> </ul> <p>The flipper coefficient tells you how much heavier we’d expect the longer-flippered penguin to be.</p> <p>Of course, we can’t actually find such perfectly matched penguins. Multiple regression does this statistically by partitioning the variation in body mass among all the predictors.</p> <h3 id="how-good-is-our-model"> <a href="#how-good-is-our-model" class="anchor-heading" aria-labelledby="how-good-is-our-model"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> How Good Is Our Model? </h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Predict on test data (data the model has never seen)
</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Calculate metrics
</span><span class="n">r2</span> <span class="o">=</span> <span class="n">metrics</span><span class="p">.</span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">rmse</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">metrics</span><span class="p">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="n">mae</span> <span class="o">=</span> <span class="n">metrics</span><span class="p">.</span><span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"R-squared: </span><span class="si">{</span><span class="n">r2</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"RMSE: </span><span class="si">{</span><span class="n">rmse</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s"> g"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"MAE: </span><span class="si">{</span><span class="n">mae</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s"> g"</span><span class="p">)</span>
</code></pre></div></div> <p><strong>Understanding the Evaluation Metrics:</strong></p> <ul> <li> <p><strong>R² (R-squared)</strong>: Same interpretation as before - proportion of variance explained. But now we’re measuring it on the <em>test set</em>, so it tells us how well the model generalizes.</p> </li> <li> <p><strong>RMSE (Root Mean Squared Error)</strong>: The square root of the average squared prediction error. It’s in the same units as your response variable (grams), so you can interpret it directly: “on average, our predictions are off by about RMSE grams.” RMSE penalizes large errors heavily because of the squaring.</p> </li> <li> <p><strong>MAE (Mean Absolute Error)</strong>: The average absolute prediction error. Also in grams. MAE treats all errors equally regardless of size. If RMSE is much larger than MAE, you have some predictions with large errors.</p> </li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Plot predicted vs observed
</span><span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y_pred</span><span class="p">,</span>
                 <span class="n">labels</span><span class="o">=</span><span class="p">{</span><span class="s">'x'</span><span class="p">:</span> <span class="s">'Observed Mass (g)'</span><span class="p">,</span> <span class="s">'y'</span><span class="p">:</span> <span class="s">'Predicted Mass (g)'</span><span class="p">})</span>
<span class="n">fig</span><span class="p">.</span><span class="n">add_scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="p">[</span><span class="n">y_test</span><span class="p">.</span><span class="nb">min</span><span class="p">(),</span> <span class="n">y_test</span><span class="p">.</span><span class="nb">max</span><span class="p">()],</span> 
                <span class="n">y</span><span class="o">=</span><span class="p">[</span><span class="n">y_test</span><span class="p">.</span><span class="nb">min</span><span class="p">(),</span> <span class="n">y_test</span><span class="p">.</span><span class="nb">max</span><span class="p">()],</span>
                <span class="n">mode</span><span class="o">=</span><span class="s">'lines'</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">'1:1 line'</span><span class="p">,</span>
                <span class="n">line</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span><span class="n">dash</span><span class="o">=</span><span class="s">'dash'</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s">'red'</span><span class="p">))</span>
<span class="n">fig</span><span class="p">.</span><span class="n">update_layout</span><span class="p">(</span><span class="n">template</span><span class="o">=</span><span class="s">'simple_white'</span><span class="p">,</span>
                  <span class="n">title</span><span class="o">=</span><span class="sa">f</span><span class="s">'Multiple Regression: R² = </span><span class="si">{</span><span class="n">r2</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">'</span><span class="p">,</span>
                  <span class="n">font_size</span> <span class="o">=</span> <span class="mi">36</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">update_traces</span><span class="p">(</span><span class="n">marker_size</span> <span class="o">=</span> <span class="mi">24</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div> <p><strong>Interpreting the predicted vs. observed plot</strong>: If predictions were perfect, all points would fall exactly on the red 1:1 line. The scatter around that line shows prediction error. Look for patterns - if errors are larger for heavy penguins than light ones, your model might have heteroscedasticity issues.</p> <h2 id="does-adding-variables-help"> <a href="#does-adding-variables-help" class="anchor-heading" aria-labelledby="does-adding-variables-help"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Does Adding Variables Help? </h2> <p>Let’s compare models with different numbers of predictors:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Just flipper length
</span><span class="n">m1</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">().</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">[[</span><span class="s">'flipper_length_mm'</span><span class="p">]],</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">results</span><span class="p">.</span><span class="n">append</span><span class="p">({</span>
    <span class="s">'Model'</span><span class="p">:</span> <span class="s">'Flipper only'</span><span class="p">,</span>
    <span class="s">'R²'</span><span class="p">:</span> <span class="n">m1</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">[[</span><span class="s">'flipper_length_mm'</span><span class="p">]],</span> <span class="n">y_test</span><span class="p">)</span>
<span class="p">})</span>

<span class="c1"># Flipper + species
</span><span class="n">m2</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">().</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">[[</span><span class="s">'flipper_length_mm'</span><span class="p">,</span> <span class="s">'species_code'</span><span class="p">]],</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">results</span><span class="p">.</span><span class="n">append</span><span class="p">({</span>
    <span class="s">'Model'</span><span class="p">:</span> <span class="s">'Flipper + Species'</span><span class="p">,</span>
    <span class="s">'R²'</span><span class="p">:</span> <span class="n">m2</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">[[</span><span class="s">'flipper_length_mm'</span><span class="p">,</span> <span class="s">'species_code'</span><span class="p">]],</span> <span class="n">y_test</span><span class="p">)</span>
<span class="p">})</span>

<span class="c1"># Flipper + species + sex
</span><span class="n">m3</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">().</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">[[</span><span class="s">'flipper_length_mm'</span><span class="p">,</span> <span class="s">'species_code'</span><span class="p">,</span> <span class="s">'sex_code'</span><span class="p">]],</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">results</span><span class="p">.</span><span class="n">append</span><span class="p">({</span>
    <span class="s">'Model'</span><span class="p">:</span> <span class="s">'Flipper + Species + Sex'</span><span class="p">,</span>
    <span class="s">'R²'</span><span class="p">:</span> <span class="n">m3</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">[[</span><span class="s">'flipper_length_mm'</span><span class="p">,</span> <span class="s">'species_code'</span><span class="p">,</span> <span class="s">'sex_code'</span><span class="p">]],</span> <span class="n">y_test</span><span class="p">)</span>
<span class="p">})</span>

<span class="c1"># All predictors
</span><span class="n">m4</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">().</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">results</span><span class="p">.</span><span class="n">append</span><span class="p">({</span>
    <span class="s">'Model'</span><span class="p">:</span> <span class="s">'All predictors'</span><span class="p">,</span>
    <span class="s">'R²'</span><span class="p">:</span> <span class="n">m4</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="p">})</span>

<span class="k">print</span><span class="p">(</span><span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">))</span>
</code></pre></div></div> <p>You should see R² improve as you add relevant predictors - species and sex both contribute meaningful information about body mass.</p> <p><strong>But be careful!</strong> R² will always increase (or stay the same) when you add more predictors to a model, even if those predictors are useless. This is because more parameters give the model more flexibility to fit the training data.</p> <p>This is why we evaluate on a <strong>test set</strong> - on new data, useless predictors will actually hurt performance by adding noise. This is called <strong>overfitting</strong>, and it’s a central concern in machine learning.</p><hr /> <div style="background-color: #f5f5f5; padding: 10px; border-radius: 5px; margin-bottom: 5px;"> <div class="notice--primary"> <h3> <a href="#does-adding-variables-help" class="anchor-heading" aria-labelledby="does-adding-variables-help"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Exercise </h3> <p>Build a multiple regression model predicting bill length from bill depth, flipper length, species, and sex. Which predictors have the strongest effects? Does the bill depth coefficient change from the simple regression?</p> <details> <summary>Solution!</summary> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">palmerpenguins</span> <span class="kn">import</span> <span class="n">load_penguins</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span>

<span class="n">penguins</span> <span class="o">=</span> <span class="n">load_penguins</span><span class="p">().</span><span class="n">dropna</span><span class="p">()</span>

<span class="c1"># Encode categoricals
</span><span class="n">le_species</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">le_sex</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">penguins</span><span class="p">[</span><span class="s">'species_code'</span><span class="p">]</span> <span class="o">=</span> <span class="n">le_species</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">penguins</span><span class="p">[</span><span class="s">'species'</span><span class="p">])</span>
<span class="n">penguins</span><span class="p">[</span><span class="s">'sex_code'</span><span class="p">]</span> <span class="o">=</span> <span class="n">le_sex</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">penguins</span><span class="p">[</span><span class="s">'sex'</span><span class="p">])</span>

<span class="c1"># Prepare data
</span><span class="n">X</span> <span class="o">=</span> <span class="n">penguins</span><span class="p">[[</span><span class="s">'bill_depth_mm'</span><span class="p">,</span> <span class="s">'flipper_length_mm'</span><span class="p">,</span> <span class="s">'species_code'</span><span class="p">,</span> <span class="s">'sex_code'</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">penguins</span><span class="p">[</span><span class="s">'bill_length_mm'</span><span class="p">]</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Full model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"R²: </span><span class="si">{</span><span class="n">model</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Coefficients:"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">coef</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">model</span><span class="p">.</span><span class="n">coef_</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">coef</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="c1"># Compare to simple regression
</span><span class="n">simple</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">simple</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">penguins</span><span class="p">[[</span><span class="s">'bill_depth_mm'</span><span class="p">]],</span> <span class="n">penguins</span><span class="p">[</span><span class="s">'bill_length_mm'</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="se">\n</span><span class="s">Simple regression bill_depth coefficient: </span><span class="si">{</span><span class="n">simple</span><span class="p">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Multiple regression bill_depth coefficient: </span><span class="si">{</span><span class="n">model</span><span class="p">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="c1"># The bill_depth coefficient changes dramatically - in simple regression
# it's negative (Simpson's paradox), but in multiple regression 
# controlling for species, it becomes positive as expected.
</span></code></pre></div> </div> </details> </div> </div><hr /> <h2 id="limitations-of-multiple-regression"> <a href="#limitations-of-multiple-regression" class="anchor-heading" aria-labelledby="limitations-of-multiple-regression"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Limitations of Multiple Regression </h2> <p>Multiple regression is powerful but has some important limitations:</p> <p><strong>1. It assumes linear relationships.</strong> The model assumes each predictor has a straight-line relationship with the response. If flipper length and mass have a curved relationship (they often do at extremes), a linear model won’t capture that properly. You might need polynomial terms or transformations.</p> <p><strong>2. It assumes additive effects.</strong> The model says the effect of flipper length is the same for all species - we just shift the intercept for each species. In reality, species might differ in their flipper-mass scaling (different slopes). This would require <strong>interaction terms</strong>.</p> <p><strong>3. It assumes independent errors.</strong> If you measured the same penguin multiple times, or penguins from the same colony are more similar, you violate the independence assumption. You’d need mixed-effects models for such data.</p> <p><strong>4. Outliers can have outsized influence.</strong> A single unusual data point can dramatically shift your regression line. Always check for influential observations.</p> <p>These limitations bring us to machine learning approaches, which can handle more complexity.</p><hr /> <h1 id="4-machine-learning-with-random-forests"> <a href="#4-machine-learning-with-random-forests" class="anchor-heading" aria-labelledby="4-machine-learning-with-random-forests"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <strong>4. Machine Learning with Random Forests</strong> </h1> <p>Alright, now we’re getting to the fun stuff. Machine learning sounds fancy, but the basic idea is simple: let the algorithm figure out the patterns in your data, rather than you specifying them in advance.</p> <h2 id="why-machine-learning"> <a href="#why-machine-learning" class="anchor-heading" aria-labelledby="why-machine-learning"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Why Machine Learning? </h2> <p>Ecological relationships are often messy:</p> <ul> <li>Relationships might be non-linear (e.g., growth rates that plateau)</li> <li>Effects of one variable depend on another (interactions)</li> <li>There might be thresholds we didn’t anticipate</li> <li>The functional form might be completely unknown</li> </ul> <p>Machine learning algorithms can discover these patterns automatically. You don’t have to know the shape of the relationship beforehand.</p> <p><strong>The trade-off</strong>: Machine learning models are often less interpretable than linear regression. You might get better predictions but less insight into <em>why</em>. This is sometimes called the “black box” problem.</p> <h2 id="decision-trees-the-building-block"> <a href="#decision-trees-the-building-block" class="anchor-heading" aria-labelledby="decision-trees-the-building-block"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Decision Trees: The Building Block </h2> <p>Before we get to Random Forests, we need to understand decision trees. They’re surprisingly intuitive.</p> <h3 id="the-basic-idea-1"> <a href="#the-basic-idea-1" class="anchor-heading" aria-labelledby="the-basic-idea-1"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> The Basic Idea </h3> <p>A decision tree is basically a flowchart of yes/no questions:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Is flipper length &gt; 206mm?
├── Yes → Probably a Gentoo, predict ~5000g
└── No → Is bill depth &gt; 18mm?
    ├── Yes → Probably Adelie, predict ~3700g
    └── No → Probably Chinstrap, predict ~3500g
</code></pre></div></div> <p>The algorithm figures out:</p> <ol> <li>Which questions to ask (which variable to split on)</li> <li>What thresholds to use (why 206mm and not 200mm?)</li> <li>When to stop asking questions</li> </ol> <h3 id="how-does-it-choose-splits"> <a href="#how-does-it-choose-splits" class="anchor-heading" aria-labelledby="how-does-it-choose-splits"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> How Does It Choose Splits? </h3> <p>At each step, the algorithm tries every possible split of every variable and picks the one that creates the most “pure” groups - groups where the response values are most similar to each other.</p> <p>For regression, “purity” is measured by variance. A good split creates child nodes with lower variance in the response than the parent node.</p> <p><strong>The Goal: Reduce Variance</strong></p> <p>Before any split, a node contains samples with some variance in the target variable. The tree wants to split these samples into two groups where:</p> <ul> <li>Group 1 (left child): samples are similar to each other</li> <li>Group 2 (right child): samples are similar to each other</li> </ul> <p>Even if the two groups have very different means, that’s fine, what matters is that <em>within</em> each group, values are more similar than before.</p> <p><strong>The Algorithm</strong></p> <p>For every possible split (every feature × every threshold):</p> <ol> <li>Divide samples into left and right groups based on the split</li> <li>Calculate the weighted average variance of the two groups</li> <li>Pick the split that gives the lowest weighted variance</li> </ol> <p>Mathematically, we want to minimize:</p> <p><strong>Cost = (n_left / n_total) × MSE_left + (n_right / n_total) × MSE_right</strong></p> <p>Where MSE (Mean Squared Error) measures variance:</p> <p><strong>MSE = (1/n) × Σ(yᵢ - ȳ)²</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span><span class="p">,</span> <span class="n">plot_tree</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="c1"># Using our penguin data
</span><span class="n">X</span> <span class="o">=</span> <span class="n">penguins</span><span class="p">[[</span><span class="s">'flipper_length_mm'</span><span class="p">,</span> <span class="s">'bill_length_mm'</span><span class="p">,</span> <span class="s">'bill_depth_mm'</span><span class="p">,</span> 
              <span class="s">'species_code'</span><span class="p">,</span> <span class="s">'sex_code'</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">penguins</span><span class="p">[</span><span class="s">'body_mass_g'</span><span class="p">]</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Fit a simple tree
# max_depth=3 limits the tree to 3 levels of questions
</span><span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">tree</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Visualize it
</span><span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plot_tree</span><span class="p">(</span><span class="n">tree</span><span class="p">,</span> <span class="n">feature_names</span><span class="o">=</span><span class="n">X</span><span class="p">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">filled</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">rounded</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">9</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Decision Tree for Penguin Body Mass'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Decision Tree R²: </span><span class="si">{</span><span class="n">tree</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div> <h3 id="reading-the-tree-visualization"> <a href="#reading-the-tree-visualization" class="anchor-heading" aria-labelledby="reading-the-tree-visualization"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Reading the Tree Visualization </h3> <p>Each box in the tree visualization shows:</p> <ul> <li>The splitting rule (e.g., “flipper_length_mm &lt;= 206.5”)</li> <li><code class="language-plaintext highlighter-rouge">samples</code>: how many training samples reached this node</li> <li><code class="language-plaintext highlighter-rouge">value</code>: the predicted value (mean of samples at this node)</li> <li><code class="language-plaintext highlighter-rouge">squared_error</code> (or <code class="language-plaintext highlighter-rouge">mse</code>): the variance of samples at this node</li> </ul> <p>The colors indicate the predicted value - similar colors mean similar predictions.</p> <h3 id="the-overfitting-problem"> <a href="#the-overfitting-problem" class="anchor-heading" aria-labelledby="the-overfitting-problem"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> The Overfitting Problem </h3> <p>Decision trees are easy to interpret - you can literally see the rules. But they have a critical flaw: <strong>they tend to overfit</strong>.</p> <p>A deep tree can grow until each leaf contains just one sample, essentially memorizing the training data perfectly. But this memorization doesn’t generalize - the model fails on new data because it learned noise, not signal.</p> <p>Try this experiment:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># A tree with no depth limit
</span><span class="n">deep_tree</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>  <span class="c1"># No max_depth
</span><span class="n">deep_tree</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Deep tree - Training R²: </span><span class="si">{</span><span class="n">deep_tree</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Deep tree - Test R²: </span><span class="si">{</span><span class="n">deep_tree</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div> <p>You’ll likely see near-perfect training R² but worse test R² - classic overfitting!</p> <h2 id="random-forests-many-trees-are-better-than-one"> <a href="#random-forests-many-trees-are-better-than-one" class="anchor-heading" aria-labelledby="random-forests-many-trees-are-better-than-one"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Random Forests: Many Trees Are Better Than One </h2> <p>Random Forests solve the overfitting problem through a clever strategy: build many trees and average their predictions. Individual trees might make mistakes, but their errors tend to cancel out.</p> <h3 id="how-it-works"> <a href="#how-it-works" class="anchor-heading" aria-labelledby="how-it-works"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> How It Works </h3> <p>Each tree in the forest is built differently:</p> <ol> <li> <p><strong>Bootstrap sampling</strong>: Each tree is trained on a random sample of the data, drawn <em>with replacement</em> (some observations appear multiple times, others not at all). This is called a “bootstrap sample.”</p> </li> <li> <p><strong>Random feature selection</strong>: At each split, instead of considering all variables, only a random subset is considered. This prevents all trees from making identical decisions.</p> </li> </ol> <p>This randomness means individual trees are “weaker” (less accurate) than a fully-grown single tree. But their collective wisdom is stronger and more robust.</p> <p><strong>The magic</strong>: When you average many imperfect but different predictions, the random errors cancel out, while the true signal remains.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>

<span class="c1"># Fit a random forest
</span><span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span>
    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>    <span class="c1"># number of trees in the forest
</span>    <span class="n">max_depth</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>        <span class="c1"># how deep each tree can go
</span>    <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>  <span class="c1"># minimum samples required at each leaf
</span>    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span>      <span class="c1"># for reproducibility
</span><span class="p">)</span>
<span class="n">rf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Evaluate
</span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">rf</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Random Forest R²: </span><span class="si">{</span><span class="n">metrics</span><span class="p">.</span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Random Forest RMSE: </span><span class="si">{</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">metrics</span><span class="p">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s"> g"</span><span class="p">)</span>
</code></pre></div></div> <h3 id="key-hyperparameters"> <a href="#key-hyperparameters" class="anchor-heading" aria-labelledby="key-hyperparameters"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Key Hyperparameters </h3> <p><strong>Hyperparameters</strong> are settings you choose before training (unlike model parameters like coefficients, which are learned from data):</p> <ul> <li> <p><strong>n_estimators</strong>: Number of trees. More trees = more stable predictions, but slower training. 100-500 is usually enough; returns diminish beyond that.</p> </li> <li> <p><strong>max_depth</strong>: Maximum depth of each tree. Shallower trees are simpler and less prone to overfitting. Try 5-20 for most problems.</p> </li> <li> <p><strong>min_samples_leaf</strong>: Minimum samples required at each leaf node. Higher values prevent the tree from creating leaves with just one or two samples, reducing overfitting.</p> </li> <li> <p><strong>max_features</strong>: Number of features to consider at each split. Default is sqrt(n_features) for classification, n_features/3 for regression. Lower values increase randomness between trees.</p> </li> </ul> <h2 id="comparing-all-our-methods"> <a href="#comparing-all-our-methods" class="anchor-heading" aria-labelledby="comparing-all-our-methods"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Comparing All Our Methods </h2> <p>Let’s see how everything stacks up on the penguin data:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Simple regression
</span><span class="n">simple</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">simple</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">[[</span><span class="s">'flipper_length_mm'</span><span class="p">]],</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">pred</span> <span class="o">=</span> <span class="n">simple</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">[[</span><span class="s">'flipper_length_mm'</span><span class="p">]])</span>
<span class="n">results</span><span class="p">.</span><span class="n">append</span><span class="p">({</span>
    <span class="s">'Method'</span><span class="p">:</span> <span class="s">'Simple regression'</span><span class="p">,</span>
    <span class="s">'R²'</span><span class="p">:</span> <span class="n">metrics</span><span class="p">.</span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">pred</span><span class="p">),</span>
    <span class="s">'RMSE'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">metrics</span><span class="p">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">pred</span><span class="p">))</span>
<span class="p">})</span>

<span class="c1"># Multiple regression
</span><span class="n">multi</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">multi</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">pred</span> <span class="o">=</span> <span class="n">multi</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">results</span><span class="p">.</span><span class="n">append</span><span class="p">({</span>
    <span class="s">'Method'</span><span class="p">:</span> <span class="s">'Multiple regression'</span><span class="p">,</span>
    <span class="s">'R²'</span><span class="p">:</span> <span class="n">metrics</span><span class="p">.</span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">pred</span><span class="p">),</span>
    <span class="s">'RMSE'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">metrics</span><span class="p">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">pred</span><span class="p">))</span>
<span class="p">})</span>

<span class="c1"># Decision tree
</span><span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeRegressor</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">tree</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">pred</span> <span class="o">=</span> <span class="n">tree</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">results</span><span class="p">.</span><span class="n">append</span><span class="p">({</span>
    <span class="s">'Method'</span><span class="p">:</span> <span class="s">'Decision tree'</span><span class="p">,</span>
    <span class="s">'R²'</span><span class="p">:</span> <span class="n">metrics</span><span class="p">.</span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">pred</span><span class="p">),</span>
    <span class="s">'RMSE'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">metrics</span><span class="p">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">pred</span><span class="p">))</span>
<span class="p">})</span>

<span class="c1"># Random forest
</span><span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">rf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">pred</span> <span class="o">=</span> <span class="n">rf</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">results</span><span class="p">.</span><span class="n">append</span><span class="p">({</span>
    <span class="s">'Method'</span><span class="p">:</span> <span class="s">'Random forest'</span><span class="p">,</span>
    <span class="s">'R²'</span><span class="p">:</span> <span class="n">metrics</span><span class="p">.</span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">pred</span><span class="p">),</span>
    <span class="s">'RMSE'</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">metrics</span><span class="p">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">pred</span><span class="p">))</span>
<span class="p">})</span>

<span class="k">print</span><span class="p">(</span><span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">).</span><span class="n">to_string</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span>
</code></pre></div></div> <h2 id="whats-driving-the-patterns-feature-importance"> <a href="#whats-driving-the-patterns-feature-importance" class="anchor-heading" aria-labelledby="whats-driving-the-patterns-feature-importance"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> What’s Driving the Patterns? Feature Importance </h2> <p>One of the nicest things about Random Forests is that they tell you which variables matter most for predictions:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">importance</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s">'Variable'</span><span class="p">:</span> <span class="n">X</span><span class="p">.</span><span class="n">columns</span><span class="p">,</span>
    <span class="s">'Importance'</span><span class="p">:</span> <span class="n">rf</span><span class="p">.</span><span class="n">feature_importances_</span>
<span class="p">}).</span><span class="n">sort_values</span><span class="p">(</span><span class="s">'Importance'</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">Variable importance:"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">importance</span><span class="p">.</span><span class="n">to_string</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span>

<span class="c1"># Plot it
</span><span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="n">bar</span><span class="p">(</span><span class="n">importance</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">'Importance'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'Variable'</span><span class="p">,</span> <span class="n">orientation</span><span class="o">=</span><span class="s">'h'</span><span class="p">,</span>
             <span class="n">title</span><span class="o">=</span><span class="s">'What drives penguin body mass?'</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">update_layout</span><span class="p">(</span><span class="n">template</span><span class="o">=</span><span class="s">'simple_white'</span><span class="p">,</span> <span class="n">yaxis</span><span class="o">=</span><span class="p">{</span><span class="s">'categoryorder'</span><span class="p">:</span> <span class="s">'total ascending'</span><span class="p">})</span>
<span class="n">fig</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div> <h3 id="how-is-feature-importance-calculated"> <a href="#how-is-feature-importance-calculated" class="anchor-heading" aria-labelledby="how-is-feature-importance-calculated"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> How Is Feature Importance Calculated? </h3> <p>The default importance measure (called “mean decrease in impurity” or “Gini importance”) is based on how much each feature contributes to reducing prediction error across all trees:</p> <ol> <li>Every time a feature is used to make a split, it reduces the impurity (variance) somewhat</li> <li>Sum up these reductions across all splits and all trees</li> <li>Normalize so importances sum to 1</li> </ol> <p>For the penguin data, you’ll probably find that sex and flipper length are the most important predictors - which makes biological sense!</p> <h3 id="a-caution-about-feature-importance"> <a href="#a-caution-about-feature-importance" class="anchor-heading" aria-labelledby="a-caution-about-feature-importance"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> A Caution About Feature Importance </h3> <p>This default importance measure has known biases:</p> <ul> <li>It favors features with many unique values (continuous &gt; categorical)</li> <li>It favors features that are correlated with other features</li> <li>It doesn’t tell you about the <em>direction</em> of the effect</li> </ul> <p>For more reliable importance estimates, consider <strong>permutation importance</strong>: randomly shuffle one feature and see how much performance drops. If shuffling a feature hurts predictions a lot, that feature was important.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.inspection</span> <span class="kn">import</span> <span class="n">permutation_importance</span>

<span class="n">perm_importance</span> <span class="o">=</span> <span class="n">permutation_importance</span><span class="p">(</span><span class="n">rf</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">n_repeats</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">perm_imp_df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span>
    <span class="s">'Variable'</span><span class="p">:</span> <span class="n">X</span><span class="p">.</span><span class="n">columns</span><span class="p">,</span>
    <span class="s">'Importance'</span><span class="p">:</span> <span class="n">perm_importance</span><span class="p">.</span><span class="n">importances_mean</span>
<span class="p">}).</span><span class="n">sort_values</span><span class="p">(</span><span class="s">'Importance'</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">Permutation importance:"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">perm_imp_df</span><span class="p">.</span><span class="n">to_string</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span>
</code></pre></div></div> <div style="background-color: #f5f5f5; padding: 10px; border-radius: 5px; margin-bottom: 5px;"> <div class="notice--primary"> <h3> <a href="#a-caution-about-feature-importance" class="anchor-heading" aria-labelledby="a-caution-about-feature-importance"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Try It Yourself </h3> <p>Remember the Simpson's paradox problem from Chapter 2? Simple regression showed a <em>negative</em> relationship between bill depth and bill length, but within each species the relationship was positive.</p> <p>Use Random Forest to predict <code>bill_length_mm</code> from bill_depth_mm, flipper_length_mm, body_mass_g, species, and sex. Compare its performance to the multiple regression model. Does Random Forest handle this complexity better?</p> <details> <summary>Solution!</summary> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">LabelEncoder</span>
<span class="kn">import</span> <span class="nn">sklearn.metrics</span> <span class="k">as</span> <span class="n">metrics</span>
<span class="kn">from</span> <span class="nn">palmerpenguins</span> <span class="kn">import</span> <span class="n">load_penguins</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">penguins</span> <span class="o">=</span> <span class="n">load_penguins</span><span class="p">().</span><span class="n">dropna</span><span class="p">()</span>

<span class="c1"># Encode categorical variables
</span><span class="n">le_species</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">le_sex</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">penguins</span><span class="p">[</span><span class="s">'species_code'</span><span class="p">]</span> <span class="o">=</span> <span class="n">le_species</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">penguins</span><span class="p">[</span><span class="s">'species'</span><span class="p">])</span>
<span class="n">penguins</span><span class="p">[</span><span class="s">'sex_code'</span><span class="p">]</span> <span class="o">=</span> <span class="n">le_sex</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">penguins</span><span class="p">[</span><span class="s">'sex'</span><span class="p">])</span>

<span class="c1"># Prepare data - predict bill_length from other features
</span><span class="n">X</span> <span class="o">=</span> <span class="n">penguins</span><span class="p">[[</span><span class="s">'bill_depth_mm'</span><span class="p">,</span> <span class="s">'flipper_length_mm'</span><span class="p">,</span> <span class="s">'body_mass_g'</span><span class="p">,</span> 
              <span class="s">'species_code'</span><span class="p">,</span> <span class="s">'sex_code'</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">penguins</span><span class="p">[</span><span class="s">'bill_length_mm'</span><span class="p">]</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Fit both models
</span><span class="n">lr</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">lr</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">rf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Compare performance
</span><span class="k">print</span><span class="p">(</span><span class="s">"Model Comparison for Predicting Bill Length:"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"-"</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>

<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">model</span> <span class="ow">in</span> <span class="p">[(</span><span class="s">"Linear Regression"</span><span class="p">,</span> <span class="n">lr</span><span class="p">),</span> <span class="p">(</span><span class="s">"Random Forest"</span><span class="p">,</span> <span class="n">rf</span><span class="p">)]:</span>
    <span class="n">pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">r2</span> <span class="o">=</span> <span class="n">metrics</span><span class="p">.</span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">pred</span><span class="p">)</span>
    <span class="n">rmse</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">metrics</span><span class="p">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">pred</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s">: R² = </span><span class="si">{</span><span class="n">r2</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">, RMSE = </span><span class="si">{</span><span class="n">rmse</span><span class="si">:</span><span class="p">.</span><span class="mi">2</span><span class="n">f</span><span class="si">}</span><span class="s"> mm"</span><span class="p">)</span>

<span class="c1"># Feature importance from Random Forest
</span><span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">Random Forest Feature Importance:"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">imp</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">rf</span><span class="p">.</span><span class="n">feature_importances_</span><span class="p">),</span> 
                        <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">imp</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="c1"># Key insight: bill_depth now shows POSITIVE importance!
# Both methods handle the confounding by controlling for species.
</span></code></pre></div> </div> </details> </div> </div><hr /> <h2 id="when-to-use-what"> <a href="#when-to-use-what" class="anchor-heading" aria-labelledby="when-to-use-what"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> When to Use What? </h2> <div class="table-wrapper"><table> <thead> <tr> <th>Method</th> <th>Use When</th> <th>Advantages</th> <th>Disadvantages</th> </tr> </thead> <tbody> <tr> <td><strong>Simple Regression</strong></td> <td>One predictor, linear relationship, need interpretability</td> <td>Simple, coefficients are meaningful</td> <td>Can’t handle multiple predictors or non-linearity</td> </tr> <tr> <td><strong>Multiple Regression</strong></td> <td>Multiple predictors, linear relationships, need to understand effects</td> <td>Coefficients are interpretable, can control for confounders</td> <td>Assumes linearity and additivity</td> </tr> <tr> <td><strong>Decision Tree</strong></td> <td>Need interpretable rules, non-linear relationships</td> <td>Easy to explain, handles non-linearity</td> <td>Prone to overfitting, unstable</td> </tr> <tr> <td><strong>Random Forest</strong></td> <td>Prediction is main goal, complex non-linear relationships</td> <td>Excellent predictive performance, robust</td> <td>Less interpretable, slower to train</td> </tr> </tbody> </table></div><hr /> <h1 id="5-gap-filling-in-time-series"> <a href="#5-gap-filling-in-time-series" class="anchor-heading" aria-labelledby="5-gap-filling-in-time-series"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <strong>5. Gap-filling in Time Series</strong> </h1> <p>Now let’s apply what we’ve learned to a practical problem: dealing with missing data in ecological time series.</p> <h2 id="the-problem"> <a href="#the-problem" class="anchor-heading" aria-labelledby="the-problem"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> The Problem </h2> <p>If you’ve worked with field data, you know this frustration. Your sensor died for a week. The battery ran out during the coldest part of winter. Someone accidentally unplugged the datalogger.</p> <p>Missing data is annoying because:</p> <ul> <li>You can’t calculate annual totals or means</li> <li>It messes up time series analyses</li> <li>Some statistical methods can’t handle NaN values</li> </ul> <h2 id="loading-messy-data"> <a href="#loading-messy-data" class="anchor-heading" aria-labelledby="loading-messy-data"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Loading Messy Data </h2> <p>We will use some data I have prepared in a way that you might find it in an online data portal.<br /> <a href="/New_BAI_DataAnalysis/assets/data/dwd_ahaus_1996_2023_missing_placeholders.parquet">Download the file here</a> To test some things we will work with the air temperature column “tair_2m_mean” here. There are several issues when we have a missing-data-placeholder like that. Try two things: Real data often has placeholder values instead of proper missing data markers. Let’s see an example:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="c1"># Load meteorological data
</span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_parquet</span><span class="p">(</span><span class="s">'./dwd_ahaus_1996_2023_missing_placeholders.parquet'</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s">"data_time"</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">"data_time"</span><span class="p">])</span>

<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Temperature range: </span><span class="si">{</span><span class="n">df</span><span class="p">[</span><span class="s">'tair_2m_mean'</span><span class="p">].</span><span class="nb">min</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s"> to </span><span class="si">{</span><span class="n">df</span><span class="p">[</span><span class="s">'tair_2m_mean'</span><span class="p">].</span><span class="nb">max</span><span class="p">()</span><span class="si">:</span><span class="p">.</span><span class="mi">1</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div> <p>If you see -999.99 as the minimum, that’s a placeholder for missing data - not an actual temperature! We need to fix this:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Replace placeholder with NaN
</span><span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df</span><span class="p">[</span><span class="s">"tair_2m_mean"</span><span class="p">]</span> <span class="o">==</span> <span class="o">-</span><span class="mf">999.99</span><span class="p">,</span> <span class="s">"tair_2m_mean"</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">NaN</span>

<span class="c1"># Now check
</span><span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Missing values: </span><span class="si">{</span><span class="n">df</span><span class="p">[</span><span class="s">'tair_2m_mean'</span><span class="p">].</span><span class="n">isna</span><span class="p">().</span><span class="nb">sum</span><span class="p">()</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div> <h2 id="method-1-linear-interpolation"> <a href="#method-1-linear-interpolation" class="anchor-heading" aria-labelledby="method-1-linear-interpolation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Method 1: Linear Interpolation </h2> <p>The simplest approach - just draw a straight line between known values:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Pandas makes this easy
</span><span class="n">df</span><span class="p">[</span><span class="s">'temp_interp'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'tair_2m_mean'</span><span class="p">].</span><span class="n">interpolate</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s">'linear'</span><span class="p">)</span>
</code></pre></div></div> <p>This works fine for short gaps. If temperature was 10°C at noon and 14°C at 2pm, it’s reasonable to guess 12°C at 1pm.</p> <p>But it fails badly for longer gaps. It can’t capture the daily temperature cycle - if you have a 24-hour gap, linear interpolation will give you a flat line right through where the daily max and min should be.</p> <h2 id="method-2-regression-based-gap-filling"> <a href="#method-2-regression-based-gap-filling" class="anchor-heading" aria-labelledby="method-2-regression-based-gap-filling"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Method 2: Regression-Based Gap Filling </h2> <p>If we have other variables that were measured continuously, we can use them to estimate the missing temperatures:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="c1"># Solar radiation and humidity are often available when temperature fails
# (different sensors)
</span><span class="n">predictors</span> <span class="o">=</span> <span class="p">[</span><span class="s">'SWIN'</span><span class="p">,</span> <span class="s">'rH'</span><span class="p">]</span>

<span class="c1"># Get data where everything is present (for training)
</span><span class="n">df_complete</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s">'data_time'</span><span class="p">,</span> <span class="s">'SWIN'</span><span class="p">,</span> <span class="s">'rH'</span><span class="p">,</span> <span class="s">'tair_2m_mean'</span><span class="p">]].</span><span class="n">dropna</span><span class="p">()</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">df_complete</span><span class="p">[</span><span class="n">predictors</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df_complete</span><span class="p">[</span><span class="s">'tair_2m_mean'</span><span class="p">]</span>

<span class="c1"># Fit model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Gap-filling model R²: </span><span class="si">{</span><span class="n">model</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div> <p>Then we can predict temperature wherever we have radiation and humidity data:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Find rows where temp is missing but predictors exist
</span><span class="n">mask</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'tair_2m_mean'</span><span class="p">].</span><span class="n">isna</span><span class="p">()</span> <span class="o">&amp;</span> <span class="n">df</span><span class="p">[</span><span class="s">'SWIN'</span><span class="p">].</span><span class="n">notna</span><span class="p">()</span> <span class="o">&amp;</span> <span class="n">df</span><span class="p">[</span><span class="s">'rH'</span><span class="p">].</span><span class="n">notna</span><span class="p">()</span>
<span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">mask</span><span class="p">,</span> <span class="s">'temp_regression'</span><span class="p">]</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">mask</span><span class="p">,</span> <span class="n">predictors</span><span class="p">])</span>
</code></pre></div></div> <h2 id="method-3-random-forest-gap-filling"> <a href="#method-3-random-forest-gap-filling" class="anchor-heading" aria-labelledby="method-3-random-forest-gap-filling"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Method 3: Random Forest Gap Filling </h2> <p>For better accuracy, especially with complex patterns, Random Forest often wins:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>

<span class="c1"># Use more predictors
</span><span class="n">all_predictors</span> <span class="o">=</span> <span class="p">[</span><span class="s">'SWIN'</span><span class="p">,</span> <span class="s">'rH'</span><span class="p">,</span> <span class="s">'pressure_air'</span><span class="p">,</span> <span class="s">'wind_speed'</span><span class="p">,</span> <span class="s">'precipitation'</span><span class="p">]</span>
<span class="n">df_complete</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">all_predictors</span> <span class="o">+</span> <span class="p">[</span><span class="s">'tair_2m_mean'</span><span class="p">]].</span><span class="n">dropna</span><span class="p">()</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">df_complete</span><span class="p">[</span><span class="n">all_predictors</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df_complete</span><span class="p">[</span><span class="s">'tair_2m_mean'</span><span class="p">]</span>

<span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">15</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">rf</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Random Forest R²: </span><span class="si">{</span><span class="n">rf</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

<span class="c1"># Check which predictors matter most for estimating temperature
</span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">imp</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">all_predictors</span><span class="p">,</span> <span class="n">rf</span><span class="p">.</span><span class="n">feature_importances_</span><span class="p">),</span> 
                        <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"  </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">imp</span><span class="si">:</span><span class="p">.</span><span class="mi">3</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div> <h2 id="which-method-when"> <a href="#which-method-when" class="anchor-heading" aria-labelledby="which-method-when"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Which Method When? </h2> <p>After working with a lot of gap-filled data, here’s what I’ve found:</p> <p><strong>Short gaps (a few hours):</strong> Linear interpolation is usually fine. Temperature doesn’t change that fast.</p> <p><strong>Medium gaps (a day or two):</strong> Regression with environmental predictors. This captures the daily cycle if you have radiation data.</p> <p><strong>Long gaps (weeks+):</strong> Random Forest or similar, but honestly… consider whether you should be filling such long gaps at all. Sometimes it’s better to acknowledge the data is missing.</p> <p><strong>General advice:</strong></p> <ul> <li>Always validate your gap-filling on data where you know the truth</li> <li>Flag gap-filled values in your final dataset</li> <li>Report the uncertainty or error in your gap-filled values</li> <li>Don’t over-fill - sometimes missing data should stay missing</li> </ul> <div style="background-color: #f5f5f5; padding: 10px; border-radius: 5px; margin-bottom: 5px;"> <div class="notice--primary"> <h3> <a href="#which-method-when" class="anchor-heading" aria-labelledby="which-method-when"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Try It Yourself </h3> <p>Compare linear interpolation vs. Random Forest for filling a 24-hour gap in temperature data. Which method captures the daily cycle better?</p> <details> <summary>Solution!</summary> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># The key insight is that linear interpolation can't capture 
# diurnal patterns, while Random Forest (using radiation as 
# a predictor) can.
</span>
<span class="c1"># For a 24-hour gap:
# - Linear interpolation draws a flat line
# - Random Forest predicts warm during day, cool at night
#   (because it learned that high radiation = high temp)
</span>
<span class="c1"># In my experience, Random Forest reduces RMSE by 30-50% 
# compared to linear interpolation for day-long gaps.
</span>
<span class="c1"># But for gaps under 3-6 hours, the methods are often similar
# because temperature hasn't changed much anyway.
</span></code></pre></div> </div> </details> </div> </div><hr /> <h1 id="wrapping-up"> <a href="#wrapping-up" class="anchor-heading" aria-labelledby="wrapping-up"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <strong>Wrapping Up</strong> </h1> <p>We’ve covered a lot of ground here. Let me leave you with the key takeaways:</p> <p><strong>Simple regression</strong> is your starting point. It’s easy to understand, easy to explain, and often good enough for straightforward questions.</p> <p><strong>Multiple regression</strong> lets you account for multiple drivers at once. The coefficients tell you the effect of each variable while controlling for the others.</p> <p><strong>Random Forests</strong> can capture complex patterns that regression misses. They’re particularly good when you don’t know the shape of the relationships in advance.</p> <p><strong>For gap-filling</strong>, match your method to your gap length. Simple interpolation for short gaps, model-based methods for longer ones.</p> <p><strong>Most importantly:</strong> always plot your data, check your assumptions, and validate on independent test data. No amount of fancy statistics can fix bad data or inappropriate models.</p> <p>Good luck with your analyses!</p> <h2 id="where-to-go-from-here"> <a href="#where-to-go-from-here" class="anchor-heading" aria-labelledby="where-to-go-from-here"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Where to Go From Here </h2> <p>If you want to dig deeper:</p> <ul> <li><strong>Generalized Additive Models (GAMs)</strong> let you fit smooth curves instead of straight lines</li> <li><strong>Mixed-effects models</strong> handle hierarchical data (e.g., measurements nested within sites)</li> <li><strong>Gradient Boosting (XGBoost)</strong> often outperforms Random Forests for prediction</li> <li><strong>Time series methods</strong> (ARIMA, etc.) are specifically designed for temporal data</li> </ul> <p>But honestly, you can get surprisingly far with just regression and Random Forests. Master these first before moving on to fancier tools.</p> </main> </div> </div> <div class="search-overlay"></div> </div> </body> </html>
