<!DOCTYPE html> <html lang="en-US"> <head> <meta charset="UTF-8"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link rel="stylesheet" href="/New_BAI_DateAnalysis/assets/css/just-the-docs-default.css"> <link rel="stylesheet" href="/New_BAI_DateAnalysis/assets/css/just-the-docs-head-nav.css" id="jtd-head-nav-stylesheet"> <style id="jtd-nav-activation"> .site-nav > ul.nav-list:first-child > li:not(:nth-child(5)) > a, .site-nav > ul.nav-list:first-child > li > ul > li a { background-image: none; } .site-nav > ul.nav-list:not(:first-child) a, .site-nav li.external a { background-image: none; } .site-nav > ul.nav-list:first-child > li:nth-child(5) > a { font-weight: 600; text-decoration: none; }.site-nav > ul.nav-list:first-child > li:nth-child(5) > button svg { transform: rotate(-90deg); }.site-nav > ul.nav-list:first-child > li.nav-list-item:nth-child(5) > ul.nav-list { display: block; } </style> <script src="/New_BAI_DateAnalysis/assets/js/vendor/lunr.min.js"></script> <script src="/New_BAI_DateAnalysis/assets/js/just-the-docs.js"></script> <meta name="viewport" content="width=device-width, initial-scale=1"> <!-- Begin Jekyll SEO tag v2.8.0 --> <title>INTERPOLATION AND GAP FILLING | Data Analysis for Ecologist</title> <meta name="generator" content="Jekyll v3.10.0" /> <meta property="og:title" content="INTERPOLATION AND GAP FILLING" /> <meta property="og:locale" content="en_US" /> <meta name="description" content="A tutorial for starters learning how to use python to munipulate climate data!" /> <meta property="og:description" content="A tutorial for starters learning how to use python to munipulate climate data!" /> <link rel="canonical" href="https://ericluoyuu.github.io/New_BAI_DateAnalysis/python_3_interpolation_gapfilling.html" /> <meta property="og:url" content="https://ericluoyuu.github.io/New_BAI_DateAnalysis/python_3_interpolation_gapfilling.html" /> <meta property="og:site_name" content="Data Analysis for Ecologist" /> <meta property="og:type" content="website" /> <meta name="twitter:card" content="summary" /> <meta property="twitter:title" content="INTERPOLATION AND GAP FILLING" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"WebPage","description":"A tutorial for starters learning how to use python to munipulate climate data!","headline":"INTERPOLATION AND GAP FILLING","url":"https://ericluoyuu.github.io/New_BAI_DateAnalysis/python_3_interpolation_gapfilling.html"}</script> <!-- End Jekyll SEO tag --> </head> <body> <a class="skip-to-main" href="#main-content">Skip to main content</a> <svg xmlns="http://www.w3.org/2000/svg" class="d-none"> <symbol id="svg-link" viewBox="0 0 24 24"> <title>Link</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-link"> <path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path> </svg> </symbol> <symbol id="svg-menu" viewBox="0 0 24 24"> <title>Menu</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"> <line x1="3" y1="12" x2="21" y2="12"></line><line x1="3" y1="6" x2="21" y2="6"></line><line x1="3" y1="18" x2="21" y2="18"></line> </svg> </symbol> <symbol id="svg-arrow-right" viewBox="0 0 24 24"> <title>Expand</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-right"> <polyline points="9 18 15 12 9 6"></polyline> </svg> </symbol> <!-- Feather. MIT License: https://github.com/feathericons/feather/blob/master/LICENSE --> <symbol id="svg-external-link" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-external-link"> <title id="svg-external-link-title">(external link)</title> <path d="M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6"></path><polyline points="15 3 21 3 21 9"></polyline><line x1="10" y1="14" x2="21" y2="3"></line> </symbol> <symbol id="svg-doc" viewBox="0 0 24 24"> <title>Document</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file"> <path d="M13 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V9z"></path><polyline points="13 2 13 9 20 9"></polyline> </svg> </symbol> <symbol id="svg-search" viewBox="0 0 24 24"> <title>Search</title> <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-search"> <circle cx="11" cy="11" r="8"></circle><line x1="21" y1="21" x2="16.65" y2="16.65"></line> </svg> </symbol> <!-- Bootstrap Icons. MIT License: https://github.com/twbs/icons/blob/main/LICENSE.md --> <symbol id="svg-copy" viewBox="0 0 16 16"> <title>Copy</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard" viewBox="0 0 16 16"> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1h1a1 1 0 0 1 1 1V14a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V3.5a1 1 0 0 1 1-1h1v-1z"/> <path d="M9.5 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3zm-3-1A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3z"/> </svg> </symbol> <symbol id="svg-copied" viewBox="0 0 16 16"> <title>Copied</title> <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-clipboard-check-fill" viewBox="0 0 16 16"> <path d="M6.5 0A1.5 1.5 0 0 0 5 1.5v1A1.5 1.5 0 0 0 6.5 4h3A1.5 1.5 0 0 0 11 2.5v-1A1.5 1.5 0 0 0 9.5 0h-3Zm3 1a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5h-3a.5.5 0 0 1-.5-.5v-1a.5.5 0 0 1 .5-.5h3Z"/> <path d="M4 1.5H3a2 2 0 0 0-2 2V14a2 2 0 0 0 2 2h10a2 2 0 0 0 2-2V3.5a2 2 0 0 0-2-2h-1v1A2.5 2.5 0 0 1 9.5 5h-3A2.5 2.5 0 0 1 4 2.5v-1Zm6.854 7.354-3 3a.5.5 0 0 1-.708 0l-1.5-1.5a.5.5 0 0 1 .708-.708L7.5 10.793l2.646-2.647a.5.5 0 0 1 .708.708Z"/> </svg> </symbol> </svg> <div class="side-bar"> <div class="site-header" role="banner"> <a href="/New_BAI_DateAnalysis/" class="site-title lh-tight"> Data Analysis for Ecologist </a> <button id="menu-button" class="site-button btn-reset" aria-label="Toggle menu" aria-pressed="false"> <svg viewBox="0 0 24 24" class="icon" aria-hidden="true"><use xlink:href="#svg-menu"></use></svg> </button> </div> <nav aria-label="Main" id="site-nav" class="site-nav"> <ul class="nav-list"><li class="nav-list-item"><a href="/New_BAI_DateAnalysis/" class="nav-list-link">Home</a></li><li class="nav-list-item"><a href="/New_BAI_DateAnalysis/python_0_installation.html" class="nav-list-link">0. INSTALLATION</a></li><li class="nav-list-item"><a href="/New_BAI_DateAnalysis/python_1_basics.html" class="nav-list-link">1. BASICS OF PYTHON</a></li><li class="nav-list-item"><a href="/New_BAI_DateAnalysis/python_2_data_visualization.html" class="nav-list-link">2. DATA HANDLING AND VISUALIZATION</a></li><li class="nav-list-item"><a href="/New_BAI_DateAnalysis/python_3_interpolation_gapfilling.html" class="nav-list-link">3. INTERPOLATION AND GAP FILLING</a></li><li class="nav-list-item"><a href="/New_BAI_DateAnalysis/python_4_extreme_detection.html" class="nav-list-link">4. EXTREME VALUE DETECTION</a></li></ul> </nav> <footer class="site-footer"> This site uses <a href="https://github.com/just-the-docs/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll. </footer> </div> <div class="main" id="top"> <div id="main-header" class="main-header"> <div class="search" role="search"> <div class="search-input-wrap"> <input type="text" id="search-input" class="search-input" tabindex="0" placeholder="Search Data Analysis for Ecologist" aria-label="Search Data Analysis for Ecologist" autocomplete="off"> <label for="search-input" class="search-label"><svg viewBox="0 0 24 24" class="search-icon"><use xlink:href="#svg-search"></use></svg></label> </div> <div id="search-results" class="search-results"></div> </div> <nav aria-label="Auxiliary" class="aux-nav"> <ul class="aux-nav-list"> <li class="aux-nav-list-item"> <a href="https://github.com/just-the-docs/just-the-docs-template" class="site-button" > Template Repository </a> </li> </ul> </nav> </div> <div class="main-content-wrap"> <div id="main-content" class="main-content"> <main> <h2 id="interpolation-and-gap-filling"> <a href="#interpolation-and-gap-filling" class="anchor-heading" aria-labelledby="interpolation-and-gap-filling"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> <strong>Interpolation and Gap Filling</strong> </h2> <p>In this exercise we look at interpolation and the rather time-series specific topic of data-gap filling.<br /> In time-series data we often have gaps due to a variety of reasons. They can result from instrumental issues or maintenance times or unfavorable weather conditions which leads to data being discarded. These data gaps can be filled with statistical methods.</p> <p>In this lesson exercises are not completely separated from the content. Just follow along, grab the code and in some parts you will get snippets to run and fiddle with yourself.</p> <p>Before we start plotting data we will see, how we can deal with missing values which are already handled by the institution measuring the data, e.g. the DWD. For example it is common that the data is included with a specific placeholder value, which we first need to handle.</p> <h3 id="1-loading-and--converting-data"> <a href="#1-loading-and--converting-data" class="anchor-heading" aria-labelledby="1-loading-and--converting-data"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 1. Loading and converting data: </h3> <p>We will use some data I have prepared in a way that you might find it in an online data portal.<br /> <a href="https://nicbehr.github.io/new_bai_data_analysis/assets/data/dwd_diepholz_1996_2023_missing_placeholders.parquet">Download the file here</a></p> <p>To test some things we will work with the air temperature column “tair_2m_mean” here. There are several issues when we have a missing-data-placeholder like that. Try two things:</p> <div class="notice--primary"> <h3> <a href="#1-loading-and--converting-data" class="anchor-heading" aria-labelledby="1-loading-and--converting-data"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Exercise </h3> <p>Look at a quick express plot of the data. Is that a meaningful representation? Then try to resample this data to daily values. Plot the data, do the values make sense?</p> <details> <summary>Solution!</summary> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_dwd</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_parquet</span><span class="p">(</span><span class="o">&lt;</span><span class="n">path</span> <span class="n">to</span> <span class="nb">file</span><span class="o">&gt;</span><span class="p">)</span>
<span class="n">df_dwd</span><span class="p">[</span><span class="s">"date_time"</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">to_datetime</span><span class="p">(</span><span class="n">df_dwd</span><span class="p">[</span><span class="s">"date_time"</span><span class="p">])</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_dwd</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="s">"ta_2m_mean"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div> </div> <p>Plotting the data with the missing value placeholder makes the data barely readable:<br /> <img src="/New_BAI_DateAnalysis/assets/images/python/3/missing_values.png" alt="Image of data with missing values" /></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_dwd_daily</span> <span class="o">=</span> <span class="n">df_dwd</span><span class="p">.</span><span class="n">resample</span><span class="p">(</span><span class="n">rule</span><span class="o">=</span><span class="s">"d"</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="s">"date_time"</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_dwd_daily</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="s">"tair_2m_mean"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div> </div> <p>When averaging the values, the -999.99 values are taken into account leading to unrealistic results:<br /> <img src="/New_BAI_DateAnalysis/assets/images/python/3/bad_averaged_data.png" alt="Image of poorly average data" /></p> </details> </div> <p>Lets look at the first way to solve this issue. We have to find the rows, where the values are the placeholder value. You can identify these rows by grabbing the right-row indices of the dataframe with a condition:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># we find the indices in the column of air temperature, where
# the values are -999.99:
</span>
<span class="c1">#                                     |  here we find the rows we need:    | column we need|
#                                     | those where the row tair is -999.99|
#                                                        |                      |
#                                                        v                      v
</span><span class="n">indices_of_missing_values</span> <span class="o">=</span> <span class="n">df_dwd</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">df_dwd</span><span class="p">[</span><span class="s">"tair_2m_mean"</span><span class="p">]</span> <span class="o">==</span> <span class="o">-</span><span class="mf">999.99</span><span class="p">,</span> <span class="s">"tair_2m_mean"</span><span class="p">].</span><span class="n">index</span>
</code></pre></div></div> <p>We can then go ahead and replace these values with a special type, a “NaN”-value. NaN stands for Not a Number and represents specifically missing numeric values. This object is provided by the numpy package and works nicely with pandas:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span> <span class="c1"># do this if you have not already imported numpy
</span><span class="n">df_dwd</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">indices_of_missing_values</span><span class="p">,</span> <span class="s">"tair_2m_mean"</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">NaN</span>
</code></pre></div></div> <p>Now the irritating -888.88 values are replaced and you can easily plot and resample the data in a meaningful manner:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_dwd</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">"tair_2m_mean"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

<span class="n">df_dwd_daily</span> <span class="o">=</span> <span class="n">df_dwd</span><span class="p">.</span><span class="n">resample</span><span class="p">(</span><span class="n">rule</span><span class="o">=</span><span class="s">"d"</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="s">"date_time"</span><span class="p">).</span><span class="n">mean</span><span class="p">()</span>
<span class="n">fig_daily</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df_dwd_daily</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">"tair_2m_mean"</span><span class="p">)</span>
<span class="n">fig_daily</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

</code></pre></div></div> <h3 id="2-gap-filling-interpolation-and-modelling"> <a href="#2-gap-filling-interpolation-and-modelling" class="anchor-heading" aria-labelledby="2-gap-filling-interpolation-and-modelling"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 2. Gap Filling, interpolation and modelling </h3> <p>In the next part we will discuss how we can work with timeseries that have gaps of different sizes. This is a regular task when working with long-time observations and there are a couple of options, depending on what data is available to you and what is the final evaluation goal you have in mind.</p> <h4 id="21-simple-linear-interpolation"> <a href="#21-simple-linear-interpolation" class="anchor-heading" aria-labelledby="21-simple-linear-interpolation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 2.1: Simple linear interpolation </h4> <p>You do basic interpolation in your every day live. You want to bake a cake and only find a receipe for an 8 person cake, but only 3 friends are coming over for cake time. In the receipe you have to use 1 kg of flour. Intuitively you can see that since you will only be four at the table, you can alter the receipe and only use 500 g of flour. And already did you do some interpolation! What you easily did right away in your head could be mathematically formulated as: y = 125 * x where y is the amount of flour in grams and x is the number of people eating cake.</p> <p>The formula for an interpolation between two points (x1,y1) and (x2,y2) at a specific point (xn, yn) is:</p> <div> $$ yn = y1 + \frac{(y_{2}-y_{1})}{(x_{2}-x_{1})} * (x_{n} - x_{1}) $$ </div> <p>We simply construct a straight line where y1 is our y-intercept, the slope is derived from the two points with the well known slope-formula</p> <div> $$ m = (y2-y1)/(x2-x1) $$ </div> <p>and our x value on this constructed line is difference between the point we want to look at minus the starting point</p> <p>Note that in this form of y = mx + b we only have one x which we use to explain our y-value. We have one “predictor”. Using only one predictor gives us a so called simple linear regression. This is a super simple form of interpolation and of course leaves a lot of information aside.</p> <p>Lets look at a simple example of how to actually do linear interpolation in Python:</p> <p>First we create a data set to play with. We create a simple running index from 1 to 11 and some made up y-values. We make one array in which all values are present and a second in which some values are missing. :</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">index</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">11</span><span class="p">]</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">"full_data"</span> <span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">13</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">19</span><span class="p">,</span><span class="mi">15</span><span class="p">,</span><span class="mi">13</span><span class="p">,</span><span class="mi">21</span><span class="p">,</span><span class="mi">27</span><span class="p">],</span>
    <span class="s">"missing_data"</span> <span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">np</span><span class="p">.</span><span class="n">NaN</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="n">np</span><span class="p">.</span><span class="n">NaN</span><span class="p">,</span><span class="mi">19</span><span class="p">,</span><span class="mi">15</span><span class="p">,</span><span class="mi">13</span><span class="p">,</span><span class="n">np</span><span class="p">.</span><span class="n">NaN</span><span class="p">,</span><span class="mi">27</span><span class="p">]</span>
<span class="p">}</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">index</span> <span class="o">=</span> <span class="n">index</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">)</span>
</code></pre></div></div> <p>lets take a quick look at the two datasets:</p> <p>First we create as simple plot to look at the characteristics of the data. Lets make a quick little function to keep a bit of styling:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">plotly.express</span> <span class="k">as</span> <span class="n">px</span>
<span class="k">def</span> <span class="nf">scatter_plot_interp</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="p">:</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">show</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">columns</span><span class="p">)</span>
    <span class="n">fig</span><span class="p">.</span><span class="n">update_traces</span><span class="p">(</span><span class="n">marker_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">fig</span><span class="p">.</span><span class="n">update_layout</span><span class="p">(</span><span class="n">template</span><span class="o">=</span><span class="s">"simple_white"</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">show</span><span class="p">:</span>
      <span class="n">fig</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">fig</span>
<span class="n">scatter_plot_interp</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="p">[</span><span class="s">"full_data"</span><span class="p">,</span> <span class="s">"missing_data"</span><span class="p">])</span>
</code></pre></div></div> <p>Since the two plots overlay each other, you can see the “missing” values in blue and all the ones in the reduced dataset in red.</p> <p>To do a linear interpolation between each adjacent points you can use a numpy function, np.interp() or a built-in pandas method. You can find its documentation <a href="https://numpy.org/doc/stable/reference/generated/numpy.interp.html">here</a>.<br /> The function takes 3 main arguments:</p> <ol> <li>The x-coordinates for which the data shall be interpolated</li> <li>The x-coordinates of the input data</li> <li>The y-values of the input data</li> </ol> <p>The catch with the numpy function however is that the function will return NaN if there are NaN-values in the input arrays. The pandas function is a lot easier, but we will have to deal more with this problem of getting rid of NaN values later when we use other models, so we can practice getting rid of NaN data in our training data now anyways.<br /> Specifically we will do the following:</p> <ul> <li>find indices of NaN values</li> <li>find indices of non-NaN-values</li> <li>first argument is where to interpolate, so provide the indices of the NaN values</li> <li>second and third arguments are the x and y values of the adjacent non-NaN values, so provide the index and the y-values at the non-NaN indices</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 1. get indices of missing and present points:
</span>
<span class="n">indices_of_missing_points</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="s">"missing_data"</span><span class="p">].</span><span class="n">isna</span><span class="p">()].</span><span class="n">index</span>
<span class="n">indices_of_present_points</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="s">"missing_data"</span><span class="p">].</span><span class="n">notna</span><span class="p">()].</span><span class="n">index</span>

<span class="c1"># 2. interpolate missing values and store them in 
# a new column in the dataframe. We can either do this with
# the numpy function np.interp:
</span><span class="n">data</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">indices_of_missing_points</span><span class="p">,</span><span class="s">"interpolated_data"</span><span class="p">]</span><span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">interp</span><span class="p">(</span><span class="n">indices_of_missing_points</span><span class="p">,</span> 
          <span class="n">data</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">indices_of_present_points</span><span class="p">,</span><span class="s">"missing_data"</span><span class="p">].</span><span class="n">index</span><span class="p">,</span> 
          <span class="n">data</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">indices_of_present_points</span><span class="p">,</span><span class="s">"missing_data"</span><span class="p">])</span>

<span class="c1"># the pandas approach is much easier to use and is simply:
</span><span class="n">data</span><span class="p">[</span><span class="s">"interpolated_data"</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s">"missing_data"</span><span class="p">].</span><span class="n">interpolate</span><span class="p">()</span>
</code></pre></div></div> <p>In this approach all we did was to draw straight lines between adjacent points. As you see, for the first point the prediction was rather poor, the other two where pretty well reconstructed:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig</span> <span class="o">=</span> <span class="n">scatter_plot_interp</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="p">[</span><span class="s">"full_data"</span><span class="p">,</span> <span class="s">"interpolated_data"</span><span class="p">])</span>
</code></pre></div></div> <p>However, with this approach we leave all the information the other points give us about the data aside. Imagine for example that you have a timeseries where you measure temperature at midnight and at 12AM. If one datapoint was missing, you would connect the two night time temperatures and interpolate the daytime temperature way off.</p> <p>A simple measure of how well our model performed is to look at the root mean squared error.</p> <div> $$ RMSE = \sqrt{\overline{(y[i] - ypred[i])^2}} $$ </div> <p>where y is the true value and ypred is the predicted y value. You simply compute the model error for each datapoint and square them to avoid counter balancing of negative and positive errors. Then you take the mean of these values and finally take the square root, to get back into your data range.</p> <div class="notice--primary"> <h3> <a href="#21-simple-linear-interpolation" class="anchor-heading" aria-labelledby="21-simple-linear-interpolation"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Exercise </h3> <p>Use your knowledge of pandas and numpy to write a function that returns the RMSE</p> <details> <summary>Solution!</summary> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_RMSE</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_predicted</span><span class="p">):</span>
    <span class="n">RMSE</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">((</span><span class="n">y_true</span> <span class="o">-</span> <span class="n">y_predicted</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">RMSE</span>

<span class="n">y_true</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">indices_of_missing_points</span><span class="p">,</span> <span class="s">"full_data"</span><span class="p">]</span>
<span class="n">y_predicted</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">indices_of_missing_points</span><span class="p">,</span> <span class="s">"interpolated_data"</span><span class="p">]</span>
<span class="n">RMSE</span> <span class="o">=</span> <span class="n">get_RMSE</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_predicted</span><span class="p">)</span>
</code></pre></div> </div> </details> </div> <h4 id="22-simple-linear-models"> <a href="#22-simple-linear-models" class="anchor-heading" aria-labelledby="22-simple-linear-models"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 2.2: Simple linear models </h4> <p>Another approach is be to create a linear model that builds not only on the two points adjacent to the one we want to know, but rather the whole of the dataset that we have available.</p> <p>So what we want to achieve, is to find a function that constructs our unknown data points based on the data we have available in the best possible way. That means, that we want to have as little errors in our model as possible. The error is usually measured as the “sum of squared errors” (SSE) which is the total of distances between true values and the predicted values. We square it to avoid negative and positive values counterbalancing each other.</p> <p>Looking at an array of n data points we can write</p> <div> $$ SSE = \sum_{i=1}^n (y(i) - b - m * x(i))^2 $$ </div> <p>y(i) is the true y value at the predicted point, b is the y-intercept of the linear model, m is the first coefficient of the linear model and x(i) is the x-value at the predicted point.</p> <p>Since we want to find the straight line, that MINIMIZES the SSE, we call a procedure like this a “minimization problem” and specifically the estimation of this line is called a “least squares estimation”.</p> <p>In the easiest way of fitting a linear model to such a dataset, it all depends on the mean of our dataset. To derive the model parameters we can use the following relations where we replace b with alpha and m with beta (as that is the general standard). Also we will now denote the predicted y-value with a ^ on top of that, which is the common standard in literature. Sometimes this is also referred to as y_hat.</p> <div> $$ \hat{y}_{i} = \alpha + \beta * x_{i} $$ </div> <div> $$ \alpha = \bar{y} - (m \bar{x}) $$ </div> <div> $$ \beta = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2} $$ </div> <p>If we would do it by hand, we would simply plug in all the numbers we have into the expression for beta and use the result to derive our alpha</p> <p>But we are working with Python so we will now introduce an awesome modelling and machine-learning library called “scikit-learn”.<br /> Scikit-learn has a huge amount of model-packages available, from simple linear regression all the way advanced statistical regressions, classifications and analysis tools. <a href="https://scikit-learn.org/stable/index.html">The documentation is also quite nice and extensive!</a><br /> We will make use of scikit-learn to fit a simple linear regression model to our data. However to do so we need to some tweaking of our data. Especially two things are important:</p> <ol> <li>Scikit learn can not work with NaN-data. That means we need to filter these out of the data we feed to the linear model</li> <li>The model needs two-dimensional data. A list like [1,2,3] is one-dimensional and does not work with creating linear models. Instead we have to bring it to a form of [[1],[2],[3]] etc. We can do this using the “reshape” function.</li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="c1"># Scikit learn is quite object oriented. That means,
# we imported a Class called "LinearRegression", which contains
# all the following functions to work with the model.
</span>
<span class="c1"># Step 1: instantiate the class
</span><span class="n">linearModel</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>

<span class="c1"># Step 2: Fit the model. This is the process of 
# feeding our known data to the model and tweaking the
# parameters so that the prediction error gets minimized.
# However, sklearn can not work with NaN values, so again
# we need to leave them out in the fitting:
</span>
<span class="c1"># --- repetition from before:
</span><span class="n">indices_of_present_points</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">data</span><span class="p">[</span><span class="s">"missing_data"</span><span class="p">].</span><span class="n">notna</span><span class="p">()].</span><span class="n">index</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">indices_of_present_points</span><span class="p">,</span><span class="s">"missing_data"</span><span class="p">].</span><span class="n">index</span><span class="p">.</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">indices_of_present_points</span><span class="p">,</span><span class="s">"missing_data"</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="c1"># --- fitting the model:
</span><span class="n">linearModel</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># Step 3: Check how well our model performed! sklearn has an inbuilt
# function for it called "score". It returns the R^2 value for 
# the true values and the values predicted by the model:
</span><span class="n">linearModel</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
</code></pre></div></div> <p>We can obtain the paramters of the linear model, that scikit-learn has created for us:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">m</span> <span class="o">=</span> <span class="n">linearModel</span><span class="p">.</span><span class="n">coef_</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">linearModel</span><span class="p">.</span><span class="n">intercept_</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"Linear equation: </span><span class="si">{</span><span class="n">m</span><span class="si">}</span><span class="s">*x+</span><span class="si">{</span><span class="n">b</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div> <p>The last thing left to do is to use this model to predict our missing values. All we need to do is use the models “predict()” function and give it the indices we want to prediction for:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># first we create a new column consisting of NaN values
</span><span class="n">data</span><span class="p">[</span><span class="s">"sklearn_prediction"</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">NaN</span>
<span class="c1"># then we replace the values in the missing rows with our model prediction:
</span><span class="n">linear_prediction</span> <span class="o">=</span> <span class="n">linearModel</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">indices_of_missing_points</span><span class="p">.</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">data</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">indices_of_missing_points</span><span class="p">,</span> <span class="s">"sklearn_prediction"</span><span class="p">]</span> <span class="o">=</span> <span class="n">linear_prediction</span>
</code></pre></div></div> <p>We can look at out interpolated values by plotting them as red dots together with our reduced dataset:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fig_linmod</span> <span class="o">=</span> <span class="n">scatter_plot_interp</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="p">[</span><span class="s">"full_data"</span><span class="p">,</span> <span class="s">"sklearn_prediction"</span><span class="p">],</span> <span class="n">show</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div> <p>As you can see, the linear model already performs a bit better than the simple linear interpolation does. We can visualize the linear regression line by predicting the full array of x-values and plotting the result as a line:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span><span class="p">[</span><span class="s">"yhat_full"</span><span class="p">]</span> <span class="o">=</span> <span class="n">linearModel</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">index</span><span class="p">.</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
<span class="n">fig_linmod</span><span class="p">.</span><span class="n">add_traces</span><span class="p">(</span>
    <span class="n">go</span><span class="p">.</span><span class="n">Scatter</span><span class="p">(</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">index</span><span class="p">,</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s">"yhat_full"</span><span class="p">],</span>
        <span class="n">mode</span><span class="o">=</span><span class="s">"lines"</span><span class="p">,</span>
        <span class="n">name</span><span class="o">=</span><span class="s">"linear regression line"</span>
    <span class="p">)</span>
<span class="p">)</span>
<span class="n">fig_linmod</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div> <p>Finally we need to look at statistical metrics to find out, how well our linear model performed. Luckily we can easily get a whole range of such metrics from the sklearn.metrics package. Lets define a simple function to grab a bunch of metrics at once:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="kn">import</span> <span class="nn">sklearn.metrics</span> <span class="k">as</span> <span class="n">metrics</span>
<span class="k">def</span> <span class="nf">regression_results</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>

    <span class="c1"># Regression metrics
</span>    <span class="n">mse</span><span class="o">=</span><span class="n">metrics</span><span class="p">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span> 
    <span class="n">median_absolute_error</span><span class="o">=</span><span class="n">metrics</span><span class="p">.</span><span class="n">median_absolute_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="n">r2</span><span class="o">=</span><span class="n">metrics</span><span class="p">.</span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="s">'r^2: '</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">r2</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'MAE: '</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">median_absolute_error</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'MSE: '</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">mse</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'RMSE: '</span><span class="p">,</span> <span class="nb">round</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mse</span><span class="p">),</span><span class="mi">4</span><span class="p">))</span>
</code></pre></div></div> <p>A few things we can take from this are:<br /> <strong>a)</strong> r^2 is 0.8351, which is the ratio of the sum of squared errors divided by the sum of squared deviations from the mean. You can say that r^2 is a measure of how much of the variance in the original data is reflected by the model. In this case, as our model is just a line, the amount of variance captured in the model stems from the linear trend that is inherent in the original data.</p> <p>Whether an r^2 is reflective of a good correlation depends heavily on the application. If you are a social scientist and work on voter behaviour an r_square of 0.65 may be spectacularly good. If you want to calibrate your measurement device and the reference and measured values have an r^2 of less than 0.85 you might want to check it again…</p> <p><strong>b)</strong> The mean squared error (MSE) is exactly that: we calculate the distance from each datapoint to its predicted counterpart, and to avoid negative errors counterbalancing positive ones, we square them. Then we take the mean of all errors. Due to the squaring the errors get quite high and are not directly interpretable. That is why we take the square root of the squared errors and get to the “root mean square error” (RMSE). This is an error very often reported in model performance evaluation, also often used in scientific papers.</p> <p><strong>c)</strong> Lastly the median absolute error (MAE) is a different performance metric that gets shown not as often, but is still very useful. For the MAE, we also calculate each error, take the absolute of it (make negative values positive) and then grab the median value, so the one that sits right in the middle of all datapoints. Because we take the median instead of the mean, this metric is insensitive to outliers. If we have very low errors, but then a few extremely high ones (or the other way around), the mean value can be skewed while the median would not change.</p> <p>We wont go much deeper into statistical metrics here. But as you can see, this model does represent certain characteristics of the data regarding its variance (judging by the rsquare of &gt; 0.8) but has a pretty high average error of more than 4 while we are in a domain of data that only reaches from 1 to 27.</p> <h3 id="part-23-multiple-linear-models"> <a href="#part-23-multiple-linear-models" class="anchor-heading" aria-labelledby="part-23-multiple-linear-models"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Part 2.3: Multiple linear models </h3> <p>Lets look at another way to make our models a bit more flexible So far we created a linear model with only one parameter. Obviously that did not catch all of the variance in our data. In reality we often have more data at hand which can help us explain the measure of interest. For example to fill gaps in temperature data instead of only using the indices to predict, we could add variables such as the incoming radiation or the relative humidity of the air.</p> <p>Lets continue working with the dwd data we used before. Load it just like we did in the previous exercises</p> <p>When we try to simply interpolate with the pointwise linear interpolation, you will see that we get a pretty uninformed output.</p> <p>We will now create a more sophisticated model to reconstruct our missing data. However, this time we have a whole dataset of predictors to choose from.<br /> Since we want to fill a gap in temperature data, we need to find predictors that are well correlated with temperature. To figure out which ones are suitable we can make use of the correlation matrix.<br /> A correlation matrix is a normalized form of a covariance matrix. The values vary between -1 and 1. A value of 1 signals a perfect positive, -1 a perfect negative correlation. 0 means that the two variables are not correlated at all. With pandas you can get the full correlation matrix with all variables with the .corr() function:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># lets look at the correlation matrix
</span><span class="n">df_dwd</span><span class="p">.</span><span class="n">corr</span><span class="p">()</span>
<span class="c1"># you can plot and explore it with plotly.
# The interactivity is really handy here:
</span><span class="n">px</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">df_dwd</span><span class="p">.</span><span class="n">corr</span><span class="p">()).</span><span class="n">show</span><span class="p">()</span>
<span class="c1"># To get all correlations with tair_2m_mean we have to index it:
</span><span class="n">df_dwd</span><span class="p">.</span><span class="n">corr</span><span class="p">()[</span><span class="s">"tair_2m_mean"</span><span class="p">]</span>
</code></pre></div></div> <p>Try to figure out, which variables could be suitable to fill the gaps in our data from the below table.</p> <p>Now we can go ahead and start building our multivariate model! Let go! Before we really start plugging the data into the model we need to do a bit of preparation:<br /> Since the model has to be fit with data where all the predictors we want are present AND we have observation data of our target variable to train the model on, we first need to find that data. We can do that easily by dropping the rows, where these columns are na with “dropna()”:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_dwd_noNA</span> <span class="o">=</span> <span class="n">df_dwd</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,[</span><span class="s">"date_time"</span><span class="p">,</span><span class="s">"SWIN"</span><span class="p">,</span><span class="s">"rH"</span><span class="p">,</span><span class="s">"tair_2m_mean"</span><span class="p">]].</span><span class="n">dropna</span><span class="p">()</span>
</code></pre></div></div> <p>Now we want to split these into the data we use as predictors (y) and the data we want to predict (x, also called the “predictand”):</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">df_dwd_noNA</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,[</span><span class="s">"SWIN"</span><span class="p">,</span><span class="s">"rH"</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df_dwd_noNA</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,[</span><span class="s">"tair_2m_mean"</span><span class="p">]]</span>
</code></pre></div></div> <p>Finally one last very important step is that we need to split our available data into two parts: a training and a testing dataset. The training data will ONLY be used for creating (or “fitting”) the model. To test the performance of the model, we keep a fraction of the available data out of the training set. That way we can predict the testing data and compare it to the real results. We are working with some artificiallly created gaps in the data here, but in real life you would otherwise have no way to test, how well your model actually predicts data.<br /> Additionally to splitting the data, the training datasets also get shuffled. That makes the model more robust in extrapolating it to unknown data.</p> <p>It is extremely important to do this split, because you can never test a model on data that it has already seen during its training phase. That would skew your results and make it look better than it actually is.<br /> Luckily, because this is such a common task to do, scikit learn has us covered with a very simple function to do the splitting:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span> 
<span class="c1"># creating train and test sets 
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span> 
	<span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span> 
</code></pre></div></div> <p>Now that we have our final training and testing datasets ready for use, we can go ahead and fit our model!</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">linearModel</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="c1"># Only training data used for fitting:
</span><span class="n">linearModel</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="c1"># Only testing data used for the score:
</span><span class="n">linearModel</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">)</span>

<span class="c1"># You can plot the prediction for the testing period
# as a scatter plot to get an idea of the spread
# of the errors. Put true values on one axis and predicted on the other:
</span><span class="n">y_hat_ml</span> <span class="o">=</span> <span class="n">linearModel</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">px</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">y_hat_ml</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="n">y_test</span><span class="p">[</span><span class="s">"tair_2m_mean"</span><span class="p">]).</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div> <p>As you can see the score is roughly 0.36. That is not exactly great but does indicate a weak correlation between predicted and true values.</p> <div style="background-color: #f5f5f5; padding: 15px; border-radius: 5px; margin-bottom: 20px;"> <div class="notice--primary"> <h3> <a href="#part-23-multiple-linear-models" class="anchor-heading" aria-labelledby="part-23-multiple-linear-models"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Exercise </h3> <p>Do a linear interpolation and 1-D linear model prediction for this same data. Do any of them perform equally good or better than the multiple regression? </p> <details> <summary>Solution!</summary> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#------- interpolation:
# in order to interpolate value-by-value we need to
# first sort the previously randomized data:
</span>
<span class="n">y_train_sorted</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">.</span><span class="n">sort_index</span><span class="p">()</span>
<span class="n">y_test_sorted</span> <span class="o">=</span> <span class="n">y_test</span><span class="p">.</span><span class="n">sort_index</span><span class="p">()</span>

<span class="n">interpolated_data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">interp</span><span class="p">(</span>
    <span class="n">y_test_sorted</span><span class="p">.</span><span class="n">index</span><span class="p">,</span>
    <span class="n">y_train_sorted</span><span class="p">.</span><span class="n">index</span><span class="p">,</span>
    <span class="n">y_train_sorted</span><span class="p">[</span><span class="s">"tair_2m_mean"</span><span class="p">])</span>

<span class="n">regression_results</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">interpolated_data</span><span class="p">)</span>

<span class="c1">#-------- 1-D linear model:
</span><span class="n">y</span> <span class="o">=</span> <span class="n">df_dwd_noNA</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,[</span><span class="s">"tair_2m_mean"</span><span class="p">]].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">df_dwd_noNA</span><span class="p">.</span><span class="n">index</span><span class="p">.</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span> 
<span class="c1"># creating train and test sets 
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span> 
	<span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">101</span><span class="p">)</span> 

<span class="n">linearModel</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">linearModel</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">y_hat_linear</span> <span class="o">=</span> <span class="n">linearModel</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">regression_results</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">)</span>
</code></pre></div> </div> </details> </div> </div> <h3 id="24-machine-learning-approaches-example-random-forests"> <a href="#24-machine-learning-approaches-example-random-forests" class="anchor-heading" aria-labelledby="24-machine-learning-approaches-example-random-forests"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> 2.4: Machine Learning approaches (example Random Forests) </h3> <p>We already covered quite a lot of ground on how to deal with missing data by</p> <ul> <li>cleaning the raw data</li> <li>gap fill with interpolation, 1D-linear modeling and multiple linear regression</li> </ul> <p>In this final part we will take a quick look at a more sophisticated type of model, the Random Forests algorithm. Random Forest is a so-called decision-tree algorithm and can be counted to the broad category of “machine-learning” methods. The latter however is reeeeally a broad category, as it basically just describes that the machine works through a minimization procedure on such a large amount of data, that humans could not handle it manually, thus the machine is “learning” the optimization of the model and can make predictions from it.</p> <p>Random Forests has proven to be quite effective in gap-filling applications in a variety of contexts and is available as part of the scikit-learn package. The purpose is for you to get an idea, how to implement such a sophisticated method and to hopefully get you excited about machine learning! We will not go into the details of the actual method.</p> <p>Lets dive right in and load the random forest regressor from scikit-learn. We use the regressor because we work with time-series data. Random Forest also has a classification model, which is used for categorical data (for example image-recognition, predicting an animal type from its traits etc…)</p> <p>The great thing about scikit-learn is that most of the models work in the exact same way, no matter whether it is a simple linear model or a complex machine-learning approach.<br /> For example to run a model with simple default setting all we have to do is the following:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
  <span class="c1"># I set n_estimators to 12 for a quick initial fit
</span>  <span class="c1"># we will go into the parameters a bit more later!
</span><span class="n">rf_model</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">rf_model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">rf_model</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="n">y_hat_rf</span> <span class="o">=</span> <span class="n">rf_model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">regression_results</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_hat_rf</span><span class="p">)</span>
</code></pre></div></div> <p>The predicting performance is still not that great. However, with a machine-learning approach we can feed some more data into the model and see, whether it improves the model.<br /> I will only give a very brief intro to random forests here, no need to memorize that. If you are interested, you can also look the below youtube video for a very good short video on the method.<br /> https://www.youtube.com/watch?v=v6VJ2RO66Ag</p> <p>Very generally speaking you can say that this algorithm looks at your data and the predictors and it picks a few of the predictors, leaving others out.<br /> With this reduced set it trains a model. That means, it tries to find out under which circumstances in the predictors, the data has a certain value. <br /> In random forests, many of those models are trained and compared. Each with different predictors and trained on different amounts and points of training data.<br /> After building the model, you can use it to predict unknown values. Therefore, the predictor data for these unknown datapoints is fed into each of these models and the combined output from all of them is evaluated as the final decision.</p> <p>Lets try adding some more of our weather-data into the model and see whether it improves the performance:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Lets add the other weather-data columns into the predictor data as well.
# First we find the rows where all the predictors data and our observations
# are present:
</span><span class="n">df_dwd_noNA</span> <span class="o">=</span> <span class="n">df_dwd</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,[</span><span class="s">"date_time"</span><span class="p">,</span> <span class="s">"SWIN"</span><span class="p">,</span><span class="s">"rH"</span><span class="p">,</span> <span class="s">"pressure_air"</span><span class="p">,</span> <span class="s">"wind_speed"</span><span class="p">,</span> <span class="s">"precipitation"</span><span class="p">,</span> <span class="s">"tair_2m_mean"</span><span class="p">]].</span><span class="n">dropna</span><span class="p">()</span>

<span class="c1"># Now we split them into the x-values (predictors) and the y-values
# (predictand or target variable)
</span><span class="n">x</span> <span class="o">=</span> <span class="n">df_dwd_noNA</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,[</span><span class="s">"SWIN"</span><span class="p">,</span><span class="s">"rH"</span><span class="p">,</span> <span class="s">"pressure_air"</span><span class="p">,</span> <span class="s">"wind_speed"</span><span class="p">,</span> <span class="s">"precipitation"</span><span class="p">]]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df_dwd_noNA</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,[</span><span class="s">"tair_2m_mean"</span><span class="p">]]</span>

<span class="c1"># Now we go an split the data into training and testing data:
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span> 
	<span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">101</span><span class="p">)</span> 

<span class="c1"># Finally we do the full pipeline of 
# creating the model, fitting it, and scoring:
</span><span class="n">rf_model</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">rf_model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">rf_model</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="n">y_hat_rf</span> <span class="o">=</span> <span class="n">rf_model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">regression_results</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_hat_rf</span><span class="p">)</span>
<span class="c1"># Aha, the model performs a bit better.
</span></code></pre></div></div> <p>Finally, we will do some tweaking on the random forest paramters. Parameters are options given to the model, that define how it is set up. Here for example n_estimators is one parameter we gave to the model so far.<br /> Maybe we can make the model perform even a bit better by increasing that value. In order to do so, we better use hourly data, because as we make the model bigger, the time it takes to fit the model gets substanitally larger.<br /> Lets first aggregate the data like before:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># mean for most data:
</span><span class="n">df_dwd_hourly_noNA</span> <span class="o">=</span> <span class="n">df_dwd_noNA</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,[</span><span class="s">"SWIN"</span><span class="p">,</span><span class="s">"rH"</span><span class="p">,</span> <span class="s">"pressure_air"</span><span class="p">,</span> <span class="s">"wind_speed"</span><span class="p">,</span><span class="s">"tair_2m_mean"</span><span class="p">,</span> <span class="s">"date_time"</span><span class="p">]].</span><span class="n">resample</span><span class="p">(</span><span class="n">rule</span><span class="o">=</span><span class="s">"1h"</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="s">"date_time"</span><span class="p">).</span><span class="n">mean</span><span class="p">().</span><span class="n">dropna</span><span class="p">()</span>
<span class="c1"># sum for precipitation data:
</span><span class="n">df_dwd_hourly_noNA</span><span class="p">[</span><span class="s">"precipitation"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_dwd_noNA</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,[</span><span class="s">"precipitation"</span><span class="p">,</span> <span class="s">"date_time"</span><span class="p">]].</span><span class="n">resample</span><span class="p">(</span><span class="n">rule</span><span class="o">=</span><span class="s">"1h"</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="s">"date_time"</span><span class="p">).</span><span class="nb">sum</span><span class="p">().</span><span class="n">dropna</span><span class="p">()</span>
</code></pre></div></div> <p>Now we can run a new model on the hourly data and e.g. set the n_estimators to 50. Play around with the parameter a bit and see how the performance changes:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">x_hourly</span> <span class="o">=</span> <span class="n">df_dwd_hourly_noNA</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,[</span><span class="s">"SWIN"</span><span class="p">,</span><span class="s">"rH"</span><span class="p">,</span> <span class="s">"pressure_air"</span><span class="p">,</span> <span class="s">"wind_speed"</span><span class="p">,</span> <span class="s">"precipitation"</span><span class="p">]]</span>
<span class="n">y_hourly</span> <span class="o">=</span> <span class="n">df_dwd_hourly_noNA</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,[</span><span class="s">"tair_2m_mean"</span><span class="p">]]</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span> 
	<span class="n">x_hourly</span><span class="p">,</span> <span class="n">y_hourly</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">101</span><span class="p">)</span> 
<span class="n">rf_model</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">rf_model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">rf_model</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="n">y_hat_rf</span> <span class="o">=</span> <span class="n">rf_model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">regression_results</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_hat_rf</span><span class="p">)</span>
<span class="c1"># as you can see, the model performs yet another bit better.
</span></code></pre></div></div> <div class="notice--primary"> <h3> <a href="#24-machine-learning-approaches-example-random-forests" class="anchor-heading" aria-labelledby="24-machine-learning-approaches-example-random-forests"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Exercise </h3> <p>Practice makes perfect! For the hourly data, see how the linear interpolation, 1D-linear model and multiple linear models perform compared to the random forest regression.</p> <details> <summary>Solution!</summary> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#------- preparation of  data:
# mean for most data aggregation:
</span><span class="n">df_dwd_hourly_noNA</span> <span class="o">=</span> <span class="n">df_dwd_noNA</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,[</span><span class="s">"SWIN"</span><span class="p">,</span><span class="s">"rH"</span><span class="p">,</span> <span class="s">"pressure_air"</span><span class="p">,</span> <span class="s">"wind_speed"</span><span class="p">,</span><span class="s">"tair_2m_mean"</span><span class="p">,</span> <span class="s">"date_time"</span><span class="p">]].</span><span class="n">resample</span><span class="p">(</span><span class="n">rule</span><span class="o">=</span><span class="s">"1h"</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="s">"date_time"</span><span class="p">).</span><span class="n">mean</span><span class="p">().</span><span class="n">dropna</span><span class="p">()</span>
<span class="c1"># sum for precipitation aggregation:
</span><span class="n">df_dwd_hourly_noNA</span><span class="p">[</span><span class="s">"precipitation"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_dwd_noNA</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,[</span><span class="s">"precipitation"</span><span class="p">,</span> <span class="s">"date_time"</span><span class="p">]].</span><span class="n">resample</span><span class="p">(</span><span class="n">rule</span><span class="o">=</span><span class="s">"1h"</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="s">"date_time"</span><span class="p">).</span><span class="nb">sum</span><span class="p">().</span><span class="n">dropna</span><span class="p">()</span>

<span class="n">x_hourly</span> <span class="o">=</span> <span class="n">df_dwd_hourly_noNA</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,[</span><span class="s">"SWIN"</span><span class="p">,</span><span class="s">"rH"</span><span class="p">,</span> <span class="s">"pressure_air"</span><span class="p">,</span> <span class="s">"wind_speed"</span><span class="p">,</span> <span class="s">"precipitation"</span><span class="p">]]</span>
<span class="n">y_hourly</span> <span class="o">=</span> <span class="n">df_dwd_hourly_noNA</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,[</span><span class="s">"tair_2m_mean"</span><span class="p">]]</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span> 
	<span class="n">x_hourly</span><span class="p">,</span> <span class="n">y_hourly</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">101</span><span class="p">)</span> 

<span class="k">print</span><span class="p">(</span><span class="s">"------- linear intrpolation:"</span><span class="p">)</span>
<span class="n">y_train_sorted</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">.</span><span class="n">sort_index</span><span class="p">()</span>
<span class="n">y_test_sorted</span> <span class="o">=</span> <span class="n">y_test</span><span class="p">.</span><span class="n">sort_index</span><span class="p">()</span>
<span class="n">interpolated_data</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">interp</span><span class="p">(</span>
    <span class="n">y_test_sorted</span><span class="p">.</span><span class="n">index</span><span class="p">,</span>
    <span class="n">y_train_sorted</span><span class="p">.</span><span class="n">index</span><span class="p">,</span>
    <span class="n">y_train_sorted</span><span class="p">[</span><span class="s">"tair_2m_mean"</span><span class="p">])</span>
<span class="n">regression_results</span><span class="p">(</span><span class="n">y_test_sorted</span><span class="p">,</span> <span class="n">interpolated_data</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"------- multiple linear regression:"</span><span class="p">)</span>
<span class="n">linearModel</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">linearModel</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">linearModel</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">errors</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_test</span> <span class="o">-</span> <span class="n">y_hat</span><span class="p">).</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">0</span><span class="p">].</span><span class="n">values</span>
<span class="n">regression_results</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"------- random forest:"</span><span class="p">)</span>
<span class="n">rf_model</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">rf_model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">.</span><span class="n">values</span><span class="p">.</span><span class="n">ravel</span><span class="p">())</span>
<span class="n">rf_model</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="n">y_hat_rf</span> <span class="o">=</span> <span class="n">rf_model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">regression_results</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_hat_rf</span><span class="p">)</span>
</code></pre></div> </div> </details> </div> <p>For hourly data the linear interpolation still performs best. However, the gaps we are interpolating thus far are rather small. As a last exercise, we will see how the methods perform for longer gaps. Therefore I create a gap in the hourly dataset of a full day. We will then see how the different methods perform in filling the gap:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">df_dwd_hourly_noNA</span> <span class="o">=</span> <span class="n">df_dwd_noNA</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,[</span><span class="s">"SWIN"</span><span class="p">,</span><span class="s">"rH"</span><span class="p">,</span> <span class="s">"pressure_air"</span><span class="p">,</span> <span class="s">"wind_speed"</span><span class="p">,</span><span class="s">"tair_2m_mean"</span><span class="p">,</span> <span class="s">"date_time"</span><span class="p">]].</span><span class="n">resample</span><span class="p">(</span><span class="n">rule</span><span class="o">=</span><span class="s">"1h"</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="s">"date_time"</span><span class="p">).</span><span class="n">mean</span><span class="p">().</span><span class="n">dropna</span><span class="p">()</span>
<span class="n">df_dwd_hourly_noNA</span><span class="p">[</span><span class="s">"precipitation"</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_dwd_noNA</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,[</span><span class="s">"precipitation"</span><span class="p">,</span> <span class="s">"date_time"</span><span class="p">]].</span><span class="n">resample</span><span class="p">(</span><span class="n">rule</span><span class="o">=</span><span class="s">"1h"</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="s">"date_time"</span><span class="p">).</span><span class="nb">sum</span><span class="p">().</span><span class="n">dropna</span><span class="p">()</span>

<span class="c1"># first lets create the 14-day long gap:
# I first extract the indices of some single day and safe them
</span><span class="n">indices_for_gap</span> <span class="o">=</span> <span class="n">df_dwd_hourly_noNA</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">505</span><span class="p">:</span><span class="mi">529</span><span class="p">,</span> <span class="p">:].</span><span class="n">index</span>
<span class="c1"># Now I make a copy of the original data to not mess it up
</span><span class="n">gapped_data_hourly</span> <span class="o">=</span> <span class="n">df_dwd_hourly_noNA</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
<span class="c1"># Then I set all the values for tair in these indices to NaN
</span><span class="n">gapped_data_hourly</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">indices_for_gap</span><span class="p">,</span> <span class="s">"tair_2m_mean"</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">NaN</span>
<span class="c1"># Finally I can extract the predictor and predictand columns with these indices:
</span><span class="n">x_hourly</span> <span class="o">=</span> <span class="n">gapped_data_hourly</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">indices_for_gap</span><span class="p">,[</span><span class="s">"SWIN"</span><span class="p">,</span><span class="s">"rH"</span><span class="p">,</span> <span class="s">"pressure_air"</span><span class="p">,</span> <span class="s">"wind_speed"</span><span class="p">,</span> <span class="s">"precipitation"</span><span class="p">]]</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">df_dwd_hourly_noNA</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">indices_for_gap</span><span class="p">,</span> <span class="s">"tair_2m_mean"</span><span class="p">]</span>

<span class="c1">#---- interpolation
</span><span class="n">interpolated_data</span> <span class="o">=</span> <span class="n">gapped_data_hourly</span><span class="p">[</span><span class="s">"tair_2m_mean"</span><span class="p">].</span><span class="n">interpolate</span><span class="p">()</span>
<span class="n">regression_results</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">interpolated_data</span><span class="p">[</span><span class="n">indices_for_gap</span><span class="p">])</span>

<span class="c1">#---- multiple linear regression:
</span><span class="n">y_hat_linear</span> <span class="o">=</span> <span class="n">linearModel</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_hourly</span><span class="p">)</span>
<span class="n">regression_results</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_hat_linear</span><span class="p">)</span>

<span class="c1">#---- Random Forest:
</span><span class="n">y_hat_rf</span> <span class="o">=</span> <span class="n">rf_model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_hourly</span><span class="p">)</span>
<span class="n">regression_results</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_hat_rf</span><span class="p">)</span>

</code></pre></div></div> <div class="notice--primary"> <h3> <a href="#24-machine-learning-approaches-example-random-forests" class="anchor-heading" aria-labelledby="24-machine-learning-approaches-example-random-forests"><svg viewBox="0 0 16 16" aria-hidden="true"><use xlink:href="#svg-link"></use></svg></a> Exercise </h3> <p>Play around with the length of the gap and observe how the performance of the different methods changes. Try to give an explanation and maybe formulate, when something like linear interpolation could be suitable and when it is better to rely on a more complex method.</p> <details> <summary>Solution!</summary> <p>The Random Forest method works quite nicely on longer prediction windows. As long as there is a clear linear trend in the data, a simple interpolation might perform very well. However, if within a data gap a shift happens and for example a warm period comes around, the linear interpolation will quickly get worse in its prediction.</p> </details> </div> <p>As long as there is a clear linear trend in the data, a simple interpolation might perform very well. However, if within a data gap a shift happens and for example a warm period comes around, the linear interpolation can not capture that.</p> <p>Keep in mind that a model is by definition NEVER the actual truth. Its goal is to come as close as possible to the truth while often times drastically reducing the complexity of the issue. In a real world scenario we would not know that our final modelled data is not the same as the true data. Therefore all we can do is create and test our models to the best of our knowledge and be honest about what they are capable of doing and what their shortcomings are!</p> </main> </div> </div> <div class="search-overlay"></div> </div> </body> </html>
