{"0": {
    "doc": "Home",
    "title": "Why You Should Learn Python?",
    "content": "Generally speaking Python is a general-purpose, high-level interpreted programming language. What does that mean? . It means that it can be used for almost any kind of application you can achieve with any programming language. The “high-level” refers so to speak to the “distance” from the hardware in the way you use the language. The most extreme other side of the spectrum would be Assembly, where you directly control the processor and memory of the PC. Python lets you speak in simple terms and the computer understands what you want. In contrast to many other languages such as Java or C you don’t even have to compile your code. Compiling is a process in which your written code gets translated into something the machine can understand. In Java for example your workflow is always write code -&gt; compile program -&gt; run program. Python is interpreted which means that whenever you execute your code, it gets internally compiled and directly executed. That takes some responsibility of our shoulders. Not to forget: Python is one of the most widely used programming languages of all. The below graphic is from the 2023 Stackoverflow software developer survey. Pythons ranking is especially impressive as the other top languages are HTML, CSS and Javascript which power the majority of the modern internet while not being very widely used in other contexts. The data science platform Kaggle conducted a similar study in 2022 but specifically for data science and machine learning engineers . In the plot below you can clearly see that Python is the most widely used technology, way before R (R even decreasing in use) and SQL. So with all this said, get ready to join us in Python heaven! (credits: https://xkcd.com/353/) . ",
    "url": "/New_BAI_DataAnalysis/#why-you-should-learn-python",
    
    "relUrl": "/#why-you-should-learn-python"
  },"1": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "/New_BAI_DataAnalysis/",
    
    "relUrl": "/"
  },"2": {
    "doc": "0. INSTALLATION",
    "title": "Installing Python",
    "content": "For this course we will use a very handy tool called Anaconda. It is basically Python in a box, meaning that it creates a closed environment on your PC which already has Python and a lot of extra packages as well as additional software such as a code editor installed on it. This makes the installation as easy as it gets. The downside is that the installer is quite large (&gt;800mb). To get started, just go to Link: the Anaconda website… and download the version for your operating system. Simply follow the download instructions and leave all the buttons as they are by default. Once the installation is finished, search for the program “Anaconda Navigator” and open it. Once it opens you are presented with the main window of your Anaconda environment . In the “Home” screen you see a bunch of different programs that can run within Anaconda. The one we will use most is “Spyder” (definitely usable by people with Arachnophobia!). Make sure that Spyder is installed, if it is not click on the button to install it. Spyder is a code editor for Python which has some handy extensions, such as line-by-line execution and nicely viewable variables and tables during execution of code. But we will come back to that later… . Additionally there is a window called “Environnments”. Click on it and you will be presented with a table of two columns. The left side shows you environments (red box). Anaconda lets you create multiple environments (again, basically separete Python installations which are secluded from each other). This can be useful if e.g. you need functionality of very specific versions of packages for some program, but the same package in a different version in another program. To avoid the different versions clashing you can put them in different environments. On the right side (blue box) you can see the packages which are installed in an environment. Python has some internal core functionalities, but there are many many (many many many…) additional packages created by the community which unlock Pythons full potential. Some of the most widely used pacakges are e.g. numpy, pandas or scikit-learn. Notice: Try searching for them in the environment using the search-box on the top right and see whether they are already installed! . If a package is missing and you want to install it, select the dropdown menu on the top that by default says “installed” and swithc to “Not installed”. Then use the search box again to find the package you want to install. Notice: Try installing the package “xgboost” in this way. Once you have finished all of this you should be good to go! To verify that everything works go to the “Home” tab and start Spyder. Time for your first line of code! In the open Editor you will have to save the file in order to run it. Then copy and paste the following code by clicking the button on the top right in the box below. Insert it into spyder. Crypticlist = ['⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀', ' ⣀⣤⣤⠤⠐⠂⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡌⡦⠊⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡀⣼⡊⢀⠔⠀⠀⣄⠤⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣤⣤⣄⣀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣶⠃⠉⠡⡠⠤⠊⠀⠠⣀⣀⡠⠔⠒⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⣾⣿⢟⠿⠛⠛⠁', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣼⡇⠀⠀⠀⠀⠑⠶⠖⠊⠁⠀⠀⠀⡀⠀⠀⠀⢀⣠⣤⣤⡀⠀⠀⠀⠀⠀⢀⣠⣤⣶⣿⣿⠟⡱⠁⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢰⣾⣿⡇⠀⢀⡠⠀⠀⠀⠈⠑⢦⣄⣀⣀⣽⣦⣤⣾⣿⠿⠿⠿⣿⡆⠀⠀⢀⠺⣿⣿⣿⣿⡿⠁⡰⠁⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣾⣿⣿⣧⣠⠊⣠⣶⣾⣿⣿⣶⣶⣿⣿⠿⠛⢿⣿⣫⢕⡠⢥⣈⠀⠙⠀⠰⣷⣿⣿⣿⡿⠋⢀⠜⠁⠀⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠠⢿⣿⣿⣿⣿⣰⣿⣿⠿⣛⡛⢛⣿⣿⣟⢅⠀⠀⢿⣿⠕⢺⣿⡇⠩⠓⠂⢀⠛⠛⠋⢁⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠀', '⠘⢶⡶⢶⣶⣦⣤⣤⣤⣤⣤⣀⣀⣀⣀⡀⠀⠘⣿⣿⣿⠟⠁⡡⣒⣬⢭⢠⠝⢿⡡⠂⠀⠈⠻⣯⣖⣒⣺⡭⠂⢀⠈⣶⣶⣾⠟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀', '⠀⠀⠙⠳⣌⡛⢿⣿⣿⣿⣿⣿⣿⣿⣿⣻⣵⣨⣿⣿⡏⢀⠪⠎⠙⠿⣋⠴⡃⢸⣷⣤⣶⡾⠋⠈⠻⣶⣶⣶⣷⣶⣷⣿⣟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠈⠛⢦⣌⡙⠛⠿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡀⠀⠀⠩⠭⡭⠴⠊⢀⠀⠀⠀⠀⠀⠀⠀⠀⠈⣿⣿⣿⣿⣿⡇⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠈⠙⠓⠦⣄⡉⠛⠛⠻⢿⣿⣿⣿⣷⡀⠀⠀⠀⠀⢀⣰⠋⠀⠀⠀⠀⠀⣀⣰⠤⣳⣿⣿⣿⣿⣟⠑⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠓⠒⠒⠶⢺⣿⣿⣿⣿⣦⣄⣀⣴⣿⣯⣤⣔⠒⠚⣒⣉⣉⣴⣾⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠛⠹⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣭⣉⣉⣤⣿⣿⣿⣿⣿⣿⡿⢀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⠟⡁⡆⠙⢶⣀⠀⢀⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⣴⣶⣾⣿⣟⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠿⢛⣩⣴⣿⠇⡇⠸⡆⠙⢷⣄⠻⣿⣦⡄⠀⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣼⣿⣿⣿⣿⣿⣿⣿⣎⢻⣿⣿⣿⣿⣿⣿⣿⣭⣭⣭⣵⣶⣾⣿⣿⣿⠟⢰⢣⠀⠈⠀⠀⠙⢷⡎⠙⣿⣦⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣼⣿⣿⣿⣿⣿⣿⣿⣿⡟⣿⡆⢻⣿⣿⣿⣿⣿⣿⣿⣿⣿⠿⠿⠟⠛⠋⠁⢀⠇⢸⡇⠀⠀⠀⠀⠈⠁⠀⢸⣿⡆⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡜⡿⡘⣿⣿⣿⣿⣿⣶⣶⣤⣤⣤⣤⣤⣤⣤⣴⡎⠖⢹⡇⠀⠀⠀⠀⠀⠀⠀⠀⣿⣷⡄⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣦⡀⠘⢿⣿⣿⣿⣿⣿⣿⣿⣿⠿⠿⠛⠋⡟⠀⠀⣸⣷⣀⣤⣀⣀⣀⣤⣤⣾⣿⣿⣿⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⣸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣭⣓⡲⠬⢭⣙⡛⠿⣿⣿⣶⣦⣀⠀⡜⠀⠀⣰⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⢀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣭⣛⣓⠶⠦⠥⣀⠙⠋⠉⠉⠻⣄⣀⣸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⣼⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣶⣆⠐⣦⣠⣷⠊⠁⠀⠀⡭⠙⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡆⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⢠⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⢉⣛⡛⢻⡗⠂⠀⢀⣷⣄⠈⢆⠉⠙⠻⢿⣿⣿⣿⣿⣿⠇⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠘⣿⣿⡟⢻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡟⣉⢁⣴⣿⣿⣿⣾⡇⢀⣀⣼⡿⣿⣷⡌⢻⣦⡀⠀⠈⠙⠛⠿⠏⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠙⢿⣿⡄⠙⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠿⠛⠛⠛⢯⡉⠉⠉⠉⠉⠛⢼⣿⠿⠿⠦⡙⣿⡆⢹⣷⣤⡀⠀⠀⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠿⠄⠈⠻⠿⠿⠿⠿⠿⠿⠛⠛⠿⠛⠉⠁⠀⠀⠀⠀⠀⠀⠻⠿⠿⠿⠿⠟⠉⠀⠀⠤⠴⠶⠌⠿⠘⠿⠿⠿⠿⠶⠤⠀'] for line in Crypticlist: print(line) . Now its time to run your first script! You can run all of your script by clicking the green arrow on top (red circle) or you can run the code line-by-line with the green arrow with the line-symbol next to it (white circle). Once you ran the code you will see the variables created in the list on the top right (see picture below) and you will see the output of your code in the bottom right in the console (see picture below). May the code be with you! . ",
    "url": "/New_BAI_DataAnalysis/python_0_installation.html#installing-python",
    
    "relUrl": "/python_0_installation.html#installing-python"
  },"3": {
    "doc": "0. INSTALLATION",
    "title": "0. INSTALLATION",
    "content": " ",
    "url": "/New_BAI_DataAnalysis/python_0_installation.html",
    
    "relUrl": "/python_0_installation.html"
  },"4": {
    "doc": "1. BASICS OF PYTHON",
    "title": "The Basics of Python",
    "content": "This interactive tutorial will get you started on your data-exploration road and make you familiar with some core concepts of Python programming and data analysis. Notice: In all following sections I will insert some code snippets. You are very much encouraged to copy and paste them with the button on the top right and run them in your IDE (e.g. Spyder). Table of Contents . | General stuff about Python | Data Types and Variables | Operators | Loops and Conditionals | Functions and Classes | . Notice that it is not at all expected that you learn all these things and they are burnt into your brain (!!!!!). It is more of a broad intrdocution to all the basics so you have herd of them, but programmers do look up stuff all the time! So don’t worry if it is a lot of input right now, just try to understand the concepts and you can always come back and find help in here, in the internet or from me directly. Here are some useful ressources to look things up: . Link: w3schools.com: Tutorials on many topics where you can quickly look up things… Link: geeks4geeks.com: Another nice overview of many functionalities of Python (requires login)… Geeksforgeeks requires you to make an account or use e.g. a google login, but it features many tutorials, project ideas, quizzes and so on on many programming languages and general topics such as Machine Learning, Data Visualization, Data Science, Web Development and many more Link: Pandas cheat sheet: Later on we will use the library “Pandas” (so cute!) for data handling. A nice cheat sheet is provided by the developers… . ",
    "url": "/New_BAI_DataAnalysis/python_1_basics.html#the-basics-of-python",
    
    "relUrl": "/python_1_basics.html#the-basics-of-python"
  },"5": {
    "doc": "1. BASICS OF PYTHON",
    "title": "1. General things about Python",
    "content": "I quickly want to highlight some things which are special about Python compared to many other programming languages and that one needs to get used to. Python is indentation sensitive . In Python it matters, how far you indent your lines, meaning how much space you have at the beginning of a line. As an example this will work: . a = 5 b = 1 . but this will throw an error: . a = 5 b = 5 . will result in an error: . File \"&lt;stdin&gt;\", line 1 b = 5 IndentationError: unexpected indent . Variables . Generally in Python variables are created by assigning a value to them with an equal sign, just like we did above. Theire output can be shown by just typing the variable: . a = 5 a 5 . Comments . Comments are lines in the code that are not executed and are there for documentation. For now it is a good idea to use comments in your code to keep track of what is happening where. Single line comments are always created with an ‘#’. Everything after that symbol in the line is not executed. Multi-line comments can be written by enclosing them in three ‘: . # first I create a single line comment, this is not executed a = 5 # this line is executed, but the comment gets ignored ''' Now I write a multi-line comment I can continue the comment on the next line b = 5 &lt;-- this is ignored ''' . Python is 0-indexed . In Python, the first index in for example a list always has the number 0! This takes some time to get used to, especially if you come from e.g. R which has 1-based indexing, but most programming lanuge handle indexing like that and it is worth getting used to it. I won’t go into why it is handled like that but there are many discussions on the internet about it, feel free to dive in if you feel like diving into a rabbit-hole ;) . Separators . In Python separators for decimal numbers are ALWAYS dots! Commas are used e.g. to separete variables from each other or entries in a list . correct_decimal = 2.5 correct_decimal 2.5 . Naming of variables, functions, and anything at all . This is not Python-specific but a very important note! Always use descriptive names for variables, functions or anything that you give a name! Especially in scientific programming you see it time and time again that people name variables and functions using abbreviations that just came to their mind. This makes code much, much harder to read and to use by other people or your own future self. It happens so often that people look back at what they wrote 3 weeks ago and do not understand half of it because they did not give descriptive names. You can also use comments to document your code a bit, but that always takes up extra space, often does not look good because you barely keep the same formatting throughout the code and gives the next user more work to do when trying to understand the code. Just making the code explain itself is the best solution of all. Here is a very simple example: . # Bad code with abbreviations # it requires the user to interprete the variables and look at # used functions to understand what this even does l = [1,5,12,17,18,14,11] n = len(l) s = sum(l) m = s/n # Fixing it with comments # With comments we require the user to read all the extra text to # 1) understand what the data is # 2) understand what is calculated l = [1,5,12,17,18,14,11] # a list of temperature values n = len(l) # get total number of samples s = sum(l) # get total sum of samples m = s/n # calculate the mean value # This gets so much easier to read when using declarative naming. # You dont even have to look into the function to understand what it is doing: monthly_temperature = [1,5,12,17,18,14,11] number_of_samples = len(monthly_temperature) sum_of_samples = sum(monthly_temperature) mean_monthly_temperature = sum_of_samples/number_of_samples # This does not mean that naming has to replace comments completely # (although some people argue like that). It is still alright to use comments # to clarify parts of your code, just try keep it to a minimum and make the # code as self-explanatory as possible! . ",
    "url": "/New_BAI_DataAnalysis/python_1_basics.html#1-general-things-about-python",
    
    "relUrl": "/python_1_basics.html#1-general-things-about-python"
  },"6": {
    "doc": "1. BASICS OF PYTHON",
    "title": "2. Data Types and Variables",
    "content": "Python knows different types of data. A number is a different kind of variable than a word. That helps organizing the variables and defines, which operations are possible with which data. For example, computing the mean of a word would be difficult, just as translating a number to all-uppercase letters… . In Python you dont have to define the data type yourself because Python is smart and finds the type of data on its own. For example when we define a number Python will understand and give it the type “int” or “float”, which means “integer” or “floating point number” (decimal) We will not cover all data types as we probably won’t need all of them for our purpose. However these ones are important: . Primitive Datatypes Primitive datatypes are simple constructs that consists basically of one chunk of information, e.g. a number or a word: . | int: Integer, a number without a floating point | float: Floating point number, a number with decimals | str: A string of characters, e.g. letters, words and sentences | bool: Boolean, a value that can only be True or False. This helps us make decisions in our code | . Non-Primitive Datatypes Non-primitive datatypes consist of aggregations of primitive datatypes. A list for example holds several numbers or words or something else . | list: An ordered sequence of data, for example [1,2,3] is a list where each of the entries have a specific position and the entries can be accessed by indices | dict: A non-ordered mapping that consists of keys and values. That simply means, we can not get entries from the dictionary by indices (e.g. the 0th entry in a dictionary) but instead grab data from the dictionary by using the key. Imagine it like a digital telephone-book. The comparison does not hold completely because in theory a telephone book is ordered, but you would never search the 5001231th entry in a telephone book. Instead you would search the phone number of Mr. Smith”, so you go to the “key” Mr. Smith and get the “value” 0251/1234567. | . Lets look at some examples for data types: . # Primitive datatypes: letter_a = \"a\" # &lt;-- a string name = \"Josefine\" # &lt;-- a longer string age = 24 # &lt;-- an integer total_playtime = 354.5 # &lt;-- a float is_injured = False # &lt;-- a boolean # Non-Primitive datatypes: # list: # a list is alwasys enclosed by brackets # and the items are separeted with commas: scores_last_games = [5,3,0,1] # To access the values we can use the index, for example scores_last_games[0] # &lt;-- gets the first entry scores_last_games[2] # &lt;-- gets the 3rd entry scores_last_games[-1] # &lt;-- gets the last entry scores_last_games[-2] # &lt;-- gets the second last entry # dictionary: # is always enclosed by {}, # and has the structure \"key\":value, lines are separeted by a comma. josefine = { \"age\":age, \"total_playtime\":total_playtime, \"is_injured\":is_injured, \"scores_last_games\":scores_last_games } # Now the values of the dictionary can be accessed using the key like this: josefine[\"age\"] 24 # new entries can be added by assigning a value to a new key: josefine[\"trikot_number\"] = 9 . You can always find the type of a variable by using the type() function (more on functions later): . type(name) type(age) type(total_playtime) type(is_injured) type(scores_last_games) type(josefine) . It is possible to change the type of a variable, but only if Python is able to understand what the outcome should be. The functions to do that have the same name as the target data type, for example int() or str(): . int(\"10\") # &lt;-- this works str(500) # &lt;-- this works float(500) #&lt;-- this works float(\"500.5\") #&lt;-- this works float(\"abc\") #&lt;-- this won't work, how should you translate a word to a number? . One last thing is important to note. When you assign a non-primitive variable to another non-primitive variable, the two variables share the same data. That means, when you manipulate one you also manipulate the other. This can lead to confusion when you don’t keep it in mind. list_1 = [1,2,3] # a list is non-primitive list_2 = list_1 # here we assign the non-primitive list_1 to the variable list_2 list_2.append(4) # we add a fourth value, 4, to list_2 list_2 # list_2 is now [1,2,3,4] list_1 # BUT! list_1 is now also [1,2,3,4] # We can avoid this and extract the values from list_1 to create a completely new variable by using the .copy() function list_1 = [1,2,3] list_2 = list_1.copy() # we copy the values of list_1 to the new variable list_2 list_2.append(4) # we add a fourth value, 4, to the list list_2 list_2 # list_2 is now [1,2,3,4] list_1 # list_1 is still [1,2,3] . On the other hand when you assign a variable containing a primitive datatype to another variable, the value gets simply copied to the new variable. Here is an example: . a = 5 b = a # we assign the value of a to the variable b b = b + 1 # we increase the value of b by one b # b is now 6 a # a is still 5 . ",
    "url": "/New_BAI_DataAnalysis/python_1_basics.html#2-data-types-and-variables",
    
    "relUrl": "/python_1_basics.html#2-data-types-and-variables"
  },"7": {
    "doc": "1. BASICS OF PYTHON",
    "title": "3. Operators",
    "content": "An operator is something that allows you to interact with variables. Some very examples are mathematical operations or comparisons. 3.1 Arithmetic operations . Most operations are very intuitive. For example you can add numbers and also add words to concatenate them, but you can not subtract words from each other… Here is a list of operations: . a = 5 b = 10 word1 = \"Hi\" word2 = \"there\" # Airthmetic Operators: c = a + b # adding numbers concatenated_words = word1 + \" \" + word2 # adding words d = b - c # subtracting numbers e = a * b # multiplying numbers f = b / 5 # dividing numbers g = a ** 2 # Exponentation, this is a² h = 12 % 5 # Modulus, this returns the remaining amount after fitting one number into the other as many times as possible. Exercise . With what you know so far, grab the scores josefine scored in the last games and compute the average amount of goals per game she scores . Solution! scores = josefine[\"scores_last_games\"] total_scores = scores[0] + scores[1] + scores[2] + scores[3] mean_scores = total_scores / 4 . There are much better solutions to this, for example the iteration over all scores can be done with the built-in function sum() and the total number of score-values can be found using the len() function. A one-line solution could look like this: . mean_scores = sum(josefine[\"scores_last_games\"]) / len(josefine[\"scores_last_games\"]) . 3.2 Comparison operations . Comparison operations are used to compare values with each other in order to make decisions in your script. The output of a comparison is always a boolean value that is “True” if the comparison is evaluated as correct and “False” otherwise. goals_team1 = 5 goals_team2 = 2 goals_team1 &gt; goals_team2 # &gt; larger than goals_team1 &gt;= goals_team2 # &gt;= larger than or equal goals_team1 &lt; goals_team2 # &lt; smaller than goals_team1 &lt;= goals_team2 # &lt;= smaller than or equal goals_team1 == goals_team2 # == equal goals_team1 != goals_team2 # != not equal # you can also store the result in a variable: is_team1_winner = goals_team1 &gt; goals_team2 is_team2_winner = goals_team1 &lt; goals_team2 . 3.3 Logical operators . Logical operators can combine multiple comparisons. Namely there are three: and, or and not. The use of these is pretty intuitive. If we combine two comparisons with an “and”, the result is only True if all conditions hold. If we combine two comparisons with an “or”, the result is True if one of the conditions hold, even if the other is False. Not is a special case, that reverts the result. # Lets use a new example peter = { \"age\":24, \"height\":1.73, \"is_enrolled\": True } joana = { \"age\":25, \"height\":1.75, \"is_enrolled\": False } # Now we can do some comparisons: is_peter_taller_and_older_than_joana = peter[\"age\"] &gt; joana[\"age\"] and peter[\"height\"] &gt; joana[\"height\"] is_peter_not_enrolled = not peter[\"is_enrolled\"] is_joana_not_enrolled = not joana[\"is_enrolled\"] is_peter_or_joana_enrolled = peter[\"is_enrolled\"] or joana[\"is_enrolled\"] . 3.4 Identity and membership operators . The identitiy operator “is” is to check whether two objects are the same. On the other side, the membership operator “in” checks whether an object is contained within another object. Simple examples: . a = [1,2,3] # a simple list b = a # we assign a to b, remember non-primitive data types? a is b # What will be the result of this? 1 in a # we can test whether a contains a number 1 c = [a,b] # here we create a new list that contains the lists a and b a in b # now we can check whether one of the lists is within another list a in c . Code block in details in a notice . Exercise . Now you know all about operators. Try to use your knowledge and figure out what we test for in the following operations and what the result is: . joana = { \"enrolled\": True, \"grade_ecophysiology\": 1.3, \"grade_archery\": 1.3 } alfonso = { \"enrolled\": True, \"grade_ecophysiology\": 1.7, \"grade_archery\": 4.3 } legolas = { \"enrolled\": False, \"grade_ecophysiology\": 4.0, \"grade_archery\": 1.0 } # 1. a = (legolas[\"grade_ecophysiology\"] &lt; joana[\"grade_ecophysiology\"]) and (legolas[\"grade_ecophysiology\"] &lt; alfonso[\"grade_ecophysiology\"]) # 2. b = (legolas[\"grade_archery\"] &lt; joana[\"grade_archery\"]) and (legolas[\"grade_archery\"] &lt; alfonso[\"grade_archery\"]) # 3. c = not (legolas[\"grade_ecophysiology\"] &lt; joana [\"grade_ecophysiology\"]) or (alfonso[\"grade_ecophysiology\"] &lt; joana [\"grade_ecophysiology\"]) # 4. d = joana[\"enrolled\"] and alfonso[\"enrolled\"] and legolas[\"enrolled\"] # 5. e = alfonso[\"grade_ecophysiology\"] &gt; 4.0 or alfonso[\"grade_archery\"] &gt; 4.0 # 6. f = (alfonso[\"grade_ecophysiology\"] &gt; 4.0 or alfonso[\"grade_archery\"] &gt; 4.0) or (legolas[\"grade_ecophysiology\"] &gt; 4.0 or legolas[\"grade_archery\"] &gt; 4.0) or (joana[\"grade_ecophysiology\"] &gt; 4.0 or joana[\"grade_archery\"] &gt; 4.0) . Solution! . | Check 1 tests whether legolas is the best ecophysiologist. The result is False. | Check 2 tests whether legolas is the best archer. The result is True. | Check 3 tests whether legolas or alfonso are better ecophysiologists than joana. With the \"not\" in the beginning, the result is turned into whether Joana is better than any of the two. The result is True. | Check 4 tests whether everyone is enrolled. The result is False. Legolas is probably buisy somewhere else... | Check 5 tests whether Alfonso failed one of the exams with a grade higher than 4.0. The result is True. | Check 6 tests whether anyone failed one of the exams with a grade higher than 4.0. The result is True. | . ",
    "url": "/New_BAI_DataAnalysis/python_1_basics.html#3-operators",
    
    "relUrl": "/python_1_basics.html#3-operators"
  },"8": {
    "doc": "1. BASICS OF PYTHON",
    "title": "4. Conditionals and Loops",
    "content": "Conditionals and loops are constructs in your code that are often combined. A loop is used to do a certain task on many elements sequentially, a conditional uses a certain condition (or truth-evaluation) to decide whether a piece of code should be executed. 4.1 Conditionals . Remember how we talked about comparison, logical and identitiy and membership operators? They all result in a boolean, stating whether a condition is True or False. We can make use of that by utilizing conditionals. Here is a simple example: . is_peter_smart = True if is_peter_smart == True: print(\"Peter is smart\") . Notice how indentation plays a role here! We end the line of the if-check with a “:” and start the new line indented. Indented lines signal a code block, that always belongs to the previous statement that ended with a “:”. In the above example the print() command will be executed because the value of is_peter_smart is True. If we check for a boolean value (True or False) we can also leave the comparison operation out and ask very coloquially: . is_peter_smart = True if is_peter_smart: print(\"Peter is smart\") . We can also define a code that should be executed, ONLY if the if-check is evaluated as False. For that we use the keyword “else”. is_peter_smart = True if is_peter_smart: print(\"Peter is smart\") else: print(\"Peter is not smart\") . Finally, you can also chain if-checks by using the “elif” keyword. This stands for “else if”, meaning that “if the previous checks failed and this check is evaluated as True, run the code” . is_peter_smart = False is_peter_big = True if is_peter_smart: print(\"Peter is smart\") elif is_peter_big: print(\"Peter is big\") else: print(\"Peter is not smart and not big\") . 4.2 Loops . A loop is a structure that allows you to iteratively perform actions, either with several elements (e.g. stored in a list) or while a specific condition holds. These two types are called “for-loops” and “while-loops”. They always consist of two parts: The definition how and over what you want to iterate (or “loop”) and the actual action you want to perform. 4.2.1 The for-loop . The most “classical” loop is the for-loop. The syntax is, as often in Python, held very simple. Here is an easy example: . temperatures = [12,14,16,15,16,17,20,21] for temperature in temperatures: print(temperature) . Notice that in the definition of the loop, we define a new variable called “temperature”. This variable represents the element we are currently working on in each step of the loop. So in the first step, temperature is 12, in the next temperature is 14 and so on. There is one very handy built-in method that can give you both the value of the list-entry and its corresponding index, called enumerate(). You can put them both in variables by using a comma in the loop-definition. A quick demo: . temperatures = [12,14,16,15,16,17,20,21] hour_of_day = [8,9,10,11,12,13,14,15] # When using enumerate, each iteration we get the index and value of the current list entry. # So in the first loop index will be 0 and temperature 12, # next index will be 1 and temperature 14 and so on... for index, temperature in enumerate(temperatures): print(\"Temperature at \"+str(hour_of_day[index]) + \":00: \"+str(temperature) + \"°C\") . 4.2.2 The while-loop . A while loop is not used as often as a for-loop. In the definition you define a condition and “while” that condition holds, the loop is executed. Look at this example: . a = 1 while a &lt;= 10: print(a) a = a +1 . Can you guess what will be display? . Solution It will print the numbers 1 to 10, including 10 Warning . When you define a while-loop, always make sure that the condition will at some point be fullfilled. Otherwise it can easily happen that youre while-loop just keeps running endlessly! . a = 1 # This loop will run forever, because a will never be &gt; 10! while a &lt;= 10: print(a) . Exercise . Now you already know quite some tools for writing a Python script! Use your knowledge to complete the code below. The goal is to print the sentence \"{month} was a hot month\" whenever the mean monthly temperature is above two times the mean and \"{month} was a dry month\" whenever the precipitation was less than half of the mean. One tip: For the printing you can use formatted strings. They make inserting variables in a string much easier! just put an \"f\" in front of the string and insert the variable in curly braces {}. For example print(f\"Hello {name}\" would print \"Hello Peter\" if the variable name=Peter is defined. Here is your starter code: . months = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"Juli\",\"August\", \"September\", \"October\", \"November\", \"December\"] monthly_temperature = [4, 4, 7, 11, 15, 17, 28, 24, 27, 12, 7, 4] monthly_precipitation = [15, 40, 60, 75, 65, 32, 10, 80, 60, 70, 57, 100] mean_temperature = mean_precipitation = for ... in enumerate(...): if ...: ... if ...: ... Solution! months = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"Juli\",\"August\", \"September\", \"October\", \"November\", \"December\"] monthly_temperature = [4, 4, 7, 11, 15, 17, 28, 24, 27, 12, 7, 4] monthly_precipitation = [15, 40, 60, 75, 65, 32, 10, 80, 60, 70, 57, 100] mean_temperature = sum(monthly_temperature)/len(monthly_temperature) mean_precipitation = sum(monthly_precipitation)/len(monthly_precipitation) for index, month in enumerate(months): if monthly_temperature[index] &gt; 2*mean_temperature: print(f\"{month} was a hot month\") if monthly_precipitation[index] &lt; mean_precipitation/2: print(f\"{month} was a dry month\") . ",
    "url": "/New_BAI_DataAnalysis/python_1_basics.html#4-conditionals-and-loops",
    
    "relUrl": "/python_1_basics.html#4-conditionals-and-loops"
  },"9": {
    "doc": "1. BASICS OF PYTHON",
    "title": "5. Functions and Classes",
    "content": "Congratulations! You made it this far down, that means you have accquired knowledge of the basic building blocks of Python. You are now ready to go into two concepts that go beyond basic scripting (meaning, just putting all your code line by line into one file), and learn about the fundamental blocks that help strcuturing your program: Functions and Classes! . 5.1 Functions . Functions are constructs of own, separate blocks of code in your program which take care of certain tasks. They are super useful, because often you want to do the same operation many times in your code but don’t want to write the same code every time again. Just write your own function and call it whenever you need its expertise! Lets just look at a simple example: . def calculate_mean(list_of_values): n_samples = len(list_of_values) sum_of_values = sum(list_of_values) mean = sum_of_values/n_samples return mean . Pretty intuitive, right? . A function is always defined by starting with the keyword “def”, then we give it a name, calculate_mean in this case. Afterwards in the braces are the “arguments” that the function takes. Arguments are pieces of information from the outside code, which the function requires to work. Here it is the list_of_values the function shall calculate the mean value of. After the “:” we follow with the indented codeblock that belongs to the function. Here we do all the operations the function should do. Finally, we use the “return” keyword which ends the function and defines, which piece of information should be returned to the outside code. Important The variables which are defined inside a function are restricted to that function! The outside code won’t know of the variables n_samples or “mean” which are defined in the function. Calling the function would for example look like this: . monthly_temperature = [4, 4, 7, 11, 15, 17, 28, 24, 27, 12, 7, 4] mean_monthly_temperature = calculate_mean(monthly_temperature) . You do not have to return a value. You could also for example print something in the function and then return, without providing a value to return. In older versions of Python this was all there was to writing a function. However, nowadays you can add some additonal information to make it even easier for the next person or your future self to understand it. With some extra bits you can add the infos, what type of data you expect as an input to the function and what type of data it will output. This is generally a good thing to do and now considered best practice when writing functions. For the above code it would look like this: . def calculate_mean(list_of_values:list[float]) -&gt; int: n_samples = len(list_of_values) sum_of_values = sum(list_of_values) mean = sum_of_values/n_samples return mean . In the first line, after the list_of_values we write “:list[float]” to specify that we expect a list of float (floats actually imply integers, so we can use that to also accept integers). After the closing bracket we write “-&gt; int” which states that this function will return an integer value. Exercise 5.1.1 . As a first exercise, try to figure out what the output of the below function will be without executing it! . def square_value(value:int) -&gt; int: return value * value def divide_value_by(numerator:int, denominator:int) -&gt; int: return numerator / denominator a = square_value(2) b = square_value(a) c = divide_value_by(b,a) d = square_value(divide_value_by(c,2)) print(d) . Solution! The result is 4! . def square_value(value:int) -&gt; int: return value * value def divide_value_by(numerator:int, denominator:int) -&gt; int: return numerator / denominator a = square_value(2) # 2*2 = 4 b = square_value(a) # 4*4 = 16 c = divide_value_by(b,a) # 16/4 = 4 d = square_value(divide_value_by(c,2)) # square_value gets the output of divide_value(c,2) as argument. print(d) # so 4/2 is 2, thn 2*2 is 4 . Exercise 5.1.2 . Lets go for a bit more challenging of an exericse (I am sure you are ready for it!) There is a built-in function that allows the user to give an input through the command-line to the program. It is simply called \"input()\". E.g. \"testword = input()\" would stop the program and wait for the user to input something in the console and then press enter. Imagine you want a program in which you set a new password. Write a function that checks whether the new password is longer than 9 symbols and that returns the corresponding boolean. the function should also print that the password is too short if it is too short and that it is ok when it is ok. Use the returned boolean to keep asking for new input from the user while the word is less than 9 characters long . Here is some starter code: . def is_password_too_short(word:str, min_length:int)-&gt;bool: is_password_too_short = ... if ...: .... else: .... return ... password_is_bad = True while ...: print(\"Please enter your password:\") password = input() password_is_bad = ... Solution! def is_password_too_short(word:str, min_length:int)-&gt;bool: is_password_too_short = len(word) &lt; min_length if is_password_too_short: print(f\"Password has to be at least {min_length} characters long!\") else: print(\"New password set!\") return is_password_too_short password_is_bad = True while password_is_bad: print(\"Please enter your new password:\") password = input() password_is_bad = is_password_too_short(password, 8) . 5.2 Classes . Classes are the final fundamental building block of Python we will look at here. A class basically represents a blueprint of an object that has certain properties. As an example, when I am working with data on ecosystems it could be convenient to have an ecosystem class that includes information about the ecosystem type, the location as latitude and longitude, and some meteorological data. Lets look at an example: . class Ecosystem(): def __init__(self, id, IGBP_ecosystem_class, lat, lon, mean_annual_Tair, mean_annual_precip): self.id = id self.IGBP_ecosystem_class = IGBP_ecosystem_class self.lat = lat self.lon = lon self.mean_annual_Tair = mean_annual_Tair self.mean_annual_precip = mean_annual_precip . The definition of a class always begins with the class-keyword followed by the name of the class. It always has a first function called init() which is also called “constructor”. This method is used to create new instances of the class and assigns values to the class. The “self” keyword is in this context always used within a class to reference the class itself. Note, that “self” also has to be in the list of arguments for the function, but is does not get passed when you call the function. Many new words but stay with me, it is pretty simple when we look at an example, how we create a new instance: . # First we define the class class Ecosystem(): # see how the first argument here is \"self\" def __init__(self, id, IGBP_ecosystem_class, lat, lon, mean_annual_Tair, mean_annual_precip): self.id = id self.IGBP_ecosystem_class = IGBP_ecosystem_class self.lat = lat self.lon = lon self.mean_annual_Tair = mean_annual_Tair self.mean_annual_precip = mean_annual_precip # now we use that class to create a new ecosystem-object, # see how we have to provide every value defined in the constructor except for \"self\": amtsvenn = Ecosystem(id=\"amtsvenn\", IGBP_ecosystem_class = \"open shrublands\", lat = 52.176, lon = 6.955, mean_annual_Tair = 10.5, mean_annual_precip = 870) # Now you have stored all the info about amtsvenn in the \"amtsvenn\" # object and can access them whenever you want: print(amtsvenn.id) print(amtsvenn.IGBP_ecosystem_class) print(amtsvenn.lat) print(amtsvenn.lon) . Classes can not only comprise of the information associated with them but can also have methods associated specifically with them. For example we can create a function that prints all the information enclosed in the object. class Ecosystem(): def __init__(self, id, IGBP_ecosystem_class, lat, lon, mean_annual_Tair, mean_annual_precip): self.id = id self.IGBP_ecosystem_class = IGBP_ecosystem_class self.lat = lat self.lon = lon self.mean_annual_Tair = mean_annual_Tair self.mean_annual_precip = mean_annual_precip def print_ecosystem_information(self): print(\"=====================\") print(\"Ecosystem information\") print(f\"ID: {self.id}\") print(f\"IGBP ecosystem class: {self.IGBP_ecosystem_class}\") print(f\"Location (lat/lon): {self.lat}°/{self.lon}°\") print(f\"Mean annual air temperature: {self.mean_annual_Tair} °C\") print(f\"Mean annual precipitation: {self.mean_annual_precip} mm\") amtsvenn = Ecosystem(id=\"amtsvenn\", IGBP_ecosystem_class = \"open shrublands\", lat = 52.176, lon = 6.955, mean_annual_Tair = 10.5, mean_annual_precip = 870) # After creating the object we can use the classes functions like this: amtsvenn.print_ecosystem_information() . Exercise 5.2.1 . Lets do one exercise that can further show, why classes are great for creating reusable code. Try to write a function called \"Statistics\". This class will be a \"behavioural\" class, meaning it does not need to hold own data but rather holds some methods, that belong to the same topic. In that class, define functions that calculate the mean, the variance and the standard deviation of a given list. Then use that class to calculate these metrics of an arbitrary list. Hint: For the standard deviation you need to take the square root. You can do that with pythons built-in math module. You can use it like this: . import math math.sqrt(24) . Try to work out the solution yourself first! There is some starter code below, in case you get stuck though. Starter code class Statistics(): def calculate_mean(self, ...): ... def calculate_variance(self, ...): mean = ... squares = [] for value in values: squares.append(...) variance = ... return ... def calculate_stdev(self, ...): variance = ... stdev = .... return ... Solution! import math class Statistics(): def calculate_mean(self, values:list[float]): return sum(values)/len(values) def calculate_variance(self, values:list[float]): mean = self.calculate_mean(values) squares = [] for value in values: squares.append((value-mean)**2) variance = sum(squares) / (len(values)-1) return variance def calculate_stdev(self, values:list[float]): variance = self.calculate_variance(values) stdev = math.sqrt(variance) return stdev stat = Statistics() example_list = [1,2,3,4,5,5,6,7,123,1,1,4] mean = stat.calculate_mean(example_list) stdev = stat.calculate_stdev(example_list) variance = stat.calculate_variance(example_list) . We will not go deeper into classes here, but it is very important to understand the concept. Most Python packages are written in object-oriented style, which (in very simple terms) means that the methods are enclosed in classes. So knowing the basics makes it much easier to understand the following bits. ",
    "url": "/New_BAI_DataAnalysis/python_1_basics.html#5-functions-and-classes",
    
    "relUrl": "/python_1_basics.html#5-functions-and-classes"
  },"10": {
    "doc": "1. BASICS OF PYTHON",
    "title": "1. BASICS OF PYTHON",
    "content": " ",
    "url": "/New_BAI_DataAnalysis/python_1_basics.html",
    
    "relUrl": "/python_1_basics.html"
  },"11": {
    "doc": "2. DATA HANDLING AND VISUALIZATION",
    "title": "Of plots and pandas: Data handling and Visualization",
    "content": "In this second part of the course we will talk about how to handle, process and visualize data in Python. For that purpose we will make use of a few third-party libraries. NumPy and Pandas will help us store the data in array- and matrix-like structures (in Pandas more specifically Dataframes) and do some processing of the data. Pandas already has some visualization capabilities, but for nicer looks and configurability we will make use of the Plotly package. To underline that these are essential tools in Python, let me once again pull out the Stackoverflow 2023 survey: According to the ~3k respondants, Numpy, Pandas and Scikit-Learn (which we will use in the next lesson) are 3 of the 8 most used technologies in programming across all languages (disregarding web-technologies)! . For this part we will use some example data. It is a dataset from the german weather service DWD from the Ahaus Station (ID 7374) ranging from 1996 to 2023. Click here to download (25mb)…. Note The data is in .parquet-format. You may not have heard of it, but this is a very compressed and fast format. For example this dataset with 27 years worth of data, in Parquet this is 25mb of data, in .csv its 208mb. While you can not open .parquet directly in excel or a text editor like a .csv file, it is much much faster to load e.g. when using it in programming languages, which is exactly what we are going to do here. As a last note: NumPy is one of the older Python libraries and Pandas is actually built on top of it. However, because we work with example data and want to get hands-on as fast as possible, we will cover Pandas first and then go from there. Table of Contents . | Pandas | NumPy | Plotly | . ",
    "url": "/New_BAI_DataAnalysis/python_2_data_visualization.html#of-plots-and-pandas-data-handling-and-visualization",
    
    "relUrl": "/python_2_data_visualization.html#of-plots-and-pandas-data-handling-and-visualization"
  },"12": {
    "doc": "2. DATA HANDLING AND VISUALIZATION",
    "title": "0. Importing modules",
    "content": "Just a quick forword on importing libraries in Python. Pandas, Plotly and Numpy are all external libraries we need to import to our script in order to make them work. Usually we would also have to install them, but since we work in Anaconda, this is already taken care of for us! Very simply, to import a library you type “import” and then the respective name. Typically you want to give an “alias” to the package, which is basically a variable that you can then use to access all the methods in the package. For some packages there are long-standing standards of what names to use. For pandas for example this is “pd”: . import pandas as pd . You can also only import specific parts of a package, which can save memory. Going back to one exercise from the previous lesson, if you know that you will only use the sqrt function from the math package you can use the syntax . # Importing only a single function, squareroot from math import sqrt # Importing several functions, squareroot and greatest common divider from math import sqrt, gcd # theoretically you could also give an alias here from math import sqrt as squareroot # this does not make much sense though . ",
    "url": "/New_BAI_DataAnalysis/python_2_data_visualization.html#0-importing-modules",
    
    "relUrl": "/python_2_data_visualization.html#0-importing-modules"
  },"13": {
    "doc": "2. DATA HANDLING AND VISUALIZATION",
    "title": "1. Pandas",
    "content": "Pandas is around since 2008 and one of the most wiedely used tools for data analysis of all. The usage is all about two types of objects: The pandas Series and the Pandas DataFrame, where a Series is more or less one column of a dataframe (basically a vector). If you already worked with R, the concept of a DataFrame is not new to you. However for starters, a DataFrame is basically a table, in which each row has an index and each column has a label. Simple right? . (credit: https://www.geeksforgeeks.org/creating-a-pandas-dataframe/) . Creating DataFrames . Lets create a first little DataFrame. There are several ways to do it, one rather intuitive way is to use a dictionary. Think about it, a dictionary already has values which are labeled by keys. You can easily imagine this in a table-format: The keys will be the column-labels and the indices (row-labels) are by default just numbered. import pandas as pd # Note that we create an instance of the class \"DataFrame\" # Therefore we have to call the function pd.DataFrame(). Within # the brackets we then define a dictionary using the {}-style syntax values_column_1 = [2,4,6,8,10] values_column_2 = [3,6,9,12,15] df = pd.DataFrame({ \"column_1\": values_column_1, \"column_2\": values_column_2 }) . Another option to create a dataframe is of course to read in data. Lets go ahead and read the data from the german weather service that you can download above. Now we can use pandas built-in data-reader to directly create a DataFrame from the parquet-file: . # The path can either be the absolute path to the place where you saved the file # or the relative path, meaning the path relative to the place where your script is. # I'd recommend to create a subfolder where your script is called \"data\" and then # import the data from the path \"./data/ahaus_data_1996_2023.parquet\" df_dwd = pd.read_parquet('path_to_file') . Accessing rows and column . Once you createad a dataframe, you can access individual columns by using the column names. Either you can directly access them using brackets, or you use the built-in “.loc”-function. I would recommend getting used to the .loc right away, as it rules out some errors you can run into otherwise. With .loc you always have to provide first the rows you want to access and then the column, separated with a comma. If you want to get all rows, that is done using a colon (“:”) To get a list of all availabel columns you can simply type “df.columns” . # First we can take a look at the available columns df_dwd_columns = df_dwd.columns print(df_dwd_columns) # Then we can use the column names to extract a column # from the dataframe # Either you use only the column name in brackets: df_dwd[\"tair_2m_mean\"] # But even better: use the .loc function: df_dwd.loc[:,\"tair_2m_mean\"] # get all rows df_dwd.loc[20:50,\"tair_2m_mean\"] # get rows 20 to 50 df_dwd.loc[:20,\"tair_2m_mean\"] # get all rows up to 20 (including 20) df_dwd.loc[20:,\"tair_2m_mean\"] # get all rows after 20 (including 20) . Note that the .loc examples above all assume numeric index. But Pandas is not restricted to that! The index (or “row-label”) could also be something like “mean” or “standard-deviation”. Keep that in mind for the exercise below! . Built-in methods to describe the data . Pandas has a great set of convenience functions for us to look at and evaluate the data we have. | .info() gives us a summary of columns, number of non-null values and datatypes | .head() and .tail() show the first or last five rows of the dataframe | .describe() directly gives us some statistical measures (number of samples, mean, standard deviation, min, max and quantiles) Note that the output of .describe() is again a DataFrame, that you can save in a variable to evaluate it. There are also built-in methods that you can run directly on single columns. Examples of such functions are .mean(), .min(), .max() and .std(). | . Exercise . You already know, how to call a method that is attached to a class. With that knowledge, explore the Ahaus DWD dataset and figure out the mean, standard deviation, min and max for air temperature, precipitation height, air pressure and short wave radiation (SWIN) . Hint It may be that the output of the .describe() function has a pretty bad formatting with 5 decimal numbers or more. In that case you can change the formatting of the output using . df.describe().applymap('{:,.2f}'.format) . Solution! # There are lots of ways to complete this exercise. # You can use the above mentioned describe() method # First get the summary. Save the output of .describe() # in a new dataframe df_dwd_summary = df_dwd.describe().applymap('{:,.2f}'.format) # Then you can access values in that dataframe like this: tair_2m_mean = df_dwd_summary.loc[\"mean\", \"tair_2m_mean\"] tair_2m_min = df_dwd_summary.loc[\"min\", \"tair_2m_mean\"] # and so on... # You could also directly use the pandas built-in .min, .max, # .mean and .std methods. For example: tiar_2m_mean = df_dwd[\"tair_2m_mean\"].mean() tiar_2m_min = df_dwd[\"tair_2m_mean\"].min() # and so on... Challenge . There is a one-line solution to this task, that only grabs the values asked for in the exercise. I wouldn’t say that that would be the recommended solution for the sake of overview, but to fiddle around it is a good challenge. Hint: You can pass lists for the row- and column-labels to .loc . Solution # We can chain all the commands above to a one-line operation, meaning we # directly call .describe().map().loc[] on each others output. # By passing the list [\"mean\", \"std\", \"min\", \"max\"] as row-indices and # [\"tair_2m_mean\",\"precipitation\",\"SWIN\",\"pressure_air\"] as column-labels # we can directly access the range of values asked for in the exercise. df.describe().applymap('{:,.2f}'.format).loc[[\"mean\", \"std\", \"min\", \"max\"], [\"tair_2m_mean\",\"precipitation\",\"SWIN\",\"pressure_air\"]] . Datetime . Pandas has a specific datatype that is extremely useful when we are working with time series data (a s our example DWD dataset). It is called datetime64[ns] and allows us to do a range of super useful things like slicing based on dates or resampling from 10-minute to daily, weekly, monthly data and so on. With datetime-indices, handling timeseries gets so much more convenient. # get data newer than 31.12.2022 df_dwd[df_dwd[\"data_time\"] &gt; \"2022-12-31\"] # get only data from 2022 df_dwd[df_dwd[\"data_time\"].dt.year == 2022] # But wait! Its not working, is it? # Can you figure out why not? Remember the type() function! . Now, how do we get this to work for us? Well, the methods work with the datetime64 data type, so we need to change the “data_time” column type! Luckily, Pandas has a function for that. It is called to_datetime() and is part of the main library, so you call it as pd.to_datetime(). It takes the column you want to convert to datetime64 type as argument, tries to parse it to datetime64 and returns the result series. If it fails to parse, maybe because your data_time column is in a country-specific formatting, you can pass an additional “format” argument in which you provide the input format. But we will not cover it here, as the default should work for the DWD dataset. example_df = pd.DataFrame({ \"data_time\": [\"2022-01-01 01:00:00\",\"2022-01-01 12:00:00\", \"2022-01-02 01:00:00\", \"2022-01-02 12:00:00\", \"2022-01-03 01:00:00\", \"2022-01-03 12:00:00\"], \"values\" : [1,5,4,20,6,-10] }) type(example_df[\"data_time\"]) example_df[\"data_time\"] = pd.to_datetime(example_df[\"data_time\"]) . Exercise . 1. In your dataframe, turn the \"data_time\" column into a datetime64 type column. Then create dataframes for each season across all years, meaning one for spring, summer, autumn and winter each. The respective months are March to May, June to August, September to November and December to February. Compare the mean air temperature, precipitation and radiation between the different seasons. 2. Find the dates of the maximum temperatures measured in the dwd dataset. One hint: What we want to do here is to find those rows, where the value is one of a set of values. To do so you can use the built-in pandas function .isin(). An example: . # Here is an example series (representing a column of a dataframe) series = pd.Series([1,2,3,1,2,3,1,2,3]) # Wen want to extract the rows where the value is 1 or 3: desired_values = [1,3] series_ones_and_threes = series[series.isin(desired_values)] # Note that the indices in the extracted series are the ones from series, where the value is 1 or 3, # so it really represents an extracted subset of the original series . Solution! df_dwd[\"data_time\"] = pd.to_datetime(df_dwd[\"data_time\"]) df_dwd[\"data_time\"] = pd.to_datetime(df_dwd[\"data_time\"]) # First of all we create 4 dataframes, one for each season # We do it by accessing the numeric value of the months in the \"data_time\" # column. 1 refers to January and so on. With the .isin() method we extract # those rows where the values correspond to the numbering of the month df_dwd_summer = df_dwd.loc[df_dwd[\"data_time\"].dt.month.isin([6,7,8])] df_dwd_autumn = df_dwd.loc[df_dwd[\"data_time\"].dt.month.isin([9,10,11])] df_dwd_winter = df_dwd.loc[df_dwd[\"data_time\"].dt.month.isin([12,1,2])] df_dwd_spring = df_dwd.loc[df_dwd[\"data_time\"].dt.month.isin([3,4,5])] # To find the mean for each season we have a range of different options # how we want to get the means and compare them. I'll show three different # ways which are all valid. # We know we will want to do some operation on all of the 4 datasets, so it is # already a good idea to put them in a list. That way we can easily iterate over them seasonal_datasets = [df_dwd_spring, df_dwd_summer, df_dwd_autumn, df_dwd_winter] # Now one option would be to iterate over the datasets and print # the mean values of the desired columns: seasons = [\"spring\", \"summer\", \"autumn\", \"winter\"] for idx, df in enumerate(seasonal_datasets): print(\"----------\") print(seasons[idx]) print(\"----------\") print(f'mean Ta: {df[\"tair_2m_mean\"].mean()}') print(f'mean precipitation: {df[\"precipitation\"].mean()}') print(f'mean SWIN: {df[\"SWIN\"].mean()}') # This way we have the outputs grouped by seasons # Another option would be to iterate over the variables we want # to evaluate. Then we can print the variable values for each # season directly below each other: variables = [\"tair_2m_mean\", \"precipitation\", \"SWIN\"] for idx, variable in enumerate(variables): print(\"--------\") print(variable) print(\"--------\") for i, df in enumerate(seasonal_datasets): stats = df.describe() print(f\"{seasons[i]}: {stats.loc['mean', variable]}\") # Often times we don't even want to print the output but rather # just extract and keep it for later use, e.g. for visualizing it later. # So another option is to create a new dataframe that holds # the seasons as columns and variables as rows. That way we can # just look at the whole new dataframe and easily compare the values seasonal_df = pd.DataFrame(columns = seasons) for idx, df in enumerate([df_dwd_spring, df_dwd_summer, df_dwd_autumn, df_dwd_winter]): season = seasons[idx] seasonal_df.loc[\"Ta\", season] = df[\"tair_2m_mean\"].mean() seasonal_df.loc[\"Precip\", season] = df[\"precipitation\"].mean() seasonal_df.loc[\"SWIN\", season] = df[\"SWIN\"].mean() print(seasonal_df) . In this exercise we extracted seasonal information from 5-minute interval data. This type of frequency-conversion is something we do very often when working with time-series data. We also call this operation “resampling”. Pandas actually has a great convenience function, that makes resampling a breeze, utilizing the wonderful datetime64-format. The operation consists basically only of two function calls on the pandas dataframe. The first is “.resample()”. We must define the column that contains the datetimes with the “on” argument and our target frequency with the “rule” argument as a string. The most useful frequency specifiers are: . | “S”: seconds | “T” or “min”: minutes | “H”: hours | “D”: days All of these can be extended with numbers, such as “7D” for 7 days or “30min” for half-hourly values. | . Afterwards we also have to call a function that specifies how we want to resample. You see, if we change the frequency from 5 minute data to daily data, the daily value can be computed in different ways. For example for temperature it would make sense to use the daily mean value. For precipitation on the other hand it is probably more useful to get the daily sum, if we are interested in the amount of rain per day. That is why “.resample()” has to be followed by a function like “.mean()” or “.sum()”. Here is a full example: . example_data_time = pd.to_datetime([\"2022-01-01 01:00:00\",\"2022-01-01 12:00:00\", \"2022-01-02 01:00:00\", \"2022-01-02 12:00:00\", \"2022-01-03 01:00:00\", \"2022-01-03 12:00:00\"]) example_df = pd.DataFrame({ \"data_time\": example_data_time, \"values1\" : [1,5,4,20,6,-10], \"values2\" : [100,500,400,2000,600,-1000], }) df_daily_means = example_df.resample(rule=\"1D\", on=\"data_time\").mean() df_daily_sums = example_df.resample(rule=\"1D\", on=\"data_time\").sum() . I mentioned before that pandas itself already has some built-in plotting capabilities. I won’t go deep onto it, but it is definitely worth mentioning because you can use it to get a quick overview of data in a pandas dataframe. You can simply call the “.plot()” function on any dataframe. You can run this on the whole dataframe, which will plot all available columns, or you extract specific rows/columns with the methods we discussed before and then run “.plot()”: . # lets use the example_df from above: # First we plot both values1 and values2: example_df.plot(x=\"data_time\") # Note that we have to say that we want data_time on the # x-axis, because otherwise it will by default use a numeric # index on the x-axis and plot the data_time column against it # as well. You can try it out by leaving the x=... out # The figure should now pop up in the \"plots\" tab on the right # side of your Spyder window # Now we just plot values 1: exampl_df.plot(x=\"data_time\", y=\"values1\") . There is a lot more functionality, but I want to leave the pandas plotting with that, as we will dive deeper into plotting later in this course. Ok, we have covered quite some ground on handling pandas dataframes. We covered . | how to create dataframes | how to read data from .csv or .parquet files | how indexing works | how we get some descriptive information on the data | how to compute some informative values such as the min, max and mean of a series | even how datetime-indices work (honestly we just scratched the surface, but for an introduction course this is already quite advanced) | and how to resample time-series data to another frequency Finally I just want to give some “honorable mentions”, to tell you about functions with pandas that you will probably need at some point. No exercise here, I just want you to have heard of these: | . # 1. pd.concat(): # this function concatenates dataframes with matching columns or pandas series # meaning it simply glues one dataframe on the bottom of the next: df_1 = pd.Series([1,2,3]) df_2 = pd.Series([4,5,6]) df_3 = pd.concat([df_1, df_2]) # note that we have to put the two dataframes in a list # 2. pd.merge() # This functions combines dataframes based on common indices. # It is a rather complex function but this is a simple example # how to combine two dataframes that have overlapping indices: df_1 = pd.DataFrame( index = [1,2,3], data = { \"col_1\": [1,2,3], \"col_2\": [4,5,6] }) df_2 = pd.DataFrame( index = [3,4,5], data = { \"col_1\": [7,8,9], \"col_2\": [10,11,12] }) df_3 = df_1.merge(right=df_2, how = \"outer\") # 3. df.apply() # In the call to apply you can define a function that will be # executed on each element of the dataframe: df_1_plus_one = df_1.apply(lambda x: x+1) # don't worry about the \"lambda\", it simply creates # the variable \"x\" we can use for \"x+1\". x is only there # during the computation and then immediately vanishes again . ",
    "url": "/New_BAI_DataAnalysis/python_2_data_visualization.html#1-pandas",
    
    "relUrl": "/python_2_data_visualization.html#1-pandas"
  },"14": {
    "doc": "2. DATA HANDLING AND VISUALIZATION",
    "title": "2. A quick touch on Numpy",
    "content": "Many Python programmers and data scientists would probably shun me for not giving more time to numpy, but we want to get to the applications as fast as possible. However, if you want to know more you can . | download a little Numpy cheat sheet here | check out the official Numpy documentation | read about numpy at w3schools.com Numpy is like the grandmaster of handling data in Python. It has always been there, it can do everything, but it is not neccessarily pleasant to deal with. With Numpy you can create vectors and multi-dimensional matrices, do computations and much more. It is very lightweight (meaning it uses very little memory) and super fast. Actually, Pandas has Numpy as its underlying framework. Every column or row in a pandas datframe is actually a Numpy array with fancy extras. That makes Pandas slower than Numpy but also much more convenient to use. While we can do most of our analysis in this course only with Pandas, I think you should know about the basic functionality and the core uses of Numpy. So lets take a look at some simple structures and computations: | . 2.1. Numpy Arrays . The most used structure in Numpy are arrays. In contrast to normal Python lists, they are faster, they force the values to be homogeneous (e.g. no strings and integers mixed in a Numpy array), and with Numpy arrays you can compute some mathematical operations between arrays such as element-wise addtion, cross-products and so on. Additionally, numpy provides a range of functions you can run directly on arrays, such as .mean(), .min(), .max(), .median() and so on. Generally you can think of Numpy arrays/matrices vs Pandas Dataframes/Series as the difference between pure vectors or structures with pure numeric data in them vs. fully fledged and labeled tables. There are different ways to create Numpy arrays: . import numpy as np # a simple vector is created by calling np.array # with a list as argument: vector_1 = np.array([0,1,2,3,4,5]) # alternatively, you can directly create a vector # filled with zeros or ones providing a shape. # The shape has round brackets and defines the # dimensions of the data structure. For example # (2,3) will create a matrix with 2 rows and 3 columns vector_zeros = np.zeros(shape=(2,3)) vector_ones = np.ones(shape=(2,3)) # with np.random.rand() you can create a matrix with # random elements between 0 and 1, by multiplying it # you can get e.g. values between 0 and 10: vector_randoms_0_to_1 = np.random.rand(3,10) vector_randoms_0_to_10 = np.random.rand(3,10)*10 # You can then get individual elements from that 2-D # structure with indexing. For example to get the # the second element in the first column: vector_randoms_0_to_10[0,2] = 2 # You can find the shape of a numpy object with vector_zeros.shape() # Lastly you can create arrays with consecutive numbers with np.arange() # I takes a start, an end and an interval as arguments: range_10 = np.arange(0,10,1) range_10_halfsteps = np.arange(0,10,0.5) # The ranges are created including the first and excluding the # last number. 2.2. Useful Numpy functions . In addition to Numpys own data structures it provides a whole range of useful functions that can be used in other contexts as well. One function I probably use more than any other are np.floor(), np.ceil() and np.round(). These all round values. Floor returns the nearest lower integer, ceil the nearest upper integer and round rounds to a desired decimal point: . vector = np.array([1.1, 10.523124, 3.341]) vector_ceiled = np.ceil(vector) vector_floored = np.floor(vector) vector_rounded = np.round(vector, 2) . Numpy also provides some mathematical functions and constants. For example np.pi returns the value of pi, np.e returns Eulers number. Other mathematical operations include all angle computations such as np.sin(), np.cos() etc. These are all computed in radians, but you can turn them into degrees with np.degrees() . Exercise . Lets just do one quick exercise on numpy to get familiar. 1. Create a numpy array from 0 to 20 in steps of 0.1. 2. Compute the sin of the data, then compute the standard deviation of the sin data 3. Add some random noise to the data. To do so, use the np.random.rand(). The range of the noise should be between 0 and 0.5. Then compute the standard deviation of the noisy data. 4. Round the noisy values to 3 decimal places . Solution! vector = np.arange(0,21,.1) sin_vector = np.sin(vector) std_sin_vector = sin_vector.std() noisy_sin_vector = sin_vector + np.random.rand(len(sin_vector))*0.5 std_noisy_sin_vector = noisy_sin_vector.std() rounded_noisy_sin_vector = np.round(noisy_sin_vector, 3) . ",
    "url": "/New_BAI_DataAnalysis/python_2_data_visualization.html#2-a-quick-touch-on-numpy",
    
    "relUrl": "/python_2_data_visualization.html#2-a-quick-touch-on-numpy"
  },"15": {
    "doc": "2. DATA HANDLING AND VISUALIZATION",
    "title": "3. Data Visualization: Plotly",
    "content": "Finally! It is time to not only create endless boring arrays of numbers, but to mold them into beautiful, descriptive images that tell the story of what the data actually means. Because that is essentially what we are doing when plotting data. Nobody can look at a table of 100.000 rows and start talking about it, that is what we can achieve with data visualization. There are several libraries we could use for plotting in Python: . | Matplotlib: One of the most widely used frameworks. It is lightweight, built into Pandas but nobody really likes the syntax | Seaborn: A library built on top of Matplotlib. It makes the syntax quite a bit easier, provides nice out-of-the-box plot styles, but the documentation is a bit lacking and plots are not that easy to customize | Plotly: The solution we will be using here. Plotly is built on a Javascript library Plotly.js and therefor brings some unique features to the table. The syntax and strcuture is quite good to learn, it offers a load of customization. Additionally, it offers very nice interactivity with the plots which makes exploration of your data much easier | . Lucky for us, Plotly is already included in Anaconda, so we do not need to install it. Plotly provides two different approaches to plotting: . | Quick and easy plots with less customization using plotly express | fully fledged figures with full customization options using graphic-objects | . To get a good understanding of Plotly it makes sense to go from large to small, first looking at the general structure of Plotly figures and the way graphic_objects work. If you have a broad overview of these you can still learn about the quick-and-easy ways, but you will have a much easier time when you want to change something about the express solutions manually. ",
    "url": "/New_BAI_DataAnalysis/python_2_data_visualization.html#3-data-visualization-plotly",
    
    "relUrl": "/python_2_data_visualization.html#3-data-visualization-plotly"
  },"16": {
    "doc": "2. DATA HANDLING AND VISUALIZATION",
    "title": "Plotly - The modern plotting library",
    "content": "Where to find help . First of all lets gather some ressources. The two best places to find advice about any plotly-related questions are . | the official documentation at plotly.com | the plotly community forum | as always, Stackoverflow… | . The general structure of Plotly figures . First of all we need to go through a little bit of vocabulary to be able to talk about Plotly. In Plotly-world the whole image of a plot, including the axes, the data, the labels, the title and everything is called the “graph-object”. This is the top-level of every Plotly figure and it is also the name of the Python class, with which we build the plots. Within the graph-object there are two layers: One is the “data” layer with everything that is directly related to the displayed data. That is the data itself, the mode of repesentation in the graph for example the line (in a line-plot) or points (in a scatter-plot) and the styling such as the size or color of the line/points. In plotly, they also call the group of data-related attributes “traces”. Don’t ask me how they came up with it but we have to live with it… We will come back to that later! The second part of the figure is the “layout” layer. It includes everything that makes the graph besides the data itself, for example the axes, the titles on the axes, the title of the graph itself, the legend, colors, maybe a template and so on. In the image below I tried to highlight the areas including the “data” area in red and the “layout” related areas in green: . Lets dive into the code and create a first figure object. Its easy: . # Before we start, lets resample the dwd data down to daily values. # You will create quite some plots and plotting 27 years of 10-minute # data takes a bit of time. df_dwd_daily = df_dwd.resample(\"d\", on=\"data_time\").mean() # First import the graph_objects module from plotly. # We call it \"go\" because that is convention import plotly.graph_objects as go import plotly.io as pio pio.renderers.default = \"browser\" #This will force Plotly to open plots in your default web browser. # Then we create out figure like this: fig = go.Figure() # Check out what happens, when you print this # object with print(fig). You will see the structure # we talked about above! . Well, now we have a graph-object without any data. From printing the figure you can see that the “data” is an empty list. Lets change that and add some data from out dwd-dataset. To do so, we have to add a “trace” (remember how we introduced that above). We do that by calling the .add_trace() method on the figure. In the function the first thing we have to define is, what kind of graph we want to create. Otherwise the empty figure wouldn’t know whether it should become a scatter-plot, a histogram, a line-plot or anything else. We define the type of graph by giving an object of the graph-type we want to the “add_trace()” method. These objects are also included in our “graph-objects” (or “go”). Sounds complicated, but really it is not. Check this out: . # This is the bare figure fig = go.Figure() # Now we will add some data: fig.add_trace( # On fig we call the \"add_trace()\" method go.Scatter( # In the method we provide an object of type \"Scatter\" from \"go\" x = df_dwd_daily.index, # then, in go.Scatter we define, which data should be plotted y = df_dwd_daily[\"tair_2m_mean\"],# on the x- and on the y-axis ) ) # Now print the figure again and look the output # You will see that the \"data\" level now has the x- and y-data in it # Plotly has very nice interactivity. To open the graph # in an interactive browser-window type this: fig.show() # or you save the figure to an image like this: fig.write_image(\"daily_tair_2m_mean.png\") . Above we created a scatter-plot (every data point is a dot in the graph). But if you look at the plot, you’ll note that there is still a lot missing. Most importantly, it does not have axis-labels. We need to add those, so people know what is plotted here! Lets do it. Which part of the figure do you think we need to change to add axis-labels? . SolutionThe \"layout\" bit of the figure So lets see how we can change the layout of the figure! . # to get to the layout of the figure we have two options: # 1. The figure object \"fig\" has the \"layout\" property, # which has an \"xaxis\" property, which again has a \"title\" # property. We can go down this path manually like this: fig.layout.xaxis.title = \"Date\" fig.layout.yaxis.title = \"Tair [F]\" # 2. The second option is to use the \"update_layout() method. # This was was made to make styling more convenient. We can use # it to \"group\" our styling in a single function call. fig.update_layout( xaxis_title=\"Date\", # Note that we use an underscore yaxis_title=\"Tair [F]\" # \"_\" to grab the \"title\" property from \"xaxis\" ) # Now you'll see, that the labels are changed in the figure: fig.show() . This is pretty much the way you can change any attribute that is related to the layout of the figure. The only thing you have to figure out for whatever you want to change in your figure is, where the respective property lies. Is it part of the data or the layout layer? Which sub-layers are there? Sometimes you can figure it out by thinking about it, however you can always refer to the documentation and the hive-mind of the internet. Especially in the beginning you’ll need to google quite a bite, but once you get the hang of it, it is actually quite intuitive. Lets do some more styling. Above we created a scatter-plot. This is a time-series, so maybe a line-plot would be more appropriate… . Exercise 1 . Try to change the style of our plot above to a line-plot. To do so, you need to change the \"mode\" property which is part of the \"data\" layer, or \"trace\". You can change the trace just like we changed the \"layout\" above with a function called \"update_traces(). Challenge: Can you come up with two different ways to change the mode? . Solution! # Option 1: fig.update_traces( mode=\"lines\" ) # Option 2 (which you usually wouldn't use): fig.data[0].mode = \"lines\" # The trick is that we have to write # fig.data[0], because the \"data\" property is # a list! You can see that if you look at the # printed figures \"data\" property, it starts with a \"[\". # The reason is of course that you could plot several # lines within a single plot. This way you could style # them one-by-one. However, generally you use the # \"update_traces()\" method to apply styles that # are used for all plotted data and pass everything # else directly when you create the data with \"add_trace()\" . Exercise 2 . Now lets expand the plot a bit. Add two more lines to the plot, the tair_2m_min and tair_2m_max columns from our dwd data. You can simply add them to the existing plot with the \"add_trace()\" method. When calling add_trace(), try to directly change the mode to \"lines\". When adding the lines, also add the argument \"name\" to the add_trace() method. That defines, how the line will be reprented in the legend. Give appropriate names to the lines. Additionally, try to change the line style of the min and max temperature to \"dashed\". If you want, you can also change the colors of the lines. To do so, change the line_color property. To define the color you can use either a string in the form of \"rgb(0,0,0)\" where you have to replace the zeros with rgb values, or you use one of the pre-defined colors which you can also pass as string. You can find a list of available color-names here: . w3schools list of CSS colors… . Solution! fig = go.Figure() fig.add_trace( go.Scatter( y = df_dwd_daily[\"tair_2m_mean\"], name=\"Tair 2m\", line_color=\"black\" ) ) # after adding the first line we just keep adding # more lines. We can directly change the name, # line_dash and line_color attributes: fig.add_trace( go.Scatter( y = df_dwd_daily[\"tair_2m_min\"], name=\"Tair 2m min\", line_dash=\"dash\", line_color = \"lightblue\" ) ) fig.add_trace( go.Scatter( y = df_dwd_daily[\"tair_2m_max\"], name=\"Tair 2m max\", line_dash=\"dash\", line_color=\"lightcoral\" ) ) fig.update_layout(xaxis_title=\"Date\", yaxis_title=\"T2m [F]\") fig.show() . Great, you are on the best way to becoming a Data-Painting Plotly-Wizard! Of course there are not just simple line and scatter charts. There is a whole world of graphs to explore!. For now, lets look at just one more type of graph, a bar-chart. This is a common type of graph to compare measured amounts (as opposed to discrete values such as a temperature). Such a value would be our rainfall measurement! . Exercise . Go ahead and create a bar chart of the sum of daily rainfall. You can create a bar-chart just like we did with the scatter plots above, only that you call \"go.Bar\" instead of \"go.Scatter\" Remember that when resampling precipitation to daily values, you need to use the sum instead of the mean! . Solution! # First of all we grab daily precipitation data by # resampling with the \"sum\" aggregation function # Here I directly grabbed only the precipitation-column, # but you can also do it another way precip = df_dwd.resample(rule=\"d\", on=\"data_time\").sum()[\"precipitation\"] # Now we create the graph just like before: fig_precip = go.Figure() fig_precip.add_trace( go.Bar( # Here we simply use go.Bar instead of go.Scatter x=precip.index, y=precip # Note: when using a Series instead of a dataframe ) # I dont have to pass the column name, because I ) # only have one column anyways... fig_precip.update_layout( xaxis_title=\"Date\", yaxis_title=\"Rain amount, daily [mm]\" ) fig_precip.show() . Right on, this was quite a deep dive into the Plotly library! But if you followed all the way down here, you are on a very good way to become super proficient in plotting data in python! The skills you got from the exercise above should get you quite far in designing your own figures in the future. If it is all a bit much in the start, don’t worry! As time comes you will do things much faster. For now, keep trying, keep googling, consult the documentation and most importantly: be happy with the progress you make! . Before we finish the visualization exercises I want to show a few more very helpful things. Subplots . Often you want to create not just one but multiple plots in one figure, for example one big figure with a temperature plot on top and a precipitation plot on the bottom. This way readers can easily get an overview of the climate at the station. Creating such a “subplot” in Plotly is super easy! Instead of using go.Figure(), you use a different function to create your top-layer “graph-object”. The function we need is plotly.subplots.make_subplots(). In it we can define the number of rows and columns of figures we want to create with the “rows” and “cols” keywords. Think about the whole figure like a matrix. The figure on the top-left will be row 1, column 1, second on the left row 2, column 1 etc. Then whenever you are adding a new trace, you can define its position with the properties “row” and “col”: . # First we create the subplots graph_object. # To do so we have to import that specific method: from plotly.subplots import make_subplots # Now we create a subplot figure with two rows: fig_subplots = plotly.subplots.make_subplots(rows=2, cols=1) # Now we can start adding traces to the figure: fig_subplots.add_trace( go.Scatter( x=df_dwd_daily.index, y=df_dwd_daily.tair_2m_mean, name=\"Tair mean\", line_color = \"black\" ), row = 1, # here we define, where the figure should be col=1, ) fig_subplots.add_trace( go.Bar( x=precip.index, y=precip, name=\"precip\", marker_color=\"blue\" ), row = 2, # precipitation will be the lower plot col=1 ) fig_subplots.show() . As a little exercise, print the fig_subplots object from above and try to figure out how to change the y-axis titles on the first and the second plot. Solution fig_subplots.update_layout( xaxis2_title=\"Date\", yaxis_title=\"Tair 2m [F]\", yaxis2_title=\"Rain amount, daily [mm]\" ) . Templates . One very nice way to style your figure a bit differently than the default is to use Plotly templates! You can implement them in your figure simply by adding the template in the layout: . fig_subplots.update_layout( template=\"simple-white\" ) fig_subplots.show() . Looks nice right? There is a whole gallery of templates available on the website: Plotly template gallery… . Plotly Express . For now, we will leave it with that. But wait, I was talking about an easier way to create graphs before right? Yes, for “quick and dirty” graphs you can use the awesome plotly.express shortcut. With it you can create a bunch of graphs like the ones we talked about above without all the fuzz of graph_objects etc. All you need to do to create a scatter plot is . import plotly.express as px fig_express = px.scatter(df_dwd_daily, y=\"tair_2m_mean\") fig_express.show() # You can update the fig_express just the same as the output # of go.Figure(), with update_layout and all of its beauty. Plotly express is very well connected with Pandas. This enables you to display all data of a dataframe in one combined, interactive overview char, simply by passing a pandas dataframe into it: . import plotly.express as px fig_express = px.scatter(df_dwd_daily) fig_express.show() . Plotly express includes many other graph types as well. You can find a very nice documentation on the plotly website…. One very last very useful thing I want to mention here is the addition of trendlines in Plotly Express. It is a super handy feature that is only implemented in Plotly Express, not in plain Plotly. I will show you how to do it and how you can grab all information the trendline can give you, but I will not explain the statistics behind it here. It happens very often that you want to plot two variables together, to see whether they are related. In order to explore the relationship, you can fit a linear line to the data and look at the parameters of the line-equation. For example, if I would plot air temperature on the x-axis against the same air-temperature on the y-axis, that would result in a perfect fit, the line would have a slope of exactly 1, the y-axis-intercept would be 0 and the r_square value of the regression would be 1, indicating the perfect fit. In Plotly express you can simply add a trendline to the plot by adding the “trendline” argument to the function call: . import plotly.express as px fig_express = px.scatter(df_dwd_daily, x=\"pressure_air\", y=\"tair_2m_mean\", trendline=\"ols\") results = px.get_trendline_results() print(results) fig_express.show() . But the bare line itself is not too useful, we need to grab the coefficients of the line, so the slope and the intercept of the line-equation. You can grab the results from the figure using px.get_trendline_results(fig_express). Honestly, the actual parameters are a bit hidden inside the deeper objects, but you can extract each one anyways: . import plotly.express as px fig_express = px.scatter(df_dwd_daily, x=\"pressure_air\", y=\"tair_2m_mean\", trendline=\"ols\") results = px.get_trendline_results(fig_express) # To actually access the results of the regression we need to # access the first row of the px_fit_results: ols_results = results.px_fit_results.iloc[0] print(ols_results.summary()) # ols_results is an object of the type \"OLSResults\" # which comes from a different library, \"statsmodels\" (https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLSResults.html) # From it you can grab the coefficients and a bunch of statistical infos: slope = ols_results.params[1] intercept = ols_results.params[0] rsquared = ols_results.rsquared # etc... I won’t go much more into the details because you can easily look up more plotly express functions (and I do encourage you to do so because it is super handy!), but by now you know enough about Plotly to explore that yourself. Why did we go through all the fuzz of handling the traces and layout ourselves? Because express is limited! If you want to customize your plots in some way it does not support out-of-the-box, you will have to dive into the figure-structure sooner or later, and now you know how. Still it is encouraged (even by Plotly themselves) to make use of both: run plotly express for a base figure or for quick data exploration, and then style the figure the way you want with the in-depth methods. ",
    "url": "/New_BAI_DataAnalysis/python_2_data_visualization.html#plotly---the-modern-plotting-library",
    
    "relUrl": "/python_2_data_visualization.html#plotly---the-modern-plotting-library"
  },"17": {
    "doc": "2. DATA HANDLING AND VISUALIZATION",
    "title": "Congratulations",
    "content": "on finishing this plot about plotting plots with Plotly! What a journey. I hope you can take away the knowledge to navigate plotting in Python fairly well from now on! Enjoy your future data-adventures and I wish you happy coding! . ",
    "url": "/New_BAI_DataAnalysis/python_2_data_visualization.html#congratulations",
    
    "relUrl": "/python_2_data_visualization.html#congratulations"
  },"18": {
    "doc": "2. DATA HANDLING AND VISUALIZATION",
    "title": "2. DATA HANDLING AND VISUALIZATION",
    "content": " ",
    "url": "/New_BAI_DataAnalysis/python_2_data_visualization.html",
    
    "relUrl": "/python_2_data_visualization.html"
  },"19": {
    "doc": "3. REGRESSION",
    "title": "Regression",
    "content": " ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#regression",
    
    "relUrl": "/python_3_regression.html#regression"
  },"20": {
    "doc": "3. REGRESSION",
    "title": "Table of Contents",
    "content": ". | What’s Regression All About? | Simple Linear Regression | Multiple Regression | Machine Learning with Random Forests | Gap-filling in Time Series | . Welcome! This tutorial will walk you through regression analysis - one of the most useful tools you’ll encounter for making sense of ecological data. We’ll start from the basics and work our way up to more advanced machine learning methods. Don’t worry if statistics isn’t your strong suit. We’ll take it step by step, and by the end you should feel comfortable applying these techniques to your own data. ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#table-of-contents",
    
    "relUrl": "/python_3_regression.html#table-of-contents"
  },"21": {
    "doc": "3. REGRESSION",
    "title": "1. What’s Regression All About?",
    "content": " ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#1-whats-regression-all-about",
    
    "relUrl": "/python_3_regression.html#1-whats-regression-all-about"
  },"22": {
    "doc": "3. REGRESSION",
    "title": "The Basic Idea",
    "content": "Here’s the thing: as ecologists, we’re constantly trying to figure out what drives the patterns we observe. Why are there more species in some places than others? What makes trees grow faster? How does temperature affect animal behavior? . Regression gives us a way to quantify these relationships. Instead of just saying “warmer temperatures seem to increase growth,” we can say “for every 1°C increase in temperature, tree ring width increases by 0.15 mm.” That’s powerful stuff. At its core, regression asks: how does one thing change when another thing changes? . ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#the-basic-idea",
    
    "relUrl": "/python_3_regression.html#the-basic-idea"
  },"23": {
    "doc": "3. REGRESSION",
    "title": "A Quick Example",
    "content": "Let’s say you’re studying tree growth across a temperature gradient. You measure tree ring widths at different sites: . | Mean Annual Temperature (°C) | Tree Ring Width (mm) | . | 8 | 1.2 | . | 10 | 1.8 | . | 12 | 2.4 | . | 14 | 2.9 | . | 16 | 3.2 | . You can see there’s a pattern - warmer sites have wider rings. But how strong is this relationship? Can we predict growth at a site with 11°C mean temperature? Regression helps us answer these questions. ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#a-quick-example",
    
    "relUrl": "/python_3_regression.html#a-quick-example"
  },"24": {
    "doc": "3. REGRESSION",
    "title": "What Can Regression Achieve?",
    "content": "In ecological research, regression is useful for: . Making predictions - You’ve measured carbon flux at 20 sites, but you want to estimate it across the whole landscape. Regression lets you predict values at unmeasured locations based on environmental variables you can get from satellite data. Understanding relationships - Does nitrogen addition actually increase plant biomass? By how much? Is the effect statistically significant or could it just be noise? . Figuring out what matters - When you have 15 environmental variables that might explain species richness, regression helps you sort out which ones are actually important. Supporting management decisions - If you know how much habitat area affects population size, you can make informed recommendations about reserve design. ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#what-can-regression-achieve",
    
    "relUrl": "/python_3_regression.html#what-can-regression-achieve"
  },"25": {
    "doc": "3. REGRESSION",
    "title": "Terminology",
    "content": "Before we dive in, let’s get our vocabulary straight. Different fields use different terms for the same things, which can be confusing. Target variable . | You can also call it response variable, dependent variable, outcome | The variable you’re trying to predict or explain | We usually call it y | Examples: species richness, biomass, survival rate, carbon flux | . independent variables . | Can also be termed predictors, features, independent variables, explanatory variables | The variable you use to make predictions or explain target variables | We call these x (or x₁, x₂, etc. when there are several) | Examples: temperature, precipitation, soil pH, elevation | . The model: . | This is the mathematical equation that describes how x relates to y | General form: y = f(x) + error | The “error” part is important - it acknowledges that our model won’t be perfect | . Coefficients: . | These are the numbers in our model that define the relationship | In a simple model like y = 3 + 2x, the “3” is the intercept and “2” is the slope | We estimate these from our data | . Residuals: . | The difference between what we observed and what our model predicted | Small residuals = good model fit | Patterns in residuals = something’s wrong with our model | . ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#terminology",
    
    "relUrl": "/python_3_regression.html#terminology"
  },"26": {
    "doc": "3. REGRESSION",
    "title": "How Does Regression Actually Work?",
    "content": "The basic process goes like this: . | Choose a model type. Are you assuming a straight line relationship? A curve? Multiple predictors? . | Fit the model to your data. This means finding the coefficient values that make your predictions as close to the observations as possible. | Check if it worked. Look at how well the model fits, whether the assumptions are met, and whether the results make ecological sense. | . The most common approach for step 2 is called “least squares” - we find the coefficients that minimize the sum of squared differences between observed and predicted values. We square the differences so that positive and negative errors don’t cancel out. ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#how-does-regression-actually-work",
    
    "relUrl": "/python_3_regression.html#how-does-regression-actually-work"
  },"27": {
    "doc": "3. REGRESSION",
    "title": "Evaluating Your Model",
    "content": "How do you know if your model is any good? A few key metrics: . R² (R-squared): This tells you what fraction of the variation in your data is explained by the model. An R² of 0.7 means your model explains 70% of the variance. What’s “good” depends entirely on your system - in controlled experiments 0.9 might be expected, while in field ecology 0.3 might be excellent. $$ R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2} $$ Where: . $y_i$ is the observed value . $\\hat{y}_i$ is the predicted value . $\\bar{y}$ is the mean of observed values . $SS_{res}$ is the sum of squared residuals . $SS_{tot}$ is the total sum of squares . RMSE (Root Mean Square Error): This is the average size of your prediction errors, in the same units as your response variable. An RMSE of 2.5°C for a temperature model means your predictions are typically off by about 2.5 degrees. $$ RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2} $$ MAE (Mean Absolute Error): Similar to RMSE but less sensitive to outliers. Useful when you have some weird extreme values in your data. $$ MAE = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i| $$ The difference between RMSE and MAE? RMSE penalizes large errors more heavily because of the squaring. If you have a few really bad predictions, RMSE will be much higher than MAE. This can be useful for detecting outliers or problematic predictions. ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#evaluating-your-model",
    
    "relUrl": "/python_3_regression.html#evaluating-your-model"
  },"28": {
    "doc": "3. REGRESSION",
    "title": "2. Simple Linear Regression",
    "content": "Alright, let’s actually do some regression. We’ll start with the simplest case: one predictor, one response, straight line relationship. ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#2-simple-linear-regression",
    
    "relUrl": "/python_3_regression.html#2-simple-linear-regression"
  },"29": {
    "doc": "3. REGRESSION",
    "title": "The Model",
    "content": "Simple linear regression fits this equation: . ŷ = β₀ + β₁x . Where: . | ŷ (y-hat) is our predicted value of the response variable | β₀ (beta-zero) is the intercept (value of y when x is zero) | β₁ (beta-one) is the slope (how much y changes for each unit increase in x) | x is our predictor variable | . That’s it. We’re just fitting a line through our data points. What’s Actually Happening? . When we fit a regression, we’re looking for the line that minimizes the total squared distance between our observed data points and the line. These distances are called residuals - the difference between what we actually observed and what our model predicted. The full model, including the error, is: . y = β₀ + β₁x + ε . Where ε (epsilon) represents the residual error - all the variation in y that our model doesn’t capture. In a perfect world with a perfect model, ε would be zero. In reality, it never is. Why squared distances? Squaring does two things: (1) it treats positive and negative errors equally (a prediction 10g too high is just as bad as 10g too low), and (2) it penalizes large errors more heavily than small ones. This method is called Ordinary Least Squares (OLS). ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#the-model",
    
    "relUrl": "/python_3_regression.html#the-model"
  },"30": {
    "doc": "3. REGRESSION",
    "title": "Let’s Try It: Penguin Body Mass and Flipper Length",
    "content": "We’ll use the Palmer Penguins dataset - real measurements collected by Dr. Kristen Gorman at Palmer Station, Antarctica. import numpy as np import pandas as pd import plotly.express as px import plotly.graph_objects as go # Load the penguins dataset # You can install it with: pip install palmerpenguins from palmerpenguins import load_penguins penguins = load_penguins() # Take a look at what we have print(\"Dataset shape:\", penguins.shape) print(\"\\nFirst few rows:\") print(penguins.head()) # Check for missing values and drop them for now penguins_clean = penguins.dropna(subset=['flipper_length_mm', 'body_mass_g']) print(f\"\\nComplete cases: {len(penguins_clean)}\") . Now let’s explore the relationship between flipper length and body mass: . # Quick visualization fig = px.scatter(penguins_clean, x='flipper_length_mm', y='body_mass_g', color='species', title='Penguin Body Mass vs Flipper Length') fig.update_layout(template='simple_white', font_size = 36,) fig.show() . You’ll see there’s clearly a positive relationship - longer flippers go with heavier birds. Let’s quantify it. Fitting the Regression . from sklearn.linear_model import LinearRegression # Prepare the data # sklearn expects X to be a 2D array (rows = samples, columns = features) # Even with one feature, we need shape (n_samples, 1), not (n_samples,) # That's why we use [['flipper_length_mm']] (double brackets) instead of ['flipper_length_mm'] X = penguins_clean[['flipper_length_mm']].values y = penguins_clean['body_mass_g'].values # Fit the model model = LinearRegression() model.fit(X, y) print(f\"Slope: {model.coef_[0]:.2f} g per mm\") print(f\"Intercept: {model.intercept_:.2f} g\") print(f\"R-squared: {model.score(X, y):.3f}\") . Visualizing the Fit . # Get predictions for the regression line # We create a sequence of x values spanning our data range # reshape(-1, 1) converts the 1D array to 2D (required by sklearn) X_line = np.linspace(penguins_clean['flipper_length_mm'].min(), penguins_clean['flipper_length_mm'].max(), 100).reshape(-1, 1) y_line = model.predict(X_line) fig = go.Figure() # Data points fig.add_trace(go.Scatter( x=penguins_clean['flipper_length_mm'], y=penguins_clean['body_mass_g'], mode='markers', name='Observations', marker=dict(size=24, opacity=0.6) )) # Regression line fig.add_trace(go.Scatter( x=X_line.flatten(), y=y_line, mode='lines', name='Regression line', line=dict(color='red', width=2) )) fig.update_layout( title='Penguin Body Mass vs Flipper Length', xaxis_title='Flipper Length (mm)', yaxis_title='Body Mass (g)', template='simple_white', font_size = 36 ) fig.show() . ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#lets-try-it-penguin-body-mass-and-flipper-length",
    
    "relUrl": "/python_3_regression.html#lets-try-it-penguin-body-mass-and-flipper-length"
  },"31": {
    "doc": "3. REGRESSION",
    "title": "What Do These Numbers Mean?",
    "content": "With real data, you should get something like: . | Slope ≈ 49.7 g/mm: For every 1 mm increase in flipper length, body mass increases by about 50 grams. This is our β₁. | Intercept ≈ -5781 g: This would be the predicted mass at flipper length = 0, which makes no biological sense (negative mass!), but it’s needed mathematically to position the line correctly within the range of our actual data. This is our β₀. | R² = 0.76 means flipper length explains about 76% of the variation in body mass. The remaining 24% is unexplained variation (our ε). | . Making Predictions . # What's the predicted mass for a penguin with 200mm flippers? new_flipper = np.array([[200]]) # Note: 2D array for sklearn predicted_mass = model.predict(new_flipper) print(f\"Predicted mass for 200mm flipper: {predicted_mass[0]:.0f} g\") # What about 180mm? new_flipper = np.array([[180]]) predicted_mass = model.predict(new_flipper) print(f\"Predicted mass for 180mm flipper: {predicted_mass[0]:.0f} g\") . A Note on Extrapolation . Be careful about predicting outside the range of your training data! Our model was built on penguins with flippers roughly 170-230mm. If you try to predict mass for a 100mm flipper or a 300mm flipper, you’re extrapolating - assuming the linear relationship continues outside the observed range. This is often dangerous because: . | Relationships may be non-linear at extremes | You have no data to validate predictions in that range | Biological constraints may make extrapolations impossible (like our negative-mass intercept) | . Stick to interpolation (predicting within your data range) whenever possible. Try It Yourself . Using the Palmer Penguins dataset, fit a simple regression predicting bill length from bill depth. What do you find? Is the relationship positive or negative? How does R² compare to the flipper-mass relationship? . Solution! from palmerpenguins import load_penguins from sklearn.linear_model import LinearRegression penguins = load_penguins().dropna(subset=['bill_length_mm', 'bill_depth_mm']) X = penguins[['bill_depth_mm']].values y = penguins['bill_length_mm'].values model = LinearRegression() model.fit(X, y) print(f\"Slope: {model.coef_[0]:.3f}\") print(f\"Intercept: {model.intercept_:.3f}\") print(f\"R-squared: {model.score(X, y):.3f}\") # Surprise! You'll find a NEGATIVE relationship and very low R² # This is Simpson's paradox - when you combine the species, # the overall trend is negative, but within each species # the relationship is positive. This is because Gentoo penguins # have long bills but shallow depth, while Adelie have # shorter bills but deeper depth. # Try it by species: for species in penguins['species'].unique(): subset = penguins[penguins['species'] == species] X_sp = subset[['bill_depth_mm']].values y_sp = subset['bill_length_mm'].values model_sp = LinearRegression().fit(X_sp, y_sp) print(f\"{species}: slope = {model_sp.coef_[0]:.2f}, R² = {model_sp.score(X_sp, y_sp):.3f}\") . ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#what-do-these-numbers-mean",
    
    "relUrl": "/python_3_regression.html#what-do-these-numbers-mean"
  },"32": {
    "doc": "3. REGRESSION",
    "title": "Limitation: Simpson’s Paradox",
    "content": "The exercise above reveals something important: simple regression can be misleading when you have groups in your data. This phenomenon is called Simpson’s Paradox - when a trend appears in several groups of data but disappears or reverses when the groups are combined. In our case: . | Within each species: deeper bills → longer bills (positive relationship) | Across all species combined: deeper bills → shorter bills (negative relationship!) | . How can this be? It’s because species differ systematically in both variables. Gentoo penguins have long but shallow bills; Adelie penguins have shorter but deeper bills. When you ignore species, these group differences create an artificial negative trend. The flipper-mass relationship also differs among species - Gentoo penguins are bigger than Adelie and Chinstrap overall. This is one reason we need multiple regression - to account for additional factors that might be confounding our results. ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#limitation-simpsons-paradox",
    
    "relUrl": "/python_3_regression.html#limitation-simpsons-paradox"
  },"33": {
    "doc": "3. REGRESSION",
    "title": "3. Multiple Regression",
    "content": "In the real world, ecological responses depend on many factors simultaneously. Multiple regression lets us include all of them in one model. ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#3-multiple-regression",
    
    "relUrl": "/python_3_regression.html#3-multiple-regression"
  },"34": {
    "doc": "3. REGRESSION",
    "title": "Why Go Multiple?",
    "content": "Looking at the penguin data, body mass depends on more than just flipper length: . | Species differ in overall body size | Males are larger than females | Bill dimensions correlate with mass too | . If we only model mass against flipper length, we’re missing important information. Multiple regression lets us ask: “What’s the effect of flipper length, after accounting for species and sex?” . This is fundamentally different from simple regression. We’re no longer asking “how does y change with x?” but rather “how does y change with x₁, holding x₂, x₃, etc. constant?” . ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#why-go-multiple",
    
    "relUrl": "/python_3_regression.html#why-go-multiple"
  },"35": {
    "doc": "3. REGRESSION",
    "title": "The Model",
    "content": "We extend our simple model to include multiple predictors: . ŷ = β₀ + β₁x₁ + β₂x₂ + β₃x₃ + … . Each coefficient now tells us the partial effect of that variable - its effect while holding the others constant. This is crucial for interpretation and is what makes multiple regression so powerful for teasing apart confounded relationships. ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#the-model-1",
    
    "relUrl": "/python_3_regression.html#the-model-1"
  },"36": {
    "doc": "3. REGRESSION",
    "title": "Example: Predicting Penguin Body Mass",
    "content": "Let’s build a more complete model for penguin body mass. from palmerpenguins import load_penguins from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.preprocessing import LabelEncoder import sklearn.metrics as metrics # Load and prepare data penguins = load_penguins().dropna() # Encode categorical variables (species and sex) as numbers # LabelEncoder converts text categories to integers: 0, 1, 2, etc. le_species = LabelEncoder() le_sex = LabelEncoder() penguins['species_code'] = le_species.fit_transform(penguins['species']) penguins['sex_code'] = le_sex.fit_transform(penguins['sex']) print(\"Species encoding:\", dict(zip(le_species.classes_, range(len(le_species.classes_))))) print(\"Sex encoding:\", dict(zip(le_sex.classes_, range(len(le_sex.classes_))))) # Check what we have print(f\"\\nDataset: {len(penguins)} penguins\") print(penguins[['species', 'sex', 'flipper_length_mm', 'bill_length_mm', 'bill_depth_mm', 'body_mass_g']].head()) . A Note on Encoding Categorical Variables . We just converted species (Adelie, Chinstrap, Gentoo) to numbers (0, 1, 2). This is called label encoding and it has a subtle problem: it implies an ordering. The model might think Gentoo (2) is “more” than Adelie (0) in some way. For binary variables like sex (male/female → 0/1), this is fine - we’re just measuring the difference between two groups. For multi-category variables, a better approach is one-hot encoding (also called dummy variables), where each category gets its own 0/1 column. We’ll keep label encoding here for simplicity, but be aware this is a simplification. In a real analysis, you’d want to use one-hot encoding or a statistical framework that handles categories properly. Check Correlations First . Before building a model, it’s good practice to explore how your variables relate to each other: . # Which variables correlate with body mass? numeric_cols = ['flipper_length_mm', 'bill_length_mm', 'bill_depth_mm', 'body_mass_g', 'species_code', 'sex_code'] print(\"\\nCorrelations with body mass:\") print(penguins[numeric_cols].corr()['body_mass_g'].sort_values(ascending=False)) . Why check correlations? . | It gives you a preview of which variables might be useful predictors | It reveals potential multicollinearity - when predictors are highly correlated with each other | . Multicollinearity is a problem because if two predictors are highly correlated, it becomes hard to separate their individual effects. The model can’t tell if it’s variable A or variable B causing the effect. You’ll get unstable coefficient estimates. We’ll keep an eye on this. Why Split Into Training and Test Sets? . This is a fundamental concept in predictive modeling: . # Prepare features and target X = penguins[['flipper_length_mm', 'bill_length_mm', 'bill_depth_mm', 'species_code', 'sex_code']] y = penguins['body_mass_g'] # Split into training and test sets X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=42 ) print(f\"Training set: {len(X_train)} penguins\") print(f\"Test set: {len(X_test)} penguins\") . Why do we split the data? . If we train our model on all the data and then evaluate it on the same data, we’re essentially asking “how well did you memorize this?” A model could achieve a perfect score by memorizing every data point without learning any real patterns. What we actually want to know is: “how well will this model perform on new, unseen data?” . By holding out a test set that the model never sees during training, we can get an honest estimate of how well the model will generalize to new penguins. | Training set (70%): Used to fit the model | Test set (30%): Used only for evaluation, never for fitting | . The random_state=42 ensures we get the same split every time we run the code (for reproducibility). Fitting the Multiple Regression . # Fit the model model = LinearRegression() model.fit(X_train, y_train) # Look at coefficients print(\"\\nModel coefficients:\") print(f\" Intercept: {model.intercept_:.1f}\") for name, coef in zip(X.columns, model.coef_): print(f\" {name}: {coef:.2f}\") . ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#example-predicting-penguin-body-mass",
    
    "relUrl": "/python_3_regression.html#example-predicting-penguin-body-mass"
  },"37": {
    "doc": "3. REGRESSION",
    "title": "Interpreting the Results",
    "content": "What do these coefficients actually mean? This is where multiple regression differs fundamentally from simple regression. flipper_length_mm: Each additional mm of flipper length adds about 17g to body mass, after controlling for other variables. Note this is smaller than in simple regression (~50g). Why? Because some of that apparent flipper effect was actually due to species differences—Gentoo penguins have both longer flippers AND higher mass. Once we account for species, the “pure” flipper effect is smaller. bill_length_mm: Longer bills are associated with slightly higher mass, holding other variables constant. bill_depth_mm: Deeper bills are associated with higher mass. This makes sense—it’s a measure of overall head size. species_code: The coefficient shows average difference between species (encoded as 0, 1, 2). Interpretation is tricky with encoded categories because we’re treating it as a continuous variable. This is a limitation of our simple encoding approach. sex_code: Males (coded as 1) are heavier than females (coded as 0) by about this many grams, controlling for body measurements. This is the easiest to interpret—it’s the male-female difference in mass after accounting for differences in flipper length, bill size, etc. The Key Insight: “Controlling For” Other Variables . The phrase “controlling for” or “holding constant” is crucial in multiple regression. Here’s what it means: . Imagine you could magically find two penguins that are: . | The same species | The same sex | Have the same bill length | Have the same bill depth | But differ in flipper length by 1mm | . The flipper coefficient tells you how much heavier we’d expect the longer-flippered penguin to be. Of course, we can’t actually find such perfectly matched penguins. Multiple regression does this statistically by partitioning the variation in body mass among all the predictors. How Good Is Our Model? . # Predict on test data (data the model has never seen) y_pred = model.predict(X_test) # Calculate metrics r2 = metrics.r2_score(y_test, y_pred) rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred)) mae = metrics.mean_absolute_error(y_test, y_pred) print(f\"R-squared: {r2:.3f}\") print(f\"RMSE: {rmse:.1f} g\") print(f\"MAE: {mae:.1f} g\") . Understanding the Evaluation Metrics: . | R² (R-squared): Same interpretation as before - proportion of variance explained. But now we’re measuring it on the test set, so it tells us how well the model generalizes. | RMSE (Root Mean Squared Error): The square root of the average squared prediction error. It’s in the same units as your response variable (grams), so you can interpret it directly: “on average, our predictions are off by about RMSE grams.” RMSE penalizes large errors heavily because of the squaring. | MAE (Mean Absolute Error): The average absolute prediction error. Also in grams. MAE treats all errors equally regardless of size. If RMSE is much larger than MAE, you have some predictions with large errors. | . # Plot predicted vs observed fig = px.scatter(x=y_test, y=y_pred, labels={'x': 'Observed Mass (g)', 'y': 'Predicted Mass (g)'}) fig.add_scatter(x=[y_test.min(), y_test.max()], y=[y_test.min(), y_test.max()], mode='lines', name='1:1 line', line=dict(dash='dash', color='red')) fig.update_layout(template='simple_white', title=f'Multiple Regression: R² = {r2:.3f}', font_size = 36) fig.update_traces(marker_size = 24) fig.show() . Interpreting the predicted vs. observed plot: If predictions were perfect, all points would fall exactly on the red 1:1 line. The scatter around that line shows prediction error. Look for patterns - if errors are larger for heavy penguins than light ones, your model might have heteroscedasticity issues. ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#interpreting-the-results",
    
    "relUrl": "/python_3_regression.html#interpreting-the-results"
  },"38": {
    "doc": "3. REGRESSION",
    "title": "Does Adding Variables Help?",
    "content": "Let’s compare models with different numbers of predictors: . results = [] # Just flipper length m1 = LinearRegression().fit(X_train[['flipper_length_mm']], y_train) results.append({ 'Model': 'Flipper only', 'R²': m1.score(X_test[['flipper_length_mm']], y_test) }) # Flipper + species m2 = LinearRegression().fit(X_train[['flipper_length_mm', 'species_code']], y_train) results.append({ 'Model': 'Flipper + Species', 'R²': m2.score(X_test[['flipper_length_mm', 'species_code']], y_test) }) # Flipper + species + sex m3 = LinearRegression().fit(X_train[['flipper_length_mm', 'species_code', 'sex_code']], y_train) results.append({ 'Model': 'Flipper + Species + Sex', 'R²': m3.score(X_test[['flipper_length_mm', 'species_code', 'sex_code']], y_test) }) # All predictors m4 = LinearRegression().fit(X_train, y_train) results.append({ 'Model': 'All predictors', 'R²': m4.score(X_test, y_test) }) print(pd.DataFrame(results)) . You should see R² improve as you add relevant predictors - species and sex both contribute meaningful information about body mass. But be careful! R² will always increase (or stay the same) when you add more predictors to a model, even if those predictors are useless. This is because more parameters give the model more flexibility to fit the training data. This is why we evaluate on a test set - on new data, useless predictors will actually hurt performance by adding noise. This is called overfitting, and it’s a central concern in machine learning. Exercise . Build a multiple regression model predicting bill length from bill depth, flipper length, species, and sex. Which predictors have the strongest effects? Does the bill depth coefficient change from the simple regression? . Solution! from palmerpenguins import load_penguins from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.preprocessing import LabelEncoder penguins = load_penguins().dropna() # Encode categoricals le_species = LabelEncoder() le_sex = LabelEncoder() penguins['species_code'] = le_species.fit_transform(penguins['species']) penguins['sex_code'] = le_sex.fit_transform(penguins['sex']) # Prepare data X = penguins[['bill_depth_mm', 'flipper_length_mm', 'species_code', 'sex_code']] y = penguins['bill_length_mm'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Full model model = LinearRegression() model.fit(X_train, y_train) print(f\"R²: {model.score(X_test, y_test):.3f}\\n\") print(\"Coefficients:\") for name, coef in zip(X.columns, model.coef_): print(f\" {name}: {coef:.3f}\") # Compare to simple regression simple = LinearRegression() simple.fit(penguins[['bill_depth_mm']], penguins['bill_length_mm']) print(f\"\\nSimple regression bill_depth coefficient: {simple.coef_[0]:.3f}\") print(f\"Multiple regression bill_depth coefficient: {model.coef_[0]:.3f}\") # The bill_depth coefficient changes dramatically - in simple regression # it's negative (Simpson's paradox), but in multiple regression # controlling for species, it becomes positive as expected. ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#does-adding-variables-help",
    
    "relUrl": "/python_3_regression.html#does-adding-variables-help"
  },"39": {
    "doc": "3. REGRESSION",
    "title": "Limitations of Multiple Regression",
    "content": "Multiple regression is powerful but has some important limitations: . 1. It assumes linear relationships. The model assumes each predictor has a straight-line relationship with the response. If flipper length and mass have a curved relationship (they often do at extremes), a linear model won’t capture that properly. You might need polynomial terms or transformations. 2. It assumes additive effects. The model says the effect of flipper length is the same for all species - we just shift the intercept for each species. In reality, species might differ in their flipper-mass scaling (different slopes). This would require interaction terms. 3. It assumes independent errors. If you measured the same penguin multiple times, or penguins from the same colony are more similar, you violate the independence assumption. You’d need mixed-effects models for such data. 4. Outliers can have outsized influence. A single unusual data point can dramatically shift your regression line. Always check for influential observations. These limitations bring us to machine learning approaches, which can handle more complexity. ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#limitations-of-multiple-regression",
    
    "relUrl": "/python_3_regression.html#limitations-of-multiple-regression"
  },"40": {
    "doc": "3. REGRESSION",
    "title": "4. Machine Learning with Random Forests",
    "content": "Alright, now we’re getting to the fun stuff. Machine learning sounds fancy, but the basic idea is simple: let the algorithm figure out the patterns in your data, rather than you specifying them in advance. ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#4-machine-learning-with-random-forests",
    
    "relUrl": "/python_3_regression.html#4-machine-learning-with-random-forests"
  },"41": {
    "doc": "3. REGRESSION",
    "title": "Why Machine Learning?",
    "content": "Ecological relationships are often messy: . | Relationships might be non-linear (e.g., growth rates that plateau) | Effects of one variable depend on another (interactions) | There might be thresholds we didn’t anticipate | The functional form might be completely unknown | . Machine learning algorithms can discover these patterns automatically. You don’t have to know the shape of the relationship beforehand. The trade-off: Machine learning models are often less interpretable than linear regression. You might get better predictions but less insight into why. This is sometimes called the “black box” problem. ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#why-machine-learning",
    
    "relUrl": "/python_3_regression.html#why-machine-learning"
  },"42": {
    "doc": "3. REGRESSION",
    "title": "Decision Trees: The Building Block",
    "content": "Before we get to Random Forests, we need to understand decision trees. They’re surprisingly intuitive. The Basic Idea . A decision tree is basically a flowchart of yes/no questions: . Is flipper length &gt; 206mm? ├── Yes → Probably a Gentoo, predict ~5000g └── No → Is bill depth &gt; 18mm? ├── Yes → Probably Adelie, predict ~3700g └── No → Probably Chinstrap, predict ~3500g . The algorithm figures out: . | Which questions to ask (which variable to split on) | What thresholds to use (why 206mm and not 200mm?) | When to stop asking questions | . How Does It Choose Splits? . At each step, the algorithm tries every possible split of every variable and picks the one that creates the most “pure” groups - groups where the response values are most similar to each other. For regression, “purity” is measured by variance. A good split creates child nodes with lower variance in the response than the parent node. The Goal: Reduce Variance . Before any split, a node contains samples with some variance in the target variable. The tree wants to split these samples into two groups where: . | Group 1 (left child): samples are similar to each other | Group 2 (right child): samples are similar to each other | . Even if the two groups have very different means, that’s fine, what matters is that within each group, values are more similar than before. The Algorithm . For every possible split (every feature × every threshold): . | Divide samples into left and right groups based on the split | Calculate the weighted average variance of the two groups | Pick the split that gives the lowest weighted variance | . Mathematically, we want to minimize: . Cost = (n_left / n_total) × MSE_left + (n_right / n_total) × MSE_right . Where MSE (Mean Squared Error) measures variance: . MSE = (1/n) × Σ(yᵢ - ȳ)² . from sklearn.tree import DecisionTreeRegressor, plot_tree import matplotlib.pyplot as plt # Using our penguin data X = penguins[['flipper_length_mm', 'bill_length_mm', 'bill_depth_mm', 'species_code', 'sex_code']] y = penguins['body_mass_g'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Fit a simple tree # max_depth=3 limits the tree to 3 levels of questions tree = DecisionTreeRegressor(max_depth=3, random_state=42) tree.fit(X_train, y_train) # Visualize it plt.figure(figsize=(16, 8)) plot_tree(tree, feature_names=X.columns, filled=True, rounded=True, fontsize=9) plt.title('Decision Tree for Penguin Body Mass') plt.tight_layout() plt.show() print(f\"Decision Tree R²: {tree.score(X_test, y_test):.3f}\") . Reading the Tree Visualization . Each box in the tree visualization shows: . | The splitting rule (e.g., “flipper_length_mm &lt;= 206.5”) | samples: how many training samples reached this node | value: the predicted value (mean of samples at this node) | squared_error (or mse): the variance of samples at this node | . The colors indicate the predicted value - similar colors mean similar predictions. The Overfitting Problem . Decision trees are easy to interpret - you can literally see the rules. But they have a critical flaw: they tend to overfit. A deep tree can grow until each leaf contains just one sample, essentially memorizing the training data perfectly. But this memorization doesn’t generalize - the model fails on new data because it learned noise, not signal. Try this experiment: . # A tree with no depth limit deep_tree = DecisionTreeRegressor(random_state=42) # No max_depth deep_tree.fit(X_train, y_train) print(f\"Deep tree - Training R²: {deep_tree.score(X_train, y_train):.3f}\") print(f\"Deep tree - Test R²: {deep_tree.score(X_test, y_test):.3f}\") . You’ll likely see near-perfect training R² but worse test R² - classic overfitting! . ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#decision-trees-the-building-block",
    
    "relUrl": "/python_3_regression.html#decision-trees-the-building-block"
  },"43": {
    "doc": "3. REGRESSION",
    "title": "Random Forests: Many Trees Are Better Than One",
    "content": "Random Forests solve the overfitting problem through a clever strategy: build many trees and average their predictions. Individual trees might make mistakes, but their errors tend to cancel out. How It Works . Each tree in the forest is built differently: . | Bootstrap sampling: Each tree is trained on a random sample of the data, drawn with replacement (some observations appear multiple times, others not at all). This is called a “bootstrap sample.” . | Random feature selection: At each split, instead of considering all variables, only a random subset is considered. This prevents all trees from making identical decisions. | . This randomness means individual trees are “weaker” (less accurate) than a fully-grown single tree. But their collective wisdom is stronger and more robust. The magic: When you average many imperfect but different predictions, the random errors cancel out, while the true signal remains. from sklearn.ensemble import RandomForestRegressor # Fit a random forest rf = RandomForestRegressor( n_estimators=100, # number of trees in the forest max_depth=10, # how deep each tree can go min_samples_leaf=5, # minimum samples required at each leaf random_state=42 # for reproducibility ) rf.fit(X_train, y_train) # Evaluate y_pred = rf.predict(X_test) print(f\"Random Forest R²: {metrics.r2_score(y_test, y_pred):.3f}\") print(f\"Random Forest RMSE: {np.sqrt(metrics.mean_squared_error(y_test, y_pred)):.1f} g\") . Key Hyperparameters . Hyperparameters are settings you choose before training (unlike model parameters like coefficients, which are learned from data): . | n_estimators: Number of trees. More trees = more stable predictions, but slower training. 100-500 is usually enough; returns diminish beyond that. | max_depth: Maximum depth of each tree. Shallower trees are simpler and less prone to overfitting. Try 5-20 for most problems. | min_samples_leaf: Minimum samples required at each leaf node. Higher values prevent the tree from creating leaves with just one or two samples, reducing overfitting. | max_features: Number of features to consider at each split. Default is sqrt(n_features) for classification, n_features/3 for regression. Lower values increase randomness between trees. | . ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#random-forests-many-trees-are-better-than-one",
    
    "relUrl": "/python_3_regression.html#random-forests-many-trees-are-better-than-one"
  },"44": {
    "doc": "3. REGRESSION",
    "title": "Comparing All Our Methods",
    "content": "Let’s see how everything stacks up on the penguin data: . results = [] # Simple regression simple = LinearRegression() simple.fit(X_train[['flipper_length_mm']], y_train) pred = simple.predict(X_test[['flipper_length_mm']]) results.append({ 'Method': 'Simple regression', 'R²': metrics.r2_score(y_test, pred), 'RMSE': np.sqrt(metrics.mean_squared_error(y_test, pred)) }) # Multiple regression multi = LinearRegression() multi.fit(X_train, y_train) pred = multi.predict(X_test) results.append({ 'Method': 'Multiple regression', 'R²': metrics.r2_score(y_test, pred), 'RMSE': np.sqrt(metrics.mean_squared_error(y_test, pred)) }) # Decision tree tree = DecisionTreeRegressor(max_depth=10, random_state=42) tree.fit(X_train, y_train) pred = tree.predict(X_test) results.append({ 'Method': 'Decision tree', 'R²': metrics.r2_score(y_test, pred), 'RMSE': np.sqrt(metrics.mean_squared_error(y_test, pred)) }) # Random forest rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42) rf.fit(X_train, y_train) pred = rf.predict(X_test) results.append({ 'Method': 'Random forest', 'R²': metrics.r2_score(y_test, pred), 'RMSE': np.sqrt(metrics.mean_squared_error(y_test, pred)) }) print(pd.DataFrame(results).to_string(index=False)) . ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#comparing-all-our-methods",
    
    "relUrl": "/python_3_regression.html#comparing-all-our-methods"
  },"45": {
    "doc": "3. REGRESSION",
    "title": "What’s Driving the Patterns? Feature Importance",
    "content": "One of the nicest things about Random Forests is that they tell you which variables matter most for predictions: . importance = pd.DataFrame({ 'Variable': X.columns, 'Importance': rf.feature_importances_ }).sort_values('Importance', ascending=False) print(\"\\nVariable importance:\") print(importance.to_string(index=False)) # Plot it fig = px.bar(importance, x='Importance', y='Variable', orientation='h', title='What drives penguin body mass?') fig.update_layout(template='simple_white', yaxis={'categoryorder': 'total ascending'}) fig.show() . How Is Feature Importance Calculated? . The default importance measure (called “mean decrease in impurity” or “Gini importance”) is based on how much each feature contributes to reducing prediction error across all trees: . | Every time a feature is used to make a split, it reduces the impurity (variance) somewhat | Sum up these reductions across all splits and all trees | Normalize so importances sum to 1 | . For the penguin data, you’ll probably find that sex and flipper length are the most important predictors - which makes biological sense! . A Caution About Feature Importance . This default importance measure has known biases: . | It favors features with many unique values (continuous &gt; categorical) | It favors features that are correlated with other features | It doesn’t tell you about the direction of the effect | . For more reliable importance estimates, consider permutation importance: randomly shuffle one feature and see how much performance drops. If shuffling a feature hurts predictions a lot, that feature was important. from sklearn.inspection import permutation_importance perm_importance = permutation_importance(rf, X_test, y_test, n_repeats=10, random_state=42) perm_imp_df = pd.DataFrame({ 'Variable': X.columns, 'Importance': perm_importance.importances_mean }).sort_values('Importance', ascending=False) print(\"\\nPermutation importance:\") print(perm_imp_df.to_string(index=False)) . Try It Yourself . Remember the Simpson's paradox problem from Chapter 2? Simple regression showed a negative relationship between bill depth and bill length, but within each species the relationship was positive. Use Random Forest to predict bill_length_mm from bill_depth_mm, flipper_length_mm, body_mass_g, species, and sex. Compare its performance to the multiple regression model. Does Random Forest handle this complexity better? . Solution! from sklearn.ensemble import RandomForestRegressor from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.preprocessing import LabelEncoder import sklearn.metrics as metrics from palmerpenguins import load_penguins import numpy as np penguins = load_penguins().dropna() # Encode categorical variables le_species = LabelEncoder() le_sex = LabelEncoder() penguins['species_code'] = le_species.fit_transform(penguins['species']) penguins['sex_code'] = le_sex.fit_transform(penguins['sex']) # Prepare data - predict bill_length from other features X = penguins[['bill_depth_mm', 'flipper_length_mm', 'body_mass_g', 'species_code', 'sex_code']] y = penguins['bill_length_mm'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Fit both models lr = LinearRegression() lr.fit(X_train, y_train) rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42) rf.fit(X_train, y_train) # Compare performance print(\"Model Comparison for Predicting Bill Length:\") print(\"-\" * 50) for name, model in [(\"Linear Regression\", lr), (\"Random Forest\", rf)]: pred = model.predict(X_test) r2 = metrics.r2_score(y_test, pred) rmse = np.sqrt(metrics.mean_squared_error(y_test, pred)) print(f\"{name}: R² = {r2:.3f}, RMSE = {rmse:.2f} mm\") # Feature importance from Random Forest print(\"\\nRandom Forest Feature Importance:\") for name, imp in sorted(zip(X.columns, rf.feature_importances_), key=lambda x: x[1], reverse=True): print(f\" {name}: {imp:.3f}\") # Key insight: bill_depth now shows POSITIVE importance! # Both methods handle the confounding by controlling for species. ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#whats-driving-the-patterns-feature-importance",
    
    "relUrl": "/python_3_regression.html#whats-driving-the-patterns-feature-importance"
  },"46": {
    "doc": "3. REGRESSION",
    "title": "When to Use What?",
    "content": "| Method | Use When | Advantages | Disadvantages | . | Simple Regression | One predictor, linear relationship, need interpretability | Simple, coefficients are meaningful | Can’t handle multiple predictors or non-linearity | . | Multiple Regression | Multiple predictors, linear relationships, need to understand effects | Coefficients are interpretable, can control for confounders | Assumes linearity and additivity | . | Decision Tree | Need interpretable rules, non-linear relationships | Easy to explain, handles non-linearity | Prone to overfitting, unstable | . | Random Forest | Prediction is main goal, complex non-linear relationships | Excellent predictive performance, robust | Less interpretable, slower to train | . ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#when-to-use-what",
    
    "relUrl": "/python_3_regression.html#when-to-use-what"
  },"47": {
    "doc": "3. REGRESSION",
    "title": "5. Gap-filling in Time Series",
    "content": "Now let’s apply what we’ve learned to a practical problem: dealing with missing data in ecological time series. ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#5-gap-filling-in-time-series",
    
    "relUrl": "/python_3_regression.html#5-gap-filling-in-time-series"
  },"48": {
    "doc": "3. REGRESSION",
    "title": "The Problem",
    "content": "If you’ve worked with field data, you know this frustration. Your sensor died for a week. The battery ran out during the coldest part of winter. Someone accidentally unplugged the datalogger. Missing data is annoying because: . | You can’t calculate annual totals or means | It messes up time series analyses | Some statistical methods can’t handle NaN values | . ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#the-problem",
    
    "relUrl": "/python_3_regression.html#the-problem"
  },"49": {
    "doc": "3. REGRESSION",
    "title": "Loading Messy Data",
    "content": "We will use some data I have prepared in a way that you might find it in an online data portal. Download the file here To test some things we will work with the air temperature column “tair_2m_mean” here. There are several issues when we have a missing-data-placeholder like that. Try two things: Real data often has placeholder values instead of proper missing data markers. Let’s see an example: . import pandas as pd import numpy as np # Load meteorological data df = pd.read_parquet('./dwd_ahaus_1996_2023_missing_placeholders.parquet') df[\"data_time\"] = pd.to_datetime(df[\"data_time\"]) print(f\"Temperature range: {df['tair_2m_mean'].min():.1f} to {df['tair_2m_mean'].max():.1f}\") . If you see -999.99 as the minimum, that’s a placeholder for missing data - not an actual temperature! We need to fix this: . # Replace placeholder with NaN df.loc[df[\"tair_2m_mean\"] == -999.99, \"tair_2m_mean\"] = np.NaN # Now check print(f\"Missing values: {df['tair_2m_mean'].isna().sum()}\") . ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#loading-messy-data",
    
    "relUrl": "/python_3_regression.html#loading-messy-data"
  },"50": {
    "doc": "3. REGRESSION",
    "title": "Method 1: Linear Interpolation",
    "content": "The simplest approach - just draw a straight line between known values: . # Pandas makes this easy df['temp_interp'] = df['tair_2m_mean'].interpolate(method='linear') . This works fine for short gaps. If temperature was 10°C at noon and 14°C at 2pm, it’s reasonable to guess 12°C at 1pm. But it fails badly for longer gaps. It can’t capture the daily temperature cycle - if you have a 24-hour gap, linear interpolation will give you a flat line right through where the daily max and min should be. ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#method-1-linear-interpolation",
    
    "relUrl": "/python_3_regression.html#method-1-linear-interpolation"
  },"51": {
    "doc": "3. REGRESSION",
    "title": "Method 2: Regression-Based Gap Filling",
    "content": "If we have other variables that were measured continuously, we can use them to estimate the missing temperatures: . from sklearn.linear_model import LinearRegression # Solar radiation and humidity are often available when temperature fails # (different sensors) predictors = ['SWIN', 'rH'] # Get data where everything is present (for training) df_complete = df[['data_time', 'SWIN', 'rH', 'tair_2m_mean']].dropna() X = df_complete[predictors] y = df_complete['tair_2m_mean'] # Fit model model = LinearRegression() model.fit(X, y) print(f\"Gap-filling model R²: {model.score(X, y):.3f}\") . Then we can predict temperature wherever we have radiation and humidity data: . # Find rows where temp is missing but predictors exist mask = df['tair_2m_mean'].isna() &amp; df['SWIN'].notna() &amp; df['rH'].notna() df.loc[mask, 'temp_regression'] = model.predict(df.loc[mask, predictors]) . ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#method-2-regression-based-gap-filling",
    
    "relUrl": "/python_3_regression.html#method-2-regression-based-gap-filling"
  },"52": {
    "doc": "3. REGRESSION",
    "title": "Method 3: Random Forest Gap Filling",
    "content": "For better accuracy, especially with complex patterns, Random Forest often wins: . from sklearn.ensemble import RandomForestRegressor # Use more predictors all_predictors = ['SWIN', 'rH', 'pressure_air', 'wind_speed', 'precipitation'] df_complete = df[all_predictors + ['tair_2m_mean']].dropna() X = df_complete[all_predictors] y = df_complete['tair_2m_mean'] rf = RandomForestRegressor(n_estimators=100, max_depth=15, random_state=42) rf.fit(X, y) print(f\"Random Forest R²: {rf.score(X, y):.3f}\") # Check which predictors matter most for estimating temperature for name, imp in sorted(zip(all_predictors, rf.feature_importances_), key=lambda x: x[1], reverse=True): print(f\" {name}: {imp:.3f}\") . ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#method-3-random-forest-gap-filling",
    
    "relUrl": "/python_3_regression.html#method-3-random-forest-gap-filling"
  },"53": {
    "doc": "3. REGRESSION",
    "title": "Which Method When?",
    "content": "After working with a lot of gap-filled data, here’s what I’ve found: . Short gaps (a few hours): Linear interpolation is usually fine. Temperature doesn’t change that fast. Medium gaps (a day or two): Regression with environmental predictors. This captures the daily cycle if you have radiation data. Long gaps (weeks+): Random Forest or similar, but honestly… consider whether you should be filling such long gaps at all. Sometimes it’s better to acknowledge the data is missing. General advice: . | Always validate your gap-filling on data where you know the truth | Flag gap-filled values in your final dataset | Report the uncertainty or error in your gap-filled values | Don’t over-fill - sometimes missing data should stay missing | . Try It Yourself . Compare linear interpolation vs. Random Forest for filling a 24-hour gap in temperature data. Which method captures the daily cycle better? . Solution! # The key insight is that linear interpolation can't capture # diurnal patterns, while Random Forest (using radiation as # a predictor) can. # For a 24-hour gap: # - Linear interpolation draws a flat line # - Random Forest predicts warm during day, cool at night # (because it learned that high radiation = high temp) # In my experience, Random Forest reduces RMSE by 30-50% # compared to linear interpolation for day-long gaps. # But for gaps under 3-6 hours, the methods are often similar # because temperature hasn't changed much anyway. ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#which-method-when",
    
    "relUrl": "/python_3_regression.html#which-method-when"
  },"54": {
    "doc": "3. REGRESSION",
    "title": "Wrapping Up",
    "content": "We’ve covered a lot of ground here. Let me leave you with the key takeaways: . Simple regression is your starting point. It’s easy to understand, easy to explain, and often good enough for straightforward questions. Multiple regression lets you account for multiple drivers at once. The coefficients tell you the effect of each variable while controlling for the others. Random Forests can capture complex patterns that regression misses. They’re particularly good when you don’t know the shape of the relationships in advance. For gap-filling, match your method to your gap length. Simple interpolation for short gaps, model-based methods for longer ones. Most importantly: always plot your data, check your assumptions, and validate on independent test data. No amount of fancy statistics can fix bad data or inappropriate models. Good luck with your analyses! . ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#wrapping-up",
    
    "relUrl": "/python_3_regression.html#wrapping-up"
  },"55": {
    "doc": "3. REGRESSION",
    "title": "Where to Go From Here",
    "content": "If you want to dig deeper: . | Generalized Additive Models (GAMs) let you fit smooth curves instead of straight lines | Mixed-effects models handle hierarchical data (e.g., measurements nested within sites) | Gradient Boosting (XGBoost) often outperforms Random Forests for prediction | Time series methods (ARIMA, etc.) are specifically designed for temporal data | . But honestly, you can get surprisingly far with just regression and Random Forests. Master these first before moving on to fancier tools. ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#where-to-go-from-here",
    
    "relUrl": "/python_3_regression.html#where-to-go-from-here"
  },"56": {
    "doc": "3. REGRESSION",
    "title": "3. REGRESSION",
    "content": " ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html",
    
    "relUrl": "/python_3_regression.html"
  },"57": {
    "doc": "4. FLUX CALCULATION",
    "title": "Flux Calculation",
    "content": "In this tutorial, we’re going to analyze the data you collected on your field trip to the Lüner forest! Your instruments measured raw gas concentrations, but as ecologists, we need to turn that into gas fluxes. Why? Because fluxes represent a rate—the speed at which gases are being exchanged. With CO₂ fluxes, we can estimate crucial metrics like ecosystem respiration (RECO) and net ecosystem exchange (NEE). With fluxes of a potent greenhouse gas like Nitrous Oxide (N₂O), we can understand a key part of the nitrogen cycle. This guide will walk you through the entire process: from cleaning the raw concentration data, to calculating meaningful fluxes, and finally to comparing the results between different land cover types. Notice: In the following sections, we will start using new functions and libraries that we haven’t introduced yet. Don’t worry or feel overwhelmed! This is a normal part of learning to code. For each new tool we use, I will: Briefly explain what it is and why we are using it. Provide a link to its official documentation if you’re curious and want to learn more. Think of it as adding new tools to your data analysis toolbox. We’ll introduce them one at a time, right when we need them. Table of Contents . | Read in and merge data files | Visualizing and cleaning the data | Calculating flux for a single measurement | Automating gas flux calculation | . ",
    "url": "/New_BAI_DataAnalysis/python_4_flux_calculation.html#flux-calculation",
    
    "relUrl": "/python_4_flux_calculation.html#flux-calculation"
  },"58": {
    "doc": "4. FLUX CALCULATION",
    "title": "1.Read in and merge data files",
    "content": "Different from the simple CSV files we might have worked with before, the raw data from the gas analyzer is more complex. When you open the file, you’ll see it contains two parts: A metadata header: This block at the top contains useful information about the measurement (like timezone, device model, etc.), but we don’t need it for our flux calculations. The data block: This is the core data we need, with columns for date, time, and gas concentrations. Our first challenge is to programmatically read only the data block and ignore the metadata. Model: LI-7820 SN: TG20-01072 Software Version: 2.3.8 Timestamp: 2025-08-15 11:00:00 Timezone: Europe/Paris . To do this, we’ll need the pandas library for creating our DataFrame and the io library, we need to import them. import pandas as pd import io . Our strategy will be to read the file line-by-line, find the start of the data, and then pass only those lines to pandas. 1.1 Loading N₂O Data . The analyzer produces tab-separated files with a metadata block at the top. The data section is marked by a line starting with DATAH. Our strategy is to read the file line-by-line, find the DATAH marker, and pass only the data lines to pandas. Reading and parsing the file . First, we read the entire file into a single string, and then split that string into a list of individual lines. This gives us the flexibility to find our data “landmarks.” . # Read in raw data as a string with open(\"./BAI_StudyProject_LuentenerWald/raw_data/N2O/TG20-01072-2025-08-15T110000.data.txt\") as f: file_content = f.read() # Split the string into a list of lines. # '\\n' is the special character for a newline. lines = file_content.strip().split('\\n') . Next, we need to find the exact line that contains our column headers. Looking at the file, we know this line always starts with the word DATAH. We can write a short command to find the index of that line. # This code searches through our list 'lines' and gets the index of the first line that starts with 'DATAH' header_index = next(i for i, line in enumerate(lines) if line.startswith('DATAH')) # The actual data starts 2 lines after the header line (to skip the \"DATAU\" units line) data_start_index = header_index + 2 # Now we can grab the headers themselves from that line. The values are separated by tabs ('\\t'). headers = lines[header_index].split('\\t') . Using io.StringIO to Read Our Cleaned Data . The pd.read_csv() function is built to read from a file. We don’t have a clean file; we have a list of Python strings (lines) that we’ve already processed. So, how do we make pandas read from our list? We use io.StringIO to trick pandas. It takes our cleaned-up data lines and presents them to pandas as if they were a file stored in the computer’s memory. Info: The Python io module helps us manage data streams. io.StringIO specifically allows us to treat a regular text string as a &gt;file. This is incredibly useful when you need to pass text data to a function that expects a file, just like we’re doing &gt;with pd.read_csv(). # Join our data lines back into a single string, separated by newlines data_string = '\\n'.join(lines[data_start_index:]) # Read the data string into a DataFrame df_n2o = pd.read_csv( io.StringIO(data_string), # Treat our string as a file sep='\\t', # Tell pandas the data is separated by tabs header=None, # We are providing the headers ourselves, so there isn't one in the data names=headers, # Use the 'headers' list we extracted earlier na_values='nan' # Recognize 'nan' strings as missing values ) . Data Formatting . The last step is to tidy up the DataFrame. We will: Remove the useless DATAH column. Combine the separate DATE and TIME columns into a single Timestamp object. This is crucial for time-series analysis. Set this new Timestamp as the DataFrame’s index, which makes plotting and selecting data by time much easier. # Drop the first column which is just the 'DATAH' label if 'DATAH' in df_n2o.columns: df_n2o = df_n2o.drop(columns=['DATAH']) # Combine 'DATE' and 'TIME' into a proper Timestamp and set it as the index if 'DATE' in df_n2o.columns and 'TIME' in df_n2o.columns: df_n2o['Timestamp'] = pd.to_datetime(df_n2o['DATE'] + ' ' + df_n2o['TIME']) df_n2o = df_n2o.drop(columns=['DATE', 'TIME']) df_n2o = df_n2o.set_index('Timestamp') print(\"Data loaded and formatted successfully!\") print(df_n2o.head()) . Great! Now, we have successfully read in and formatted our raw data. However, think about our field campaigns. We went out several times and generate a new data file for each trip. If we wanted to analyze all of them, we would have to copy and paste our loading code multiple times. To avoid repetition and make our code cleaner and more reliable, it’s a best practice to wrap a reusable process into a function. Let’s turn our loading and cleaning steps into a function called load_n2o_data. Exercise . Try to write this function yourself based on the code snippets we created for data loading! Tip: The function will need to accept one argument: the filepath of the file you want to open. Solution! Note: how it’s the exact same logic as before, just defined within a def block. def load_n2o_data(filepath: str) -&gt; pd.DataFrame: \"\"\" Loads N2O data from a text file, remove metadata, and returns a DataFrame. Parameters: - filepath (str): The path to the input data file. Returns: - pd.DataFrame: A cleaned DataFrame with a DatetimeIndex. \"\"\" with open(filepath) as f: file_content = f.read() lines = file_content.strip().split('\\n') header_index = next(i for i, line in enumerate(lines) if line.startswith('DATAH')) data_start_index = header_index + 2 headers = lines[header_index].split('\\t') df_n2o = pd.read_csv( io.StringIO('\\n'.join(lines[data_start_index:])), sep='\\t', header=None, names=headers, na_values='nan' ) if 'DATAH' in df_n2o.columns: df_n2o = df_n2o.drop(columns=['DATAH']) if 'DATE' in df_n2o.columns and 'TIME' in df_n2o.columns: df_n2o['Timestamp'] = pd.to_datetime(df_n2o['DATE'] + ' ' + df_n2o['TIME']) df_n2o = df_n2o.drop(columns=['DATE', 'TIME']) df_n2o = df_n2o.set_index('Timestamp') print(\"N2O data loaded and cleaned successfully.\") return df_n2o . 1.2 Loading CH₄ and CO₂ Data . The GGA analyzer produces comma-separated files with a different structure: The first line includes instrument metadata (version, serial number, etc.), The second are column headers, Lines 3+ hold measurement data. Here’s an example of the first few lines: . VC:2f90039 BD:Jan 16 2014 SN: Time, [CH4]_ppm, [CH4]_ppm_sd, [H2O]_ppm, ... 08/15/2025 11:00:03.747, 2.080375e+00, 0.000000e+00, 1.103072e+03, ... However, GGA files contain extra non-data content at the end (such as digital signatures or log messages). We need to filter these out. Let’s build our loader step by step. Read the CSV File . First, we read the file with pd.read_csv(), skipping the first metadata line: . df_gga = pd.read_csv( \"./BAI_StudyProject_LuentenerWald/raw_data/GGA/gga_2025-08-15_f0000.txt\", skiprows=1, # Skip instrument metadata header (line 1) skipinitialspace=True # Handle leading whitespace in columns ) # Clean column names (remove extra spaces) df_gga.columns = df_gga.columns.str.strip() print(f\"Rows loaded: {len(df_gga)}\") df_gga.head() . Identify Valid Data Rows . If we look at the end of some files, we might find non-data content like this: . -----BEGIN PGP MESSAGE----- Version: GnuPG v1.4.11 (GNU/Linux) jA0EAwMC1o7j8zNG6eRgye1CgI1h0/yQoOa8fycg+... We need to keep only the rows where the Time column contains an actual timestamp. Valid timestamps in our data look like 08/15/2025 11:00:03.747 — they always start with a date in MM/DD/YYYY format. To identify these rows programmatically, we use a regular expression (regex). A regex is a pattern that describes what text should look like. Info: What is a Regular Expression? . A regular expression is a sequence of characters that defines a search pattern. It’s like a template that says “I’m looking for text that looks like THIS.” Regular expressions are extremely powerful for finding, matching, and filtering text data. Here’s the regex pattern we’ll use: ^\\s*\\d{2}/\\d{2}/\\d{4} . Let’s break it down piece by piece: . | Pattern | Meaning | Example Match | . | ^ | Start of the string | (anchors the match to the beginning) | . | \\s* | Zero or more whitespace characters | Matches leading spaces like \" \" | . | \\d{2} | Exactly 2 digits | 08 (month) | . | / | A literal forward slash | / | . | \\d{2} | Exactly 2 digits | 15 (day) | . | / | A literal forward slash | / | . | \\d{4} | Exactly 4 digits | 2025 (year) | . So the full pattern ^\\s*\\d{2}/\\d{2}/\\d{4} means: “Starting from the beginning, allow optional spaces, then match a date in MM/DD/YYYY format.” . Learn More About Regular Expressions . Regular expressions are a powerful tool worth learning. Here are some helpful resources: . | RegExr — Interactive regex tester with real-time explanations | Regex101 — Another great tester with detailed breakdown of patterns | Python re documentation — Official Python regex documentation | Regular Expressions Cheat Sheet — Quick reference for common patterns | . Let’s see it in action: . import re # Test the pattern on different strings pattern = r'^\\s*\\d{2}/\\d{2}/\\d{4}' test_strings = [ ' 08/15/2025 11:00:03.747', # Valid timestamp (with leading spaces) '08/15/2025 11:00:03.747', # Valid timestamp (no leading spaces) '-----BEGIN PGP MESSAGE-----', # Invalid (PGP signature) 'jA0EAwMC1o7j8zNG6eRgye1C', # Invalid (encrypted data) 'nan', # Invalid (missing value) ] for s in test_strings: match = bool(re.match(pattern, s)) print(f\"'{s[:30]:30s}' → {match}\") . Output: . ' 08/15/2025 11:00:03.747 ' → True '08/15/2025 11:00:03.747 ' → True '-----BEGIN PGP MESSAGE----- ' → False 'jA0EAwMC1o7j8zNG6eRgye1C ' → False 'nan ' → False . Now we can use this pattern to filter our DataFrame: . # Create a boolean mask: True for valid rows, False for invalid # As regular expression can only be applied on string, we need to make sure timestamp is in string format valid_mask = df_gga['Time'].astype(str).str.match(r'^\\s*\\d{2}/\\d{2}/\\d{4}') # Count how many rows we're keeping vs. dropping print(f\"Valid rows: {valid_mask.sum()}\") print(f\"Invalid rows (will be dropped): {(~valid_mask).sum()}\") # Keep only valid rows df_gga = df_gga[valid_mask].copy() . Parse Timestamps . Now that we have only valid data, we convert the Time column to proper datetime objects: . # Remove any leading/trailing whitespace and parse the timestamp df_gga['Time'] = pd.to_datetime( df_gga['Time'].str.strip(), format='%m/%d/%Y %H:%M:%S.%f' ) # Rename to 'Timestamp' for consistency and set as index df_gga = df_gga.rename(columns={'Time': 'Timestamp'}) df_gga = df_gga.set_index('Timestamp') print(f\"Time range: {df_gga.index.min()} to {df_gga.index.max()}\") print(\"check the last five rows of the table to see if we really removed the non-data part\") df_gga.tail() . Putting It All Together . Now let’s wrap everything into a reusable function: . def load_gga_data(filepath: str) -&gt; pd.DataFrame: \"\"\" Load CH4/CO2 data from a Los Gatos GGA analyzer file. Parameters: filepath: Path to the GGA .txt file Returns: DataFrame with DatetimeIndex \"\"\" # Step 1: Read CSV, skip metadata header df_gga = pd.read_csv( filepath, skiprows=1, skipinitialspace=True ) df_gga.columns = df_gga.columns.str.strip() # Step 2: Filter to valid data rows using regex # Pattern: optional whitespace, then MM/DD/YYYY date format valid_mask = df_gga['Time'].astype(str).str.match(r'^\\s*\\d{2}/\\d{2}/\\d{4}') df_gga = df_gga[valid_mask].copy() # Step 3: Parse timestamps and set as index df_gga['Time'] = pd.to_datetime(df_gga['Time'].str.strip(), format='%m/%d/%Y %H:%M:%S.%f') df_gga = df_gga.rename(columns={'Time': 'Timestamp'}) df_gga = df_gga.set_index('Timestamp') return df_gga . Let’s test it: . df_gga = load_gga_data(\"./BAI_StudyProject_LuentenerWald/raw_data/GGA/gga_2025-08-15_f0000.txt\") print(f\"Loaded {len(df_gga):,} rows\") print(f\"Time range: {df_gga.index.min()} to {df_gga.index.max()}\") df_gga.tail() . 1.3 Loading Multiple Files . Now that we have loader functions, we can easily handle data from multiple field trips. Instead of copying code, we can simply call our function in a loop. First, we create a list of all the file paths we want to load. Then, we can loop through this list, call our function for each path, and store the resulting DataFrames in a new list. # First, let's list all the files we want to load. # Make sure the file paths are complete and correct. base_path = \"./BAI_StudyProject_LuentenerWald/raw_data/\" n2o_files = [ 'N2O/TG20-01072-2025-08-15T110000.data.txt', 'N2O/TG20-01072-2025-08-26T093000.data.txt' ] gga_files = [ 'GGA/gga_2025-08-15_f0000.txt', 'GGA/gga_2025-08-06_f0000.txt', 'GGA/gga_2025-08-26_f0000.txt' ] # Create the full file paths n2o_full_file_paths = [base_path + name for name in n2o_files] gga_full_file_paths = [base_path + name for name in gga_files] # Create an empty list to hold the loaded DataFrames n2o_data_list = [] gga_data_list = [] # Loop through each path, load the data, and append it to our list for path in n2o_full_file_paths: df = load_n2o_data(path) n2o_data_list.append(df) print(f\"\\nSuccessfully loaded {len(n2o_data_list)} N2O data files.\") for path in gga_full_file_paths: df = load_gga_data(path) gga_data_list.append(df) print(f\"\\nSuccessfully loaded {len(gga_data_list)} GGA data files.\") . The loop above is clear and correct. However, a more concise way to write this in Python is with a list comprehension. It achieves the exact same result in a single, readable line: . n2o_data_list = [load_n2o_data(path) for path in n2o_full_file_paths] gga_data_list = [load_gga_data(path) for path in gga_full_file_paths] . For our flux calculations to be accurate, we need more than just gas concentrations. The Ideal Gas Law, which is the basis of the calculation, requires the temperature and air pressure at the time of each measurement. We will use the same workflow as before: load each file and then combine them. Exercise . You have two Excel files containing air temperature. Create lists of the file paths for the temperature data. Load each Excel file into a pandas DataFrame. Try using a list comprehension as we learned before! . Click here for the solution! # the base path is the same as before base_path = \"./BAI_StudyProject_LuentenerWald/raw_data/Ta/\" # --- Load Air Temperature Data --- file_names_Ta = [ 'Haube 2025-08-06 15_24_26 CEST (Data CEST).xlsx', 'Haube 2025-08-15 15_22_46 CEST (Data CEST).xlsx' , 'Haube 2025-08-26 14_58_20 MESZ (Data MESZ).xlsx' ] full_file_paths_Ta = [base_path + name for name in file_names_Ta] ta_data_list = [pd.read_excel(path) for path in full_file_paths_Ta] print(f\"Successfully loaded {len(ta_data_list)} air temperature files.\") . 1.4 Concatenating and Merging All Data . Now we combine everything into one master DataFrame. Understanding our data structure is important: . | Gas measurements (N₂O, CH₄, CO₂): Recorded at different times. They do NOT overlap in time, we simply need to stack them together. | Temperature data: Recorded continuously and DOES overlap with all gas measurements. We need to match each gas reading with its corresponding temperature. | . This means our workflow is: . | Concatenate all gas data files into one DataFrame (stacking rows) | Merge temperature into the gas data using time-matching | . Concatenate Gas Data from All Files . First, let’s combine files of the same type: . # Concatenate N2O data from multiple files df_n2o = pd.concat(n2o_data_list) df_n2o = df_n2o.sort_index() # Concatenate GGA data (CH4 and CO2) from multiple files df_gga = pd.concat(gga_data_list) df_gga = df_gga.sort_index() # Concatenate and format temperature data df_Ta = pd.concat(ta_data_list) df_Ta['Timestamp'] = pd.to_datetime(df_Ta['Date-Time (CEST)']) # Rename temperature column df_Ta['Ta_C'] = df_Ta['Temperature , °C'] df_Ta = df_Ta.set_index('Timestamp') df_Ta = df_Ta.sort_index() print(\"--- N2O DataFrame ---\") print(f\" Rows: {len(df_n2o):,}, Time range: {df_n2o.index.min()} to {df_n2o.index.max()}\") print(\"\\n--- GGA DataFrame (CH4/CO2) ---\") print(f\" Rows: {len(df_gga):,}, Time range: {df_gga.index.min()} to {df_gga.index.max()}\") print(\"\\n--- Temperature DataFrame ---\") print(f\" Rows: {len(df_Ta):,}, Time range: {df_Ta.index.min()} to {df_Ta.index.max()}\") . Combine All Gas Data into One Master Table . Since N₂O and CH₄/CO₂ measurements don’t overlap in time, we can safely stack them together. First, we need to select and rename columns so they’re consistent: . # Select key columns from N2O data df_n2o_clean = df_n2o[['N2O']].copy() df_n2o_clean.columns = ['N2O_ppb'] # Select key columns from GGA data (dry-corrected values) df_gga_clean = df_gga[['[CH4]d_ppm', '[CO2]d_ppm']].copy() df_gga_clean.columns = ['CH4_ppm', 'CO2_ppm'] # Stack them together - rows from different time periods df_gas = pd.concat([df_n2o_clean, df_gga_clean]) df_gas = df_gas.sort_index() print(f\"Combined gas DataFrame: {len(df_gas):,} rows\") print(f\"Time range: {df_gas.index.min()} to {df_gas.index.max()}\") df_gas.head() . Notice that each row will have values in either N2O_ppb OR CH4_ppm/CO2_ppm, but not both—because the measurements were taken at different times: . # Check the structure print(\"\\nSample from N2O measurement period:\") print(df_gas.loc[df_gas['N2O_ppb'].notna()].head(3)) print(\"\\nSample from CH4/CO2 measurement period:\") print(df_gas.loc[df_gas['CH4_ppm'].notna()].head(3)) . Merge Temperature with Gas Data . The gas analyzers record data every second, while the weather station might record only every minute. A simple merge would leave many empty rows. The solution is pd.merge_asof(), which performs a “nearest-neighbor” merge—ideal for combining time-series data with different frequencies. # Reset index for merge_asof (requires sorted column, not index) df_gas_reset = df_gas.reset_index() df_Ta_reset = df_Ta.reset_index() # Merge gas data with temperature # direction='backward' means: for each gas reading, find the most recent temperature df_merged = pd.merge_asof( df_gas_reset.sort_values('Timestamp'), df_Ta_reset[['Timestamp', 'Ta_C']].sort_values('Timestamp'), on='Timestamp', direction='backward' ) print(f\"Merged DataFrame: {len(df_merged):,} rows\") df_merged.head() . Final Master DataFrame . Let’s verify our final dataset: . print(\"--- Master DataFrame ---\") print(f\"Total rows: {len(df_merged):,}\") print(f\"Time range: {df_merged.index.min()} to {df_merged.index.max()}\") print(f\"\\nColumns: {list(df_merged.columns)}\") print(\"\\n--- Sample rows with N2O data ---\") from IPython.display import display display(df_merged.loc[df_merged['N2O_ppb'].notna()].head(100)) print(\"\\n--- Sample rows with CH4/CO2 data ---\") display(df_merged.loc[df_merged['CH4_ppm'].notna()].head(100)) . The master DataFrame now contains: . | N2O_ppb: N₂O concentration (only during N₂O measurement periods) | CH4_ppm: CH₄ concentration (only during GGA measurement periods) | CO2_ppm: CO₂ concentration (only during GGA measurement periods) | Ta_C: Air temperature (matched to each gas reading) | . ",
    "url": "/New_BAI_DataAnalysis/python_4_flux_calculation.html#1read-in-and-merge-data-files",
    
    "relUrl": "/python_4_flux_calculation.html#1read-in-and-merge-data-files"
  },"59": {
    "doc": "4. FLUX CALCULATION",
    "title": "2. Visualizing and Cleaning the Data",
    "content": "Now that we have a single, merged DataFrame, our next step is to inspect the data quality. Raw sensor data from the field is almost never perfect. Visualizing it is the best way to diagnose issues like noise, drift, or outliers before we attempt any calculations. 2.1 Creating a Reusable Plotting Function with Plotly . For visualization, we’ll use Plotly, a powerful library for creating interactive plots. Unlike static plots from Matplotlib, Plotly allows you to zoom, pan, and hover over data points—perfect for inspecting time-series data. Just as we did with data loading, we’ll be plotting our time-series data multiple times. To make this efficient and keep our plots looking consistent, let’s create a dedicated function. Exercise . The plotting function is partly provided below. Complete the function by filling in the add_trace() and update_layout() calls! . Hints: . | Use go.Scatter() for the trace with x=, y=, mode=, and name= parameters | The layout should include title, xaxis_title, yaxis_title, and template | . import plotly.graph_objects as go import plotly.io as pio # This setting forces Plotly to open plots in your default web browser pio.renderers.default = \"browser\" def plot_time_series(df, y_column, title, mode='lines'): \"\"\" Generates an interactive time-series plot using Plotly. Parameters: - df (pd.DataFrame): DataFrame with a DatetimeIndex. - y_column (str): The name of the column to plot on the y-axis. - title (str): The title for the plot. - mode (str): Plotly mode ('lines', 'markers', or 'lines+markers'). \"\"\" fig = go.Figure() fig.add_trace(...) # TODO: Add a Scatter trace fig.update_layout( ... # TODO: Set title, axis labels, and template ) fig.show() . Click here for the solution! import plotly.graph_objects as go import plotly.io as pio # This setting forces Plotly to open plots in your default web browser, # which can be more stable in some environments. pio.renderers.default = \"browser\" def plot_time_series(df, y_column, title, mode='lines'): \"\"\" Generates an interactive time-series plot using Plotly. This function will automatically try to set a 'Timestamp' column as the index if the existing index is not a datetime type. Parameters: - df (pd.DataFrame): DataFrame to plot. - y_column (str): The name of the column to plot on the y-axis. - title (str): The title for the plot. - mode (str): Plotly mode ('lines', 'markers', or 'lines+markers'). \"\"\" # --- Input Validation and Auto-Correction --- # Work on a copy to avoid changing the user's original DataFrame df_plot = df.copy() if not pd.api.types.is_datetime64_any_dtype(df_plot.index): print(\"Note: The DataFrame index is not a DatetimeIndex.\") if 'Timestamp' in df_plot.columns: print(\"--&gt; Found a 'Timestamp' column. Setting it as the index.\") df_plot['Timestamp'] = pd.to_datetime(df_plot['Timestamp']) df_plot = df_plot.set_index('Timestamp') else: raise TypeError( \"The DataFrame index is not a DatetimeIndex and a 'Timestamp' \" \"column was not found. Please set a DatetimeIndex before plotting.\" ) # --- Plotting --- fig = go.Figure() fig.add_trace(go.Scatter( x=df_plot.index, y=df_plot[y_column], mode=mode, name=y_column )) # Update layout for a clean, professional look fig.update_layout( title=title, xaxis_title='Time', yaxis_title=y_column, template='plotly_white', title_font=dict(size=24), xaxis=dict(tickfont=dict(size=14), title_font=dict(size=16)), yaxis=dict(tickfont=dict(size=14), title_font=dict(size=16)) ) fig.show() . 2.2 Visualizing the Raw Gas Data . Now, let’s use our new function to look at the raw N₂O data. The interactive plot allows you to zoom and pan to inspect noisy areas. # 1. Ensure Timestamp is the index and sorted # Only set index if 'Timestamp' is currently a column if 'Timestamp' in df_merged.columns: df_merged = df_merged.set_index('Timestamp') df_merged = df_merged.sort_index() # 2. Select where N2O has valid values (Drop NaNs), we also need the temperature column for flux calculation later on df_N2O = df_merged[['N2O_ppb', 'Ta_C']].dropna() # 3. Plot plot_time_series( df_N2O, y_column='N2O_ppb', title='N₂O Concentration', mode='markers' ) . What do we see? . The raw data is very noisy! There are several problems: . | Negative values: Physically impossible for gas concentrations | Extreme spikes: Values far outside the expected range | Sensor noise: Random fluctuations due to electrical interference | . These artifacts are common in field measurements and must be removed before we can calculate meaningful fluxes. 2.3 Filtering with a Quantile Filter . To remove outliers, we’ll use a quantile filter, it can help us see the real data patterns (signal) by removing all outliers/noise. This method calculates percentiles of the data and keeps only values within a specified range. Why Quantile Filtering? . Quantile filtering is robust to outliers. Unlike methods based on mean and standard deviation, extreme values have very little influence on percentile calculations. This makes it ideal for sensor data with occasional spikes. The approach: . | Calculate the 3th percentile ($P_{3}$) and 97th percentile ($P_{97}$) | Keep only data points where: $P_{3} \\leq x \\leq P_{97}$ | Discard everything else | . Info: What are Quantiles? . A quantile divides your data into equal-sized groups. Common examples: . | Median (50th percentile): Half the data is below, half is above | Quartiles: Divide data into 4 parts (25th, 50th, 75th percentiles) | Percentiles: Divide data into 100 parts | . The 10th percentile means “10% of values are below this point.” . Resources: . | Pandas quantile() documentation | Understanding Percentiles (Khan Academy) | . Applying the Filter . # Calculate the 3rd and 97th percentiles p_3 = df_N2O['N2O_ppb'].quantile(0.03) p_97 = df_N2O['N2O_ppb'].quantile(0.97) print(f\"3rd percentile: {p_3:.2f} ppb\") print(f\"97th percentile: {p_97:.2f} ppb\") print(f\"Keeping data in range [{p_3:.2f}, {p_97:.2f}]\") # Apply the filter df_filtered = df_N2O[ (df_N2O['N2O_ppb'] &gt;= p_3) &amp; (df_N2O['N2O_ppb'] &lt;= p_97) ].copy() # Calculate statistics based on the N2O dataframe, not the merged one n_raw = len(df_N2O) n_clean = len(df_filtered) n_removed = n_raw - n_clean print(f\"\\nValid rows before filtering: {n_raw:,}\") print(f\"Rows after filtering: {n_clean:,}\") print(f\"Outliers removed: {n_removed:,} ({(n_removed/n_raw)*100:.1f}%)\") . Now let’s visualize the filtered data: . # Plot filtered data using markers to see individual points plot_time_series( df_filtered, y_column='N2O_ppb', title='Filtered N₂O Concentration Over Time', mode='markers' ) . This looks much better! The noise is gone, now please pan and zoom in to check the N2O data measured on 15th and 26th of Aug. You can see a clear, meaningful pattern. 2.4 Understanding the Data Pattern . The filtered data reveals a repeating pattern characteristic of the static chamber method. Let’s break down what we’re seeing: . | Phase | What’s Happening | What You See in the Plot | . | 1. Baseline | Chamber is open, sensor measures ambient air | Long, flat periods at ~background concentration | . | 2. Accumulation | Chamber closed over soil, gases accumulate | Steady, linear increase in concentration | . | 3. Release | Chamber lifted, gases escape | Sharp vertical drop back to baseline | . | 4. Leveling off | (If chamber left too long) Soil-air gradient decreases | Rate of increase slows, curve flattens | . The Critical Insight: Linear Increase . The rate of concentration increase during the accumulation phase is what we use to calculate flux. Mathematically: . \\[\\text{Flux} \\propto \\frac{dC}{dt}\\] Where: . | $C$ = gas concentration inside the chamber | $t$ = time | $\\frac{dC}{dt}$ = rate of change (slope of the linear portion) | . Important: Use Only the Linear Portion . If a chamber is left on the ground too long, gas buildup inside the chamber reduces the concentration gradient between soil and chamber air. This causes the accumulation rate to slow down (“leveling off”). For accurate flux calculations, we must identify and use only the initial, linear part of each accumulation period. We’ll learn how to do this in the next section. The same visualization and filtering workflow applies to the GGA data (CH₄ and CO₂). But does the GGA data actually need filtering? Let’s find out! . Exercise: Inspect and Clean the GGA Data . Part 1: Visualize the raw data . First, plot the raw CH₄ and CO₂ data to see if they contain noise or outliers. # Plot raw CH4 data plot_time_series( df_merged[df_merged['CH4_ppm'].notna()], y_column='CH4_ppm', title='Raw CH₄ Concentration Over Time', mode='markers' ) # Plot raw CO2 data plot_time_series( df_merged[df_merged['CO2_ppm'].notna()], y_column='CO2_ppm', title='Raw CO₂ Concentration Over Time', mode='markers' ) . Questions to consider: . | Do you see any obvious outliers or impossible values (like negative concentrations)? | Are there extreme spikes that look like sensor errors? | Use the zoom and pan features to inspect different time periods. | . Part 2: Decide on filtering . Based on your visual inspection: . | If the data looks clean: No filtering needed! Move on to the next section. | If the data contains outliers: Apply a quantile filter like we did for N₂O. | . Part 3: Apply filtering (if needed) . If you determined that filtering is necessary, apply the quantile filter to CH₄ and CO₂. Hints: . | Use .quantile(0.03) and .quantile(0.97) to get the 10th and 90th percentiles | . Click here for the solution! # 1. Create clean copies for CH4 and CO2 (removing NaNs first simplifies everything) df_CH4 = df_merged[['CH4_ppm']].dropna().copy() df_CO2 = df_merged[['CO2_ppm']].dropna().copy() # --- Filter CH4 data (3rd - 97th percentile) --- ch4_p03 = df_CH4['CH4_ppm'].quantile(0.03) ch4_p97 = df_CH4['CH4_ppm'].quantile(0.97) print(f\"CH4 filter range: [{ch4_p03:.3f}, {ch4_p97:.3f}] ppm\") # Apply filter to the specific dataframe df_CH4_clean = df_CH4[ (df_CH4['CH4_ppm'] &gt;= ch4_p03) &amp; (df_CH4['CH4_ppm'] &lt;= ch4_p97) ].copy() # --- Filter CO2 data (3rd - 97th percentile) --- co2_p03 = df_CO2['CO2_ppm'].quantile(0.03) co2_p97 = df_CO2['CO2_ppm'].quantile(0.97) print(f\"CO2 filter range: [{co2_p03:.1f}, {co2_p97:.1f}] ppm\") # Apply filter to the specific dataframe df_CO2_clean = df_CO2[ (df_CO2['CO2_ppm'] &gt;= co2_p03) &amp; (df_CO2['CO2_ppm'] &lt;= co2_p97) ].copy() . plot_time_series( df_CH4_clean, y_column='CH4_ppm', title='Filtered CH₄ Concentration (3rd-97th Percentile)', mode='markers' # Markers are safer for high-frequency data ) # Visualize the cleaned CO2 data plot_time_series( df_CO2_clean, y_column='CO2_ppm', title='Filtered CO₂ Concentration (3rd-97th Percentile)', mode='markers' ) . Note: Depending on your dataset, you may find that the GGA data is already quite clean and filtering removes very few points. This is because the Los Gatos GGA analyzer tends to produce more stable measurements than some other sensors. Always visualize first before deciding to filter! . ",
    "url": "/New_BAI_DataAnalysis/python_4_flux_calculation.html#2-visualizing-and-cleaning-the-data",
    
    "relUrl": "/python_4_flux_calculation.html#2-visualizing-and-cleaning-the-data"
  },"60": {
    "doc": "4. FLUX CALCULATION",
    "title": "3. Calculating Flux for a Single Measurement",
    "content": "After loading and filtering our raw data and getting an overview of the patterns, it’s time to calculate the fluxes. Excited? . In this section, we will focus on a single measurement period to understand the process in detail. We’ll break it down into these key steps: . 3.1 The Flux Calculation Formula . Before we start coding, let’s understand the physics behind flux calculation. What is “Flux”? . In the context of greenhouse gas research, flux refers to the exchange of gases between different parts of the Earth system—in our case, between the soil and the atmosphere inside our measurement chamber. Learn More: Greenhouse Gas Fluxes - Sustainability Directory . You might ask: “Doesn’t the rate of concentration change (ΔC/t) already represent the flux?” . Not quite! The raw concentration change rate (in ppb/s) is evidence of a flux, but it’s not standardized. It only describes what’s happening inside our specific chamber under specific conditions. We cannot compare measurements directly because: . | Gas density varies with temperature and pressure — The same volume can contain different amounts of gas molecules | Chamber size matters — A larger chamber captures more gas, giving a higher concentration change rate | . To make measurements comparable, we need to convert our raw observation into a standardized unit: µmol m⁻² s⁻¹ (micromoles per square meter per second). The Formula . The flux calculation is a two-step process: . Step 1: Calculate moles of air in the chamber using the Ideal Gas Law: . \\[n = \\frac{P \\cdot V}{R \\cdot T}\\] Step 2: Calculate flux by combining the slope with moles of air: . \\[\\text{Flux} = \\frac{n \\cdot \\text{slope}}{A}\\] Where: . | Symbol | Description | Unit | . | $n$ | Moles of air in the chamber | mol | . | $P$ | Atmospheric pressure | atm | . | $V$ | Chamber headspace volume | L (liters) | . | $R$ | Ideal gas constant | 0.0821 L·atm·K⁻¹·mol⁻¹ | . | $T$ | Air temperature | K (Kelvin) | . | slope | Rate of concentration change | ppm s⁻¹ | . | $A$ | Surface area covered by chamber | m² | . Understanding Why This Works . The key insight is understanding what ppm means: . ppm = parts per million = µmol per mol of air . So when we multiply: . | slope (ppm/s) × n (moles of air) = µmol/s | . Then dividing by area gives us: . | µmol/s ÷ area (m²) = µmol m⁻² s⁻¹ | . This is our final flux unit—no additional conversion factor needed! . Info: ppb vs ppm - Which Unit to Use? . Gas concentrations can be expressed in different units: . | ppm (parts per million) = 1 molecule per 1,000,000 air molecules | ppb (parts per billion) = 1 molecule per 1,000,000,000 air molecules | 1 ppm = 1000 ppb | . Typical usage by gas type: . | CO₂: Usually measured in ppm (atmospheric ~420 ppm) | CH₄: Can be ppm or ppb (atmospheric ~1.9 ppm = 1900 ppb) | N₂O: Usually measured in ppb (atmospheric ~330 ppb) | . Always check your instrument output to know which unit your slope is in! . Info: The Ideal Gas Law . The equation $PV = nRT$ relates pressure, volume, and temperature to the number of moles of gas. Rearranging: $n = \\frac{PV}{RT}$ . Important: Be careful with units! If using: . | $R = 0.0821$ L·atm·K⁻¹·mol⁻¹ → use $V$ in liters, $P$ in atm | $R = 8.314$ J·K⁻¹·mol⁻¹ → use $V$ in m³, $P$ in Pa | . Resources: . | Ideal Gas Law (Khan Academy) | Gas Laws (Chemistry LibreTexts) | . Creating the Flux Calculation Function . Now let’s implement this formula as a Python function. Exercise . The function calculate_flux is provided below but is incomplete. Fill in the missing parts based on the formula. Hints: . | Handle both ppb/s and ppm/s slope units using a parameter | Use the Ideal Gas Law to calculate moles: $n = \\frac{PV}{RT}$ | Remember: ppm means µmol per mol, so slope_ppm × n already gives µmol! | . # Define the ideal gas constant (using L, atm, K, mol units) R = 0.0821 # L·atm·K⁻¹·mol⁻¹ def calculate_flux(slope, temp_k, pressure_atm, volume_L, area_m2, slope_unit='ppb'): \"\"\" Calculates gas flux from chamber measurements. Parameters: - slope (float): Rate of concentration change - temp_k (float): Temperature in Kelvin - pressure_atm (float): Pressure in atmospheres (typically 1 atm) - volume_L (float): Chamber volume in liters - area_m2 (float): Chamber area in square meters - slope_unit (str): Unit of slope - 'ppb' for ppb/s or 'ppm' for ppm/s Returns: - float: Flux in µmol m⁻² s⁻¹ \"\"\" # Step 1: Convert slope to ppm/s if needed if slope_unit == 'ppb': slope_ppm_s = ... elif slope_unit == 'ppm': slope_ppm_s = ... else: raise ValueError(...) # Step 2: Calculate moles of air using Ideal Gas Law (n = PV/RT) n_moles = ... # Step 3: Calculate flux in µmol m⁻² s⁻¹ # Note: ppm = µmol/mol, so (slope_ppm × n_moles) gives µmol/s flux = ... return flux . Click here for the solution! # Define the ideal gas constant (using L, atm, K, mol units) R = 0.0821 # L·atm·K⁻¹·mol⁻¹ def calculate_flux(slope, temp_k, pressure_atm, volume_L, area_m2, slope_unit='ppb'): \"\"\" Calculates gas flux from chamber measurements. Parameters: - slope (float): Rate of concentration change - temp_k (float): Temperature in Kelvin - pressure_atm (float): Pressure in atmospheres (typically 1 atm) - volume_L (float): Chamber volume in liters - area_m2 (float): Chamber area in square meters - slope_unit (str): Unit of slope - 'ppb' for ppb/s or 'ppm' for ppm/s Returns: - float: Flux in µmol m⁻² s⁻¹ \"\"\" # Step 1: Convert slope to ppm/s if needed if slope_unit == 'ppb': slope_ppm_s = slope / 1000.0 # Convert ppb to ppm elif slope_unit == 'ppm': slope_ppm_s = slope # Already in ppm else: raise ValueError(f\"slope_unit must be 'ppb' or 'ppm', got '{slope_unit}'\") # Step 2: Calculate moles of air using Ideal Gas Law (n = PV/RT) n_moles = (pressure_atm * volume_L) / (R * temp_k) # Step 3: Calculate flux in µmol m⁻² s⁻¹ # Note: ppm = µmol/mol, so (slope_ppm × n_moles) gives µmol/s # Dividing by area gives µmol/m²/s - NO additional conversion needed! flux = (n_moles * slope_ppm_s) / area_m2 return flux . Example calculations: . # Example 1: Using slope in ppb/s (typical for N2O) flux_n2o = calculate_flux( slope=0.05, # 0.05 ppb/s temp_k=298.15, pressure_atm=1.0, volume_L=12.6, area_m2=0.1257, slope_unit='ppb' # Specify unit ) print(f\"N2O Flux: {flux_n2o:.6f} µmol m⁻² s⁻¹\") # Example 2: Using slope in ppm/s (typical for CO2) flux_co2 = calculate_flux( slope=0.0842, # 0.0842 ppm/s temp_k=306.11, pressure_atm=1.0, volume_L=41.46, area_m2=0.123, slope_unit='ppm' # Specify unit ) print(f\"CO2 Flux: {flux_co2:.4f} µmol m⁻² s⁻¹\") # Expected: ~1.13 µmol m⁻² s⁻¹ . 3.2 Isolating and Visualizing the Measurement Data . Now let’s apply our formula to real data. We’ll use an example measurement period from our field campaign. Step 1: Define the Time Window . Let’s select a measurement window from our filtered data: . # Define the start and end times for our measurement window start_time = '2025-08-15 12:04:00' end_time = '2025-08-15 12:09:30' # Select the data for this specific time window measurement_data = df_filtered[ (df_filtered.index &gt;= start_time) &amp; (df_filtered.index &lt; end_time) ] print(f\"Selected {len(measurement_data)} data points\") print(f\"Time range: {measurement_data.index.min()} to {measurement_data.index.max()}\") . Step 2: Visualize the Raw Measurement Window . # Plot the measurement window plot_time_series( measurement_data, y_column='N2O_ppb', title='N₂O Concentration - Full Measurement Window', mode='markers' ) . Understanding the Pattern . Looking at the plot, we can identify three distinct phases: . | Phase | Description | What You See | . | 1. Pre-measurement Baseline | Sensor measuring ambient air before chamber placement | Flat period at the beginning | . | 2. Accumulation (Linear Increase) | Chamber sealed, N₂O accumulating from soil | Steady, linear rise ✓ | . | 3. Post-measurement Drop | Chamber lifted, sensor exposed to ambient air | Sharp, sudden drop | . ⚠️ Critical: Use Only the Linear Phase . Our flux calculation relies on the slope from linear regression. If we include the flat baseline or the sharp drop, the regression line will not represent the true accumulation rate, leading to inaccurate flux values. We must visually inspect the data and select only the linear increase phase. Step 3: Refine the Time Window . Use the zoom and pan features on the interactive plot to identify the linear portion. In this example, the clean linear increase occurs approximately between 12:05:30 and 12:09:00. Exercise . Slice the DataFrame to include only the linear accumulation phase, then plot it to verify your selection. Click here for the solution! # Define the refined time window (linear portion only) start_linear = '2025-08-15 12:05:30' end_linear = '2025-08-15 12:09:00' # Create a new DataFrame with only the linear phase # Use .copy() to avoid SettingWithCopyWarning regression_data = df_filtered[ (df_filtered.index &gt; start_linear) &amp; (df_filtered.index &lt; end_linear) ].copy() print(f\"Selected {len(regression_data)} points for regression\") # Visualize to confirm our selection plot_time_series( regression_data, y_column='N2O_ppb', title='Refined Regression Window (Linear Phase Only)', mode='markers' ) . Great! The plot should now show a clear, linear increase in N₂O concentration. This is exactly what we need for our regression. 3.3 Linear Regression to Derive the Rate of Change . Now we’ll fit a linear regression line to our data. The slope of this line is the $\\frac{\\Delta C}{\\Delta t}$ we need for our flux formula. Step 1: Convert Timestamps to Elapsed Seconds . For regression, we need numeric x-values. We’ll convert timestamps to “seconds elapsed since start of measurement”: . from scipy import stats # Work on a copy to avoid modifying the original regression_data = regression_data.copy() # Get the start time start_timestamp = regression_data.index.min() # Calculate elapsed seconds for each data point regression_data['elapsed_seconds'] = ( regression_data.index - start_timestamp ).total_seconds() # Check the result print(regression_data[['elapsed_seconds', 'N2O_ppb']].head()) . Step 2: Perform Linear Regression . We’ll use SciPy’s linregress function to fit a line: . # Perform linear regression slope, intercept, r_value, p_value, std_err = stats.linregress( x=regression_data['elapsed_seconds'], y=regression_data['N2O_ppb'] ) # Calculate R-squared (coefficient of determination) r_squared = r_value ** 2 print(\"--- Regression Results ---\") print(f\"Slope (ΔC/Δt): {slope:.4f} ppb/s\") print(f\"Intercept: {intercept:.2f} ppb\") print(f\"R-squared: {r_squared:.4f}\") print(f\"P-value: {p_value:.2e}\") . Info: Interpreting R-squared . The R-squared ($R^2$) value tells us how well the line fits the data: . | $R^2 = 1.0$: Perfect fit | $R^2 &gt; 0.9$: Excellent fit | $R^2 &gt; 0.7$: Good fit (acceptable for flux calculation) | $R^2 &lt; 0.7$: Poor fit (flux may not be reliable) | . If $R^2 &lt; 0.7$, the concentration change may not be significant enough to calculate a meaningful flux. Resources: . | SciPy linregress documentation | Linear Regression (Khan Academy) | . 3.4 Visualizing the Regression Fit . Before calculating the flux, let’s visualize the regression line to confirm it fits well: . import plotly.graph_objects as go fig = go.Figure() # Add the raw data points fig.add_trace(go.Scatter( x=regression_data['elapsed_seconds'], y=regression_data['N2O_ppb'], mode='markers', name='Measured Data', marker=dict(size=8) )) # Add the fitted regression line fig.add_trace(go.Scatter( x=regression_data['elapsed_seconds'], y=intercept + slope * regression_data['elapsed_seconds'], mode='lines', name=f'Fitted Line (R²={r_squared:.3f})', line=dict(color='red', width=2) )) fig.update_layout( title=f'Linear Regression: Slope = {slope:.4f} ppb/s', xaxis_title='Elapsed Time (seconds)', yaxis_title='N₂O Concentration (ppb)', template='plotly_white', legend=dict(x=0.02, y=0.98) ) fig.show() . If the red line closely follows the data points and $R^2 &gt; 0.7$, we can proceed with confidence! . 3.5 Final Flux Calculation . Now we have all the pieces. Let’s put them together! . Step 1: Get Average Temperature and Pressure . We need the mean temperature and pressure during the measurement. Unit conversion is critical! . # Get average temperature (convert °C to Kelvin) avg_temp_c = regression_data['Ta_C'].mean() avg_temp_k = avg_temp_c + 273.15 # For pressure, we assume standard atmospheric pressure # (If you have measured pressure data, use that instead) pressure_atm = 1.0 # atm print(f\"Average Temperature: {avg_temp_c:.2f} °C = {avg_temp_k:.2f} K\") print(f\"Pressure: {pressure_atm} atm\") . Step 2: Define Chamber Dimensions . The chamber volume and area are constants for our setup. Make sure volume is in liters! . # Chamber specifications (measure these for your specific equipment!) CHAMBER_VOLUME_L = 41.4567 # liters COLLAR_AREA_M2 = 0.123 # m² print(f\"Chamber Volume: {CHAMBER_VOLUME_L} L\") print(f\"Collar Area: {COLLAR_AREA_M2} m²\") . Step 3: Calculate Moles of Air . Using the Ideal Gas Law: . # Ideal gas constant (L·atm·K⁻¹·mol⁻¹) R = 0.0821 # Calculate moles of air in the chamber n_moles = (pressure_atm * CHAMBER_VOLUME_L) / (R * avg_temp_k) print(f\"Moles of air in chamber: {n_moles:.4f} mol\") . Step 4: Calculate the Flux . # Convert slope from ppb/s to ppm/s slope_ppm_s = slope / 1000.0 # Calculate flux! # Remember: ppm = µmol/mol, so (n × slope_ppm) gives µmol/s flux_n2o = (n_moles * slope_ppm_s) / COLLAR_AREA_M2 print(\"\\n\" + \"=\"*50) print(\" FINAL FLUX CALCULATION RESULT\") print(\"=\"*50) print(f\" Slope: {slope:.4f} ppb/s = {slope_ppm_s:.6f} ppm/s\") print(f\" Temperature: {avg_temp_k:.2f} K\") print(f\" Pressure: {pressure_atm} atm\") print(f\" Volume: {CHAMBER_VOLUME_L} L\") print(f\" Moles (n): {n_moles:.4f} mol\") print(f\" Area: {COLLAR_AREA_M2} m²\") print(\"-\"*50) print(f\" N₂O Flux: {flux_n2o:.5f} µmol m⁻² s⁻¹\") print(\"=\"*50) . Or use our function: . flux_n2o = calculate_flux( slope=slope, temp_k=avg_temp_k, pressure_atm=pressure_atm, volume_L=CHAMBER_VOLUME_L, area_m2=COLLAR_AREA_M2, slope_unit='ppb' # Our regression slope is in ppb/s ) print(f\"N₂O Flux: {flux_n2o:.5f} µmol m⁻² s⁻¹\") . Congratulations! You’ve successfully converted raw gas concentration data into a standardized flux value! . 3.6 Challenge: Making the Function More Robust . Challenge Exercise . Our calculate_flux function works, but it has a hidden weakness: it assumes the user provides inputs in the correct units. What if someone accidentally passes temperature in Celsius instead of Kelvin? The function would run without error but produce wildly incorrect results. Your Task: Upgrade the function to be more robust by: . | Detecting units based on plausible value ranges | Auto-converting common unit mistakes | Raising errors for implausible values | . Tips for detecting units: . | Variable | If value is in range… | It’s probably… | . | Temperature | -50 to 60 | Celsius | . | Temperature | 220 to 330 | Kelvin | . | Pressure | 0.8 to 1.2 | atm | . | Pressure | 800 to 1200 | hPa/mbar | . | Volume | 1 to 1000 | Liters | . | Volume | 0.001 to 1 | m³ | . | Area | 100 to 10,000 | cm² | . | Area | 0.01 to 1 | m² | . Click here for the solution! # Ideal gas constant (using L, atm, K, mol units) R = 0.0821 # L·atm·K⁻¹·mol⁻¹ def calculate_flux_robust(slope, temperature, pressure, volume, area, slope_unit='ppb'): \"\"\" Calculates gas flux with automatic unit detection and conversion. All inputs are auto-converted to standard units: - Temperature → Kelvin - Pressure → atm - Volume → Liters - Area → m² Parameters: - slope (float): Rate of concentration change - temperature (float): Temperature in Celsius or Kelvin (auto-detected) - pressure (float): Pressure in atm or hPa (auto-detected) - volume (float): Chamber volume in m³ or Liters (auto-detected) - area (float): Chamber area in m² or cm² (auto-detected) - slope_unit (str): Unit of slope - 'ppb' for ppb/s or 'ppm' for ppm/s Returns: - float: Flux in µmol m⁻² s⁻¹ \"\"\" # --- Slope Unit Check --- if slope_unit == 'ppb': slope_ppm_s = slope / 1000.0 elif slope_unit == 'ppm': slope_ppm_s = slope else: raise ValueError(f\"slope_unit must be 'ppb' or 'ppm', got '{slope_unit}'\") # --- Temperature Check --- if -50 &lt;= temperature &lt;= 60: print(f\" [Auto-convert] Temperature {temperature} detected as °C → converting to K\") temp_k = temperature + 273.15 elif 220 &lt;= temperature &lt;= 330: temp_k = temperature # Already in Kelvin else: raise ValueError( f\"Temperature ({temperature}) outside plausible range. \" f\"Expected: -50 to 60 (°C) or 220 to 330 (K)\" ) # --- Pressure Check --- if 0.8 &lt;= pressure &lt;= 1.2: pressure_atm = pressure # Already in atm elif 800 &lt;= pressure &lt;= 1200: print(f\" [Auto-convert] Pressure {pressure} detected as hPa → converting to atm\") pressure_atm = pressure / 1013.25 # Convert hPa to atm else: raise ValueError( f\"Pressure ({pressure}) outside plausible range. \" f\"Expected: 0.8 to 1.2 (atm) or 800 to 1200 (hPa)\" ) # --- Volume Check --- if 1 &lt;= volume &lt;= 1000: volume_L = volume # Already in Liters elif 0.001 &lt;= volume &lt;= 1: print(f\" [Auto-convert] Volume {volume} detected as m³ → converting to L\") volume_L = volume * 1000.0 # Convert m³ to L else: raise ValueError( f\"Volume ({volume}) outside plausible range. \" f\"Expected: 1 to 1000 (L) or 0.001 to 1 (m³)\" ) # --- Area Check --- if 100 &lt;= area &lt;= 10000: print(f\" [Auto-convert] Area {area} detected as cm² → converting to m²\") area_m2 = area / 10000.0 elif 0.01 &lt;= area &lt;= 1: area_m2 = area # Already in m² else: raise ValueError( f\"Area ({area}) outside plausible range. \" f\"Expected: 100 to 10000 (cm²) or 0.01 to 1 (m²)\" ) # --- Core Calculation --- # Calculate moles of air (n = PV/RT) n_moles = (pressure_atm * volume_L) / (R * temp_k) # Calculate flux # ppm = µmol/mol, so (n × slope_ppm) gives µmol/s # Dividing by area gives µmol/m²/s flux = (n_moles * slope_ppm_s) / area_m2 return flux . Example usage: . # Example 1: N2O with slope in ppb/s flux_n2o = calculate_flux_robust( slope=50, # 50 ppb/s temperature=25, # Celsius - will be converted to K pressure=1013, # hPa - will be converted to atm volume=12.6, # Liters - already correct area=1257, # cm² - will be converted to m² slope_unit='ppb' ) print(f\"N2O Flux: {flux_n2o:.5f} µmol m⁻² s⁻¹\") # Example 2: CO2 with slope in ppm/s flux_co2 = calculate_flux_robust( slope=0.0842, # 0.0842 ppm/s temperature=33, # Celsius pressure=1.0, # atm - already correct volume=41.46, # Liters area=0.123, # m² - already correct slope_unit='ppm' ) print(f\"CO2 Flux: {flux_co2:.4f} µmol m⁻² s⁻¹\") . Info: Python’s raise Keyword . The raise keyword is used to trigger an exception (error) when something goes wrong: . raise ValueError(\"Your error message here\") . Common exception types: . | ValueError: Input has wrong value (but correct type) | TypeError: Input has wrong type | RuntimeError: General runtime error | . Resources: Python raise keyword (GeeksforGeeks) . ",
    "url": "/New_BAI_DataAnalysis/python_4_flux_calculation.html#3-calculating-flux-for-a-single-measurement",
    
    "relUrl": "/python_4_flux_calculation.html#3-calculating-flux-for-a-single-measurement"
  },"61": {
    "doc": "4. FLUX CALCULATION",
    "title": "4. Automating Gas Flux Calculation",
    "content": "In Section 3, we calculated flux for a single measurement by hand. Now it’s time to scale up! With dozens of measurements across multiple plots, dates, and gas types, we need automation. 4.1 Structuring Measurement Metadata . The first and crucial step of automation is to store the key information (metadata) for each measurement in a structured way that a program can loop through. For this, we will use a Python dictionary that can be converted to a DataFrame. The dictionary keys will be our data “columns” (e.g., ‘plot_id’, ‘land_use’), and the values will be lists containing the data for each plot. The Challenge: Multiple Measurements per Plot . Now, there is an issue: we take multiple measurements at the same plot, perhaps on different days or at different times. How can we store this information efficiently? . Solution: We can store the multiple start and end times for a single plot as a single string, with each timestamp separated by a semicolon (;). Of course, the order of the multiple starttime and endtime for a plot should match. By doing this, we can keep the metadata table concise and still tell our program to perform multiple calculations for that plot: . | plot_id: The unique plot identifier | land_use: The land cover type | start_time: All measurement start times, separated by ; | end_time: All measurement end times, separated by ; | variable: The gas measured for each time window, separated by ; | . Important: The order must match! The 1st start_time goes with the 1st end_time and 1st variable, etc. # --- Create Compact Measurement Metadata --- # Each plot appears only once; multiple measurements are semicolon-separated measurement_info = { 'plot_id': ['1-1', '1-2', '1-3', '2-1', '2-2', '2-3'], 'land_use': ['forest', 'forest', 'forest', 'grassland', 'grassland', 'grassland'], 'start_time': [ '2025-08-06 11:41:35; 2025-08-06 11:41:35; 2025-08-15 11:08:25; 2025-08-15 11:08:25; 2025-08-15 11:09:36; 2025-08-26 11:12:39; 2025-08-26 11:12:39; 2025-08-26 11:14:16', '2025-08-06 11:52:00; 2025-08-06 11:52:00; 2025-08-15 11:20:30; 2025-08-15 11:20:30; 2025-08-15 11:21:41; 2025-08-26 11:18:35; 2025-08-26 11:18:35; 2025-08-26 11:20:10', '2025-08-26 11:24:51; 2025-08-26 11:24:51; 2025-08-26 11:26:35', '2025-08-06 12:11:13; 2025-08-06 12:11:13; 2025-08-15 12:11:38; 2025-08-15 12:11:38; 2025-08-15 12:12:49; 2025-08-26 11:49:07; 2025-08-26 11:50:40; 2025-08-26 11:50:40', '2025-08-06 12:21:00; 2025-08-06 12:21:00; 2025-08-15 12:03:45; 2025-08-15 12:03:45; 2025-08-15 12:04:56; 2025-08-26 12:04:23; 2025-08-26 12:04:23; 2025-08-26 12:05:31', '2025-08-06 12:27:47; 2025-08-06 12:27:47; 2025-08-15 11:53:30; 2025-08-15 11:53:30; 2025-08-15 11:54:41; 2025-08-26 12:25:48; 2025-08-26 12:25:48; 2025-08-26 12:27:23' ], 'end_time': [ '2025-08-06 11:45:35; 2025-08-06 11:45:35; 2025-08-15 11:12:25; 2025-08-15 11:12:25; 2025-08-15 11:13:36; 2025-08-26 11:16:39; 2025-08-26 11:16:39; 2025-08-26 11:18:16', '2025-08-06 11:56:00; 2025-08-06 11:56:00; 2025-08-15 11:24:30; 2025-08-15 11:24:30; 2025-08-15 11:25:41; 2025-08-26 11:22:35; 2025-08-26 11:22:35; 2025-08-26 11:24:10', '2025-08-26 11:28:51; 2025-08-26 11:28:51; 2025-08-26 11:30:35', '2025-08-06 12:17:00; 2025-08-06 12:17:00; 2025-08-15 12:15:38; 2025-08-15 12:15:38; 2025-08-15 12:16:49; 2025-08-26 11:53:07; 2025-08-26 11:54:40; 2025-08-26 11:54:40', '2025-08-06 12:25:46; 2025-08-06 12:25:46; 2025-08-15 12:07:45; 2025-08-15 12:07:45; 2025-08-15 12:08:56; 2025-08-26 12:08:23; 2025-08-26 12:08:23; 2025-08-26 12:09:31', '2025-08-06 12:31:50; 2025-08-06 12:31:50; 2025-08-15 11:57:59; 2025-08-15 11:57:59; 2025-08-15 11:59:10; 2025-08-26 12:29:48; 2025-08-26 12:29:48; 2025-08-26 12:31:23' ], 'variable': [ 'CH4; CO2; CH4; CO2; N2O; CH4; CO2; N2O', 'CH4; CO2; CH4; CO2; N2O; CH4; CO2; N2O', 'CH4; CO2; N2O', 'CH4; CO2; CH4; CO2; N2O; N2O; CH4; CO2', 'CH4; CO2; CH4; CO2; N2O; CH4; CO2; N2O', 'CH4; CO2; CH4; CO2; N2O; CH4; CO2; N2O' ] } # Convert to DataFrame metadata_df = pd.DataFrame(measurement_info) print(\"Measurement Metadata (Compact Format):\") print(f\" Total plots: {len(metadata_df)}\") print(f\" Forest plots: {(metadata_df['land_use'] == 'forest').sum()}\") print(f\" Grassland plots: {(metadata_df['land_use'] == 'grassland').sum()}\") # Count total measurements total_measurements = sum(len(row['variable'].split(';')) for _, row in metadata_df.iterrows()) print(f\" Total measurements: {total_measurements}\") metadata_df . Understanding the Structure . Let’s look at what’s stored for Plot 1-1: . # Example: View measurements for Plot 1-1 plot_data = metadata_df[metadata_df['plot_id'] == '1-1'].iloc[0] print(f\"Plot: {plot_data['plot_id']}\") print(f\"Land use: {plot_data['land_use']}\") print(f\"\\nMeasurements:\") start_times = plot_data['start_time'].split('; ') end_times = plot_data['end_time'].split('; ') variables = plot_data['variable'].split('; ') for i, (start, end, var) in enumerate(zip(start_times, end_times, variables), 1): print(f\" {i}. {var}: {start} to {end}\") . Output: . Plot: 1-1 Land use: forest Measurements: 1. CH4: 2025-08-06 11:41:35 to 2025-08-06 11:45:35 2. CO2: 2025-08-06 11:41:35 to 2025-08-06 11:45:35 3. CH4: 2025-08-15 11:08:25 to 2025-08-15 11:12:25 4. CO2: 2025-08-15 11:08:25 to 2025-08-15 11:12:25 5. N2O: 2025-08-15 11:09:36 to 2025-08-15 11:13:36 6. CH4: 2025-08-26 11:12:39 to 2025-08-26 11:16:39 7. CO2: 2025-08-26 11:12:39 to 2025-08-26 11:16:39 8. N2O: 2025-08-26 11:14:16 to 2025-08-26 11:18:16 . Note: CH₄ and CO₂ share the same measurement times because they come from the same GGA analyzer! . 4.2 Setting Up the Data and Constants . Before we start the automation loop, let’s prepare everything we need. Gas Configuration . Different gases have different concentration units and come from different DataFrames. We’ll create a configuration dictionary to handle this: . # --- Gas Configuration --- # Maps each gas variable to its DataFrame, column name, and slope unit GAS_CONFIG = { 'N2O': { 'dataframe': df_merged[['N2O_ppb', 'Ta_C']].dropna(), # Our N2O DataFrame 'column': 'N2O_ppb', # Column name in the DataFrame 'slope_unit': 'ppb', # Unit for flux calculation 'display_name': 'N₂O' }, 'CO2': { 'dataframe': df_merged[['CO2_ppm', 'Ta_C']].dropna(), # Our CO2 DataFrame 'column': 'CO2_ppm', 'slope_unit': 'ppm', 'display_name': 'CO₂' }, 'CH4': { 'dataframe': df_merged[['CH4pm', 'Ta_C']].dropna(), # Our CH4 DataFrame 'column': 'CH4_ppm', 'slope_unit': 'ppm', 'display_name': 'CH₄' } } print(\"Configured gases:\") for gas, config in GAS_CONFIG.items(): print(f\" {gas}: column='{config['column']}', unit={config['slope_unit']}\") . Note: CH₄ and CO₂ come from the same GGA analyzer and share measurement times! N₂O comes from a separate analyzer with different measurement times. Chamber Constants . # --- Chamber Constants --- # These should match your actual equipment! CHAMBER_VOLUME_L = 41.4567 # Chamber volume in liters COLLAR_AREA_M2 = 0.123 # Collar area in square meters PRESSURE_ATM = 1.0 # Atmospheric pressure (assume standard) # Quality control thresholds R_SQUARED_THRESHOLD = 0.70 # Minimum R² for a valid flux P_VALUE_THRESHOLD = 0.05 # Maximum p-value for significance print(\"Chamber Configuration:\") print(f\" Volume: {CHAMBER_VOLUME_L} L\") print(f\" Area: {COLLAR_AREA_M2} m²\") print(f\" Pressure: {PRESSURE_ATM} atm\") print(f\"\\nQuality Control Thresholds:\") print(f\" Minimum R²: {R_SQUARED_THRESHOLD}\") print(f\" Maximum p-value: {P_VALUE_THRESHOLD}\") . 4.3 Building the Automation Pipeline . Now we’ll build a nested loop that: . | Outer loop: Iterates through each plot | Inner loop: Iterates through each measurement for that plot | . The workflow for each measurement is: . | Parse the semicolon-separated times and variables | Extract the correct data based on gas type and time window | Visualize for manual inspection | Refine the time window (remove baseline and drop phases) | Regress to get the slope | Quality check the regression fit | Calculate the flux | Store the results | . Helper Function: Get Temperature for a Time Window . Since temperature data might be in a different DataFrame, we need a helper function: . def get_mean_temperature(start_time, end_time, temp_df, temp_column='Ta_C'): \"\"\" Get the mean temperature during a measurement window. Parameters: start_time: Start of measurement window end_time: End of measurement window temp_df: DataFrame containing temperature data temp_column: Name of temperature column Returns: float: Mean temperature in Celsius, or NaN if no data \"\"\" mask = (temp_df.index &gt;= start_time) &amp; (temp_df.index &lt;= end_time) temp_data = temp_df.loc[mask, temp_column] if len(temp_data) == 0: print(f\" ⚠ Warning: No temperature data found for this window\") return np.nan return temp_data.mean() . The Main Processing Loop . from scipy import stats import numpy as np # --- Results Storage --- results = [] # --- Counter for progress tracking --- measurement_counter = 0 total_measurements = sum(len(row['variable'].split(';')) for _, row in metadata_df.iterrows()) # --- Process Each Plot --- print(\"=\"*70) print(\"STARTING AUTOMATED FLUX CALCULATION\") print(\"=\"*70) for plot_idx, row in metadata_df.iterrows(): # --- Extract Plot Info --- plot_id = row['plot_id'] land_use = row['land_use'] # --- Parse Semicolon-Separated Values --- start_times = [s.strip() for s in row['start_time'].split(';')] end_times = [s.strip() for s in row['end_time'].split(';')] variables = [v.strip() for v in row['variable'].split(';')] print(f\"\\n{'='*70}\") print(f\"PLOT: {plot_id} ({land_use})\") print(f\"Measurements to process: {len(variables)}\") print(\"=\"*70) # --- Inner Loop: Process Each Measurement for This Plot --- for start_str, end_str, variable in zip(start_times, end_times, variables): measurement_counter += 1 # Parse timestamps start_time = pd.to_datetime(start_str) end_time = pd.to_datetime(end_str) measurement_date = start_time.strftime('%Y-%m-%d') print(f\"\\n[{measurement_counter}/{total_measurements}] {variable} | {start_time} to {end_time}\") print(\"-\" * 50) # --- Get Gas Configuration --- if variable not in GAS_CONFIG: print(f\" ✗ ERROR: Unknown variable '{variable}'. Skipping.\") continue config = GAS_CONFIG[variable] df_gas = config['dataframe'] column_name = config['column'] slope_unit = config['slope_unit'] display_name = config['display_name'] # --- Step 1: Extract Data for Time Window --- mask = (df_gas.index &gt;= start_time) &amp; (df_gas.index &lt;= end_time) measurement_data = df_gas.loc[mask].copy() if len(measurement_data) &lt; 10: print(f\" ⚠ WARNING: Only {len(measurement_data)} data points. Skipping.\") results.append({ 'plot_id': plot_id, 'land_use': land_use, 'variable': variable, 'measurement_date': measurement_date, 'start_time': start_time, 'end_time': end_time, 'slope': np.nan, 'slope_unit': slope_unit, 'r_squared': np.nan, 'p_value': np.nan, 'qc_pass': False, 'flux_umol_m2_s': np.nan, 'note': 'Insufficient data' }) continue print(f\" Data points in window: {len(measurement_data)}\") # --- Step 2: Visual Inspection --- plot_time_series( measurement_data, y_column=column_name, title=f'{display_name} - Plot {plot_id} - {measurement_date}', mode='markers' ) # --- Step 3: Refine Time Window (Interactive) --- print(f\"\\n Current window: {start_time.strftime('%H:%M:%S')} to {end_time.strftime('%H:%M:%S')}\") print(\" Inspect the plot and identify the LINEAR accumulation phase.\") print(\" Press Enter to keep current times, or enter new times (HH:MM:SS):\") start_input = input(f\" Start time [{start_time.strftime('%H:%M:%S')}]: \").strip() end_input = input(f\" End time [{end_time.strftime('%H:%M:%S')}]: \").strip() # Parse refined times (keep date, update time if provided) if start_input: refined_start = pd.to_datetime(f\"{measurement_date} {start_input}\") else: refined_start = start_time if end_input: refined_end = pd.to_datetime(f\"{measurement_date} {end_input}\") else: refined_end = end_time # Re-extract data with refined window mask = (df_gas.index &gt;= refined_start) &amp; (df_gas.index &lt;= refined_end) regression_data = df_gas.loc[mask].copy() if len(regression_data) &lt; 10: print(f\" ⚠ WARNING: Refined window has only {len(regression_data)} points. Skipping.\") results.append({ 'plot_id': plot_id, 'land_use': land_use, 'variable': variable, 'measurement_date': measurement_date, 'start_time': refined_start, 'end_time': refined_end, 'slope': np.nan, 'slope_unit': slope_unit, 'r_squared': np.nan, 'p_value': np.nan, 'qc_pass': False, 'flux_umol_m2_s': np.nan, 'note': 'Insufficient data after refinement' }) continue # --- Step 4: Linear Regression --- # Create elapsed seconds column regression_data['elapsed_seconds'] = ( regression_data.index - regression_data.index.min() ).total_seconds() # Perform regression slope, intercept, r_value, p_value, std_err = stats.linregress( x=regression_data['elapsed_seconds'], y=regression_data[column_name] ) r_squared = r_value ** 2 print(f\"\\n Regression Results:\") print(f\" Slope: {slope:.6f} {slope_unit}/s\") print(f\" R²: {r_squared:.4f}\") print(f\" p-value: {p_value:.2e}\") # --- Step 5: Visualize Regression Fit --- fig = go.Figure() fig.add_trace(go.Scatter( x=regression_data['elapsed_seconds'], y=regression_data[column_name], mode='markers', name='Data', marker=dict(size=6) )) fig.add_trace(go.Scatter( x=regression_data['elapsed_seconds'], y=intercept + slope * regression_data['elapsed_seconds'], mode='lines', name=f'Fit (R²={r_squared:.3f})', line=dict(color='red', width=2) )) fig.update_layout( title=f'{display_name} Regression - Plot {plot_id} (R²={r_squared:.3f})', xaxis_title='Elapsed Time (s)', yaxis_title=f'{display_name} Concentration ({slope_unit})', template='plotly_white' ) fig.show() # --- Step 6: Quality Control --- if r_squared &lt; R_SQUARED_THRESHOLD or p_value &gt; P_VALUE_THRESHOLD: print(f\" ✗ QC FAILED: R²={r_squared:.3f} &lt; {R_SQUARED_THRESHOLD} or p={p_value:.3f} &gt; {P_VALUE_THRESHOLD}\") qc_pass = False flux = 0.0 note = f'QC failed: R²={r_squared:.3f}, p={p_value:.3f}' else: print(f\" ✓ QC PASSED\") qc_pass = True note = '' # --- Step 7: Calculate Flux --- # Get mean temperature during measurement avg_temp_c = get_mean_temperature(refined_start, refined_end, df_Ta) if np.isnan(avg_temp_c): # Fallback: try to get from regression data or use default if 'Ta_C' in regression_data.columns: avg_temp_c = regression_data['Ta_C'].mean() else: avg_temp_c = 25.0 # Default assumption print(f\" ⚠ Warning: Using default temperature {avg_temp_c}°C\") avg_temp_k = avg_temp_c + 273.15 # Calculate flux using our function flux = calculate_flux( slope=slope, temp_k=avg_temp_k, pressure_atm=PRESSURE_ATM, volume_L=CHAMBER_VOLUME_L, area_m2=COLLAR_AREA_M2, slope_unit=slope_unit ) print(f\"\\n Flux Calculation:\") print(f\" Temperature: {avg_temp_c:.1f}°C = {avg_temp_k:.1f} K\") print(f\" {display_name} Flux: {flux:.6f} µmol m⁻² s⁻¹\") # --- Store Results --- results.append({ 'plot_id': plot_id, 'land_use': land_use, 'variable': variable, 'measurement_date': measurement_date, 'start_time': refined_start, 'end_time': refined_end, 'slope': slope, 'slope_unit': slope_unit, 'r_squared': r_squared, 'p_value': p_value, 'qc_pass': qc_pass, 'flux_umol_m2_s': flux, 'note': note }) # --- Convert Results to DataFrame --- flux_results_df = pd.DataFrame(results) print(\"\\n\" + \"=\"*70) print(\"FLUX CALCULATION COMPLETE\") print(\"=\"*70) print(f\"\\nTotal measurements processed: {len(flux_results_df)}\") print(f\"QC passed: {flux_results_df['qc_pass'].sum()}\") print(f\"QC failed: {(~flux_results_df['qc_pass']).sum()}\") flux_results_df . Info: Using zip() for Parallel Iteration . The zip() function lets us iterate through multiple lists simultaneously: . for start, end, var in zip(start_times, end_times, variables): # start, end, and var are from the same index position . This ensures the 1st start_time pairs with the 1st end_time and 1st variable. Resources: Python zip() documentation . 4.4 Reviewing the Results . Let’s examine our calculated fluxes: . # --- Summary Statistics by Variable --- print(\"=\"*60) print(\"FLUX SUMMARY BY GAS TYPE\") print(\"=\"*60) for variable in flux_results_df['variable'].unique(): df_var = flux_results_df[ (flux_results_df['variable'] == variable) &amp; (flux_results_df['qc_pass'] == True) ] if len(df_var) &gt; 0: print(f\"\\n{variable}:\") print(f\" Valid measurements: {len(df_var)}\") print(f\" Mean flux: {df_var['flux_umol_m2_s'].mean():.6f} µmol m⁻² s⁻¹\") print(f\" Std dev: {df_var['flux_umol_m2_s'].std():.6f} µmol m⁻² s⁻¹\") print(f\" Min: {df_var['flux_umol_m2_s'].min():.6f} µmol m⁻² s⁻¹\") print(f\" Max: {df_var['flux_umol_m2_s'].max():.6f} µmol m⁻² s⁻¹\") . # --- View All Results --- # Show only key columns for clarity display_cols = ['plot_id', 'land_use', 'variable', 'measurement_date', 'r_squared', 'qc_pass', 'flux_umol_m2_s'] flux_results_df[display_cols].sort_values(['variable', 'measurement_date', 'plot_id']) . 4.5 Comparing Fluxes Across Land Use Types . Now for the exciting part—let’s see if there are differences in gas fluxes between forest and grassland! . Visualization with Box Plots . import seaborn as sns import matplotlib.pyplot as plt # Filter to only QC-passed measurements df_valid = flux_results_df[flux_results_df['qc_pass'] == True].copy() # Create a figure with subplots for each gas variables = df_valid['variable'].unique() n_vars = len(variables) fig, axes = plt.subplots(1, n_vars, figsize=(5*n_vars, 6)) # Handle case of single variable if n_vars == 1: axes = [axes] for ax, variable in zip(axes, variables): df_var = df_valid[df_valid['variable'] == variable] # Box plot sns.boxplot( data=df_var, x='land_use', y='flux_umol_m2_s', palette='viridis', ax=ax ) # Overlay individual points sns.stripplot( data=df_var, x='land_use', y='flux_umol_m2_s', color='black', size=8, alpha=0.7, ax=ax ) # Get display name display_name = GAS_CONFIG.get(variable, {}).get('display_name', variable) ax.set_title(f'{display_name} Flux by Land Use', fontsize=14) ax.set_xlabel('Land Use', fontsize=12) ax.set_ylabel('Flux (µmol m⁻² s⁻¹)', fontsize=12) ax.grid(axis='y', linestyle='--', alpha=0.7) plt.tight_layout() plt.show() . Statistical Comparison (Optional) . For a more rigorous comparison, we can perform statistical tests: . from scipy.stats import mannwhitneyu, ttest_ind print(\"=\"*60) print(\"STATISTICAL COMPARISON: Forest vs Grassland\") print(\"=\"*60) for variable in df_valid['variable'].unique(): df_var = df_valid[df_valid['variable'] == variable] forest_flux = df_var[df_var['land_use'] == 'forest']['flux_umol_m2_s'] grassland_flux = df_var[df_var['land_use'] == 'grassland']['flux_umol_m2_s'] display_name = GAS_CONFIG.get(variable, {}).get('display_name', variable) print(f\"\\n{display_name}:\") print(f\" Forest (n={len(forest_flux)}): mean = {forest_flux.mean():.6f} µmol m⁻² s⁻¹\") print(f\" Grassland (n={len(grassland_flux)}): mean = {grassland_flux.mean():.6f} µmol m⁻² s⁻¹\") # Only perform test if we have data in both groups if len(forest_flux) &gt;= 3 and len(grassland_flux) &gt;= 3: # Mann-Whitney U test (non-parametric, good for small samples) stat, p_value = mannwhitneyu(forest_flux, grassland_flux, alternative='two-sided') print(f\" Mann-Whitney U test: p = {p_value:.4f}\") if p_value &lt; 0.05: print(f\" → Significant difference detected (p &lt; 0.05)\") else: print(f\" → No significant difference (p ≥ 0.05)\") else: print(f\" → Insufficient data for statistical test\") . 4.6 Exporting Results . Finally, let’s save our results for future use or reporting: . # --- Export to CSV --- output_path = 'flux_results.csv' flux_results_df.to_csv(output_path, index=False) print(f\"Results exported to: {output_path}\") # --- Export Summary Statistics --- summary_stats = df_valid.groupby(['variable', 'land_use']).agg({ 'flux_umol_m2_s': ['count', 'mean', 'std', 'min', 'max'] }).round(6) summary_stats.columns = ['n', 'mean', 'std', 'min', 'max'] summary_stats.to_csv('flux_summary.csv') print(\"Summary statistics exported to: flux_summary.csv\") summary_stats . ",
    "url": "/New_BAI_DataAnalysis/python_4_flux_calculation.html#4-automating-gas-flux-calculation",
    
    "relUrl": "/python_4_flux_calculation.html#4-automating-gas-flux-calculation"
  },"62": {
    "doc": "4. FLUX CALCULATION",
    "title": "4. FLUX CALCULATION",
    "content": " ",
    "url": "/New_BAI_DataAnalysis/python_4_flux_calculation.html",
    
    "relUrl": "/python_4_flux_calculation.html"
  }
}
