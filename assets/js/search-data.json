{"0": {
    "doc": "Home",
    "title": "Why You Should Learn Python?",
    "content": "Generally speaking Python is a general-purpose, high-level interpreted programming language. What does that mean? . It means that it can be used for almost any kind of application you can achieve with any programming language. The “high-level” refers so to speak to the “distance” from the hardware in the way you use the language. The most extreme other side of the spectrum would be Assembly, where you directly control the processor and memory of the PC. Python lets you speak in simple terms and the computer understands what you want. In contrast to many other languages such as Java or C you don’t even have to compile your code. Compiling is a process in which your written code gets translated into something the machine can understand. In Java for example your workflow is always write code -&gt; compile program -&gt; run program. Python is interpreted which means that whenever you execute your code, it gets internally compiled and directly executed. That takes some responsibility of our shoulders. Not to forget: Python is one of the most widely used programming languages of all. The below graphic is from the 2023 Stackoverflow software developer survey. Pythons ranking is especially impressive as the other top languages are HTML, CSS and Javascript which power the majority of the modern internet while not being very widely used in other contexts. The data science platform Kaggle conducted a similar study in 2022 but specifically for data science and machine learning engineers . In the plot below you can clearly see that Python is the most widely used technology, way before R (R even decreasing in use) and SQL. So with all this said, get ready to join us in Python heaven! (credits: https://xkcd.com/353/) . ",
    "url": "/New_BAI_DataAnalysis/#why-you-should-learn-python",
    
    "relUrl": "/#why-you-should-learn-python"
  },"1": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "/New_BAI_DataAnalysis/",
    
    "relUrl": "/"
  },"2": {
    "doc": "0. INSTALLATION",
    "title": "Installing Python",
    "content": "For this course we will use a very handy tool called Anaconda. It is basically Python in a box, meaning that it creates a closed environment on your PC which already has Python and a lot of extra packages as well as additional software such as a code editor installed on it. This makes the installation as easy as it gets. The downside is that the installer is quite large (&gt;800mb). To get started, just go to Link: the Anaconda website… and download the version for your operating system. Simply follow the download instructions and leave all the buttons as they are by default. Once the installation is finished, search for the program “Anaconda Navigator” and open it. Once it opens you are presented with the main window of your Anaconda environment . In the “Home” screen you see a bunch of different programs that can run within Anaconda. The one we will use most is “Spyder” (definitely usable by people with Arachnophobia!). Make sure that Spyder is installed, if it is not click on the button to install it. Spyder is a code editor for Python which has some handy extensions, such as line-by-line execution and nicely viewable variables and tables during execution of code. But we will come back to that later… . Additionally there is a window called “Environnments”. Click on it and you will be presented with a table of two columns. The left side shows you environments (red box). Anaconda lets you create multiple environments (again, basically separete Python installations which are secluded from each other). This can be useful if e.g. you need functionality of very specific versions of packages for some program, but the same package in a different version in another program. To avoid the different versions clashing you can put them in different environments. On the right side (blue box) you can see the packages which are installed in an environment. Python has some internal core functionalities, but there are many many (many many many…) additional packages created by the community which unlock Pythons full potential. Some of the most widely used pacakges are e.g. numpy, pandas or scikit-learn. Notice: Try searching for them in the environment using the search-box on the top right and see whether they are already installed! . If a package is missing and you want to install it, select the dropdown menu on the top that by default says “installed” and swithc to “Not installed”. Then use the search box again to find the package you want to install. Notice: Try installing the package “xgboost” in this way. Once you have finished all of this you should be good to go! To verify that everything works go to the “Home” tab and start Spyder. Time for your first line of code! In the open Editor you will have to save the file in order to run it. Then copy and paste the following code by clicking the button on the top right in the box below. Insert it into spyder. Crypticlist = ['⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀', ' ⣀⣤⣤⠤⠐⠂⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡌⡦⠊⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡀⣼⡊⢀⠔⠀⠀⣄⠤⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣤⣤⣄⣀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣶⠃⠉⠡⡠⠤⠊⠀⠠⣀⣀⡠⠔⠒⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⣾⣿⢟⠿⠛⠛⠁', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣼⡇⠀⠀⠀⠀⠑⠶⠖⠊⠁⠀⠀⠀⡀⠀⠀⠀⢀⣠⣤⣤⡀⠀⠀⠀⠀⠀⢀⣠⣤⣶⣿⣿⠟⡱⠁⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢰⣾⣿⡇⠀⢀⡠⠀⠀⠀⠈⠑⢦⣄⣀⣀⣽⣦⣤⣾⣿⠿⠿⠿⣿⡆⠀⠀⢀⠺⣿⣿⣿⣿⡿⠁⡰⠁⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣾⣿⣿⣧⣠⠊⣠⣶⣾⣿⣿⣶⣶⣿⣿⠿⠛⢿⣿⣫⢕⡠⢥⣈⠀⠙⠀⠰⣷⣿⣿⣿⡿⠋⢀⠜⠁⠀⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠠⢿⣿⣿⣿⣿⣰⣿⣿⠿⣛⡛⢛⣿⣿⣟⢅⠀⠀⢿⣿⠕⢺⣿⡇⠩⠓⠂⢀⠛⠛⠋⢁⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠀', '⠘⢶⡶⢶⣶⣦⣤⣤⣤⣤⣤⣀⣀⣀⣀⡀⠀⠘⣿⣿⣿⠟⠁⡡⣒⣬⢭⢠⠝⢿⡡⠂⠀⠈⠻⣯⣖⣒⣺⡭⠂⢀⠈⣶⣶⣾⠟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀', '⠀⠀⠙⠳⣌⡛⢿⣿⣿⣿⣿⣿⣿⣿⣿⣻⣵⣨⣿⣿⡏⢀⠪⠎⠙⠿⣋⠴⡃⢸⣷⣤⣶⡾⠋⠈⠻⣶⣶⣶⣷⣶⣷⣿⣟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠈⠛⢦⣌⡙⠛⠿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡀⠀⠀⠩⠭⡭⠴⠊⢀⠀⠀⠀⠀⠀⠀⠀⠀⠈⣿⣿⣿⣿⣿⡇⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠈⠙⠓⠦⣄⡉⠛⠛⠻⢿⣿⣿⣿⣷⡀⠀⠀⠀⠀⢀⣰⠋⠀⠀⠀⠀⠀⣀⣰⠤⣳⣿⣿⣿⣿⣟⠑⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠓⠒⠒⠶⢺⣿⣿⣿⣿⣦⣄⣀⣴⣿⣯⣤⣔⠒⠚⣒⣉⣉⣴⣾⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠛⠹⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣭⣉⣉⣤⣿⣿⣿⣿⣿⣿⡿⢀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⠟⡁⡆⠙⢶⣀⠀⢀⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⣴⣶⣾⣿⣟⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠿⢛⣩⣴⣿⠇⡇⠸⡆⠙⢷⣄⠻⣿⣦⡄⠀⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣼⣿⣿⣿⣿⣿⣿⣿⣎⢻⣿⣿⣿⣿⣿⣿⣿⣭⣭⣭⣵⣶⣾⣿⣿⣿⠟⢰⢣⠀⠈⠀⠀⠙⢷⡎⠙⣿⣦⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣼⣿⣿⣿⣿⣿⣿⣿⣿⡟⣿⡆⢻⣿⣿⣿⣿⣿⣿⣿⣿⣿⠿⠿⠟⠛⠋⠁⢀⠇⢸⡇⠀⠀⠀⠀⠈⠁⠀⢸⣿⡆⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡜⡿⡘⣿⣿⣿⣿⣿⣶⣶⣤⣤⣤⣤⣤⣤⣤⣴⡎⠖⢹⡇⠀⠀⠀⠀⠀⠀⠀⠀⣿⣷⡄⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣦⡀⠘⢿⣿⣿⣿⣿⣿⣿⣿⣿⠿⠿⠛⠋⡟⠀⠀⣸⣷⣀⣤⣀⣀⣀⣤⣤⣾⣿⣿⣿⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⣸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣭⣓⡲⠬⢭⣙⡛⠿⣿⣿⣶⣦⣀⠀⡜⠀⠀⣰⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⢀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣭⣛⣓⠶⠦⠥⣀⠙⠋⠉⠉⠻⣄⣀⣸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⣼⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣶⣆⠐⣦⣠⣷⠊⠁⠀⠀⡭⠙⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡆⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⢠⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⢉⣛⡛⢻⡗⠂⠀⢀⣷⣄⠈⢆⠉⠙⠻⢿⣿⣿⣿⣿⣿⠇⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠘⣿⣿⡟⢻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡟⣉⢁⣴⣿⣿⣿⣾⡇⢀⣀⣼⡿⣿⣷⡌⢻⣦⡀⠀⠈⠙⠛⠿⠏⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠙⢿⣿⡄⠙⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠿⠛⠛⠛⢯⡉⠉⠉⠉⠉⠛⢼⣿⠿⠿⠦⡙⣿⡆⢹⣷⣤⡀⠀⠀⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠿⠄⠈⠻⠿⠿⠿⠿⠿⠿⠛⠛⠿⠛⠉⠁⠀⠀⠀⠀⠀⠀⠻⠿⠿⠿⠿⠟⠉⠀⠀⠤⠴⠶⠌⠿⠘⠿⠿⠿⠿⠶⠤⠀'] for line in Crypticlist: print(line) . Now its time to run your first script! You can run all of your script by clicking the green arrow on top (red circle) or you can run the code line-by-line with the green arrow with the line-symbol next to it (white circle). Once you ran the code you will see the variables created in the list on the top right (see picture below) and you will see the output of your code in the bottom right in the console (see picture below). May the code be with you! . ",
    "url": "/New_BAI_DataAnalysis/python_0_installation.html#installing-python",
    
    "relUrl": "/python_0_installation.html#installing-python"
  },"3": {
    "doc": "0. INSTALLATION",
    "title": "0. INSTALLATION",
    "content": " ",
    "url": "/New_BAI_DataAnalysis/python_0_installation.html",
    
    "relUrl": "/python_0_installation.html"
  },"4": {
    "doc": "1. BASICS OF PYTHON",
    "title": "The Basics of Python",
    "content": "This interactive tutorial will get you started on your data-exploration road and make you familiar with some core concepts of Python programming and data analysis. Notice: In all following sections I will insert some code snippets. You are very much encouraged to copy and paste them with the button on the top right and run them in your IDE (e.g. Spyder). Table of Contents . | General stuff about Python | Data Types and Variables | Operators | Loops and Conditionals | Functions and Classes | . Notice that it is not at all expected that you learn all these things and they are burnt into your brain (!!!!!). It is more of a broad intrdocution to all the basics so you have herd of them, but programmers do look up stuff all the time! So don’t worry if it is a lot of input right now, just try to understand the concepts and you can always come back and find help in here, in the internet or from me directly. Here are some useful ressources to look things up: . Link: w3schools.com: Tutorials on many topics where you can quickly look up things… Link: geeks4geeks.com: Another nice overview of many functionalities of Python (requires login)… Geeksforgeeks requires you to make an account or use e.g. a google login, but it features many tutorials, project ideas, quizzes and so on on many programming languages and general topics such as Machine Learning, Data Visualization, Data Science, Web Development and many more Link: Pandas cheat sheet: Later on we will use the library “Pandas” (so cute!) for data handling. A nice cheat sheet is provided by the developers… . ",
    "url": "/New_BAI_DataAnalysis/python_1_basics.html#the-basics-of-python",
    
    "relUrl": "/python_1_basics.html#the-basics-of-python"
  },"5": {
    "doc": "1. BASICS OF PYTHON",
    "title": "1. General things about Python",
    "content": "I quickly want to highlight some things which are special about Python compared to many other programming languages and that one needs to get used to. Python is indentation sensitive . In Python it matters, how far you indent your lines, meaning how much space you have at the beginning of a line. As an example this will work: . a = 5 b = 1 . but this will throw an error: . a = 5 b = 5 . will result in an error: . File \"&lt;stdin&gt;\", line 1 b = 5 IndentationError: unexpected indent . Variables . Generally in Python variables are created by assigning a value to them with an equal sign, just like we did above. Theire output can be shown by just typing the variable: . a = 5 a 5 . Comments . Comments are lines in the code that are not executed and are there for documentation. For now it is a good idea to use comments in your code to keep track of what is happening where. Single line comments are always created with an ‘#’. Everything after that symbol in the line is not executed. Multi-line comments can be written by enclosing them in three ‘: . # first I create a single line comment, this is not executed a = 5 # this line is executed, but the comment gets ignored ''' Now I write a multi-line comment I can continue the comment on the next line b = 5 &lt;-- this is ignored ''' . Python is 0-indexed . In Python, the first index in for example a list always has the number 0! This takes some time to get used to, especially if you come from e.g. R which has 1-based indexing, but most programming lanuge handle indexing like that and it is worth getting used to it. I won’t go into why it is handled like that but there are many discussions on the internet about it, feel free to dive in if you feel like diving into a rabbit-hole ;) . Separators . In Python separators for decimal numbers are ALWAYS dots! Commas are used e.g. to separete variables from each other or entries in a list . correct_decimal = 2.5 correct_decimal 2.5 . Naming of variables, functions, and anything at all . This is not Python-specific but a very important note! Always use descriptive names for variables, functions or anything that you give a name! Especially in scientific programming you see it time and time again that people name variables and functions using abbreviations that just came to their mind. This makes code much, much harder to read and to use by other people or your own future self. It happens so often that people look back at what they wrote 3 weeks ago and do not understand half of it because they did not give descriptive names. You can also use comments to document your code a bit, but that always takes up extra space, often does not look good because you barely keep the same formatting throughout the code and gives the next user more work to do when trying to understand the code. Just making the code explain itself is the best solution of all. Here is a very simple example: . # Bad code with abbreviations # it requires the user to interprete the variables and look at # used functions to understand what this even does l = [1,5,12,17,18,14,11] n = len(l) s = sum(l) m = s/n # Fixing it with comments # With comments we require the user to read all the extra text to # 1) understand what the data is # 2) understand what is calculated l = [1,5,12,17,18,14,11] # a list of temperature values n = len(l) # get total number of samples s = sum(l) # get total sum of samples m = s/n # calculate the mean value # This gets so much easier to read when using declarative naming. # You dont even have to look into the function to understand what it is doing: monthly_temperature = [1,5,12,17,18,14,11] number_of_samples = len(monthly_temperature) sum_of_samples = sum(monthly_temperature) mean_monthly_temperature = sum_of_samples/number_of_samples # This does not mean that naming has to replace comments completely # (although some people argue like that). It is still alright to use comments # to clarify parts of your code, just try keep it to a minimum and make the # code as self-explanatory as possible! . ",
    "url": "/New_BAI_DataAnalysis/python_1_basics.html#1-general-things-about-python",
    
    "relUrl": "/python_1_basics.html#1-general-things-about-python"
  },"6": {
    "doc": "1. BASICS OF PYTHON",
    "title": "2. Data Types and Variables",
    "content": "Python knows different types of data. A number is a different kind of variable than a word. That helps organizing the variables and defines, which operations are possible with which data. For example, computing the mean of a word would be difficult, just as translating a number to all-uppercase letters… . In Python you dont have to define the data type yourself because Python is smart and finds the type of data on its own. For example when we define a number Python will understand and give it the type “int” or “float”, which means “integer” or “floating point number” (decimal) We will not cover all data types as we probably won’t need all of them for our purpose. However these ones are important: . Primitive Datatypes Primitive datatypes are simple constructs that consists basically of one chunk of information, e.g. a number or a word: . | int: Integer, a number without a floating point | float: Floating point number, a number with decimals | str: A string of characters, e.g. letters, words and sentences | bool: Boolean, a value that can only be True or False. This helps us make decisions in our code | . Non-Primitive Datatypes Non-primitive datatypes consist of aggregations of primitive datatypes. A list for example holds several numbers or words or something else . | list: An ordered sequence of data, for example [1,2,3] is a list where each of the entries have a specific position and the entries can be accessed by indices | dict: A non-ordered mapping that consists of keys and values. That simply means, we can not get entries from the dictionary by indices (e.g. the 0th entry in a dictionary) but instead grab data from the dictionary by using the key. Imagine it like a digital telephone-book. The comparison does not hold completely because in theory a telephone book is ordered, but you would never search the 5001231th entry in a telephone book. Instead you would search the phone number of Mr. Smith”, so you go to the “key” Mr. Smith and get the “value” 0251/1234567. | . Lets look at some examples for data types: . # Primitive datatypes: letter_a = \"a\" # &lt;-- a string name = \"Josefine\" # &lt;-- a longer string age = 24 # &lt;-- an integer total_playtime = 354.5 # &lt;-- a float is_injured = False # &lt;-- a boolean # Non-Primitive datatypes: # list: # a list is alwasys enclosed by brackets # and the items are separeted with commas: scores_last_games = [5,3,0,1] # To access the values we can use the index, for example scores_last_games[0] # &lt;-- gets the first entry scores_last_games[2] # &lt;-- gets the 3rd entry scores_last_games[-1] # &lt;-- gets the last entry scores_last_games[-2] # &lt;-- gets the second last entry # dictionary: # is always enclosed by {}, # and has the structure \"key\":value, lines are separeted by a comma. josefine = { \"age\":age, \"total_playtime\":total_playtime, \"is_injured\":is_injured, \"scores_last_games\":scores_last_games } # Now the values of the dictionary can be accessed using the key like this: josefine[\"age\"] 24 # new entries can be added by assigning a value to a new key: josefine[\"trikot_number\"] = 9 . You can always find the type of a variable by using the type() function (more on functions later): . type(name) type(age) type(total_playtime) type(is_injured) type(scores_last_games) type(josefine) . It is possible to change the type of a variable, but only if Python is able to understand what the outcome should be. The functions to do that have the same name as the target data type, for example int() or str(): . int(\"10\") # &lt;-- this works str(500) # &lt;-- this works float(500) #&lt;-- this works float(\"500.5\") #&lt;-- this works float(\"abc\") #&lt;-- this won't work, how should you translate a word to a number? . One last thing is important to note. When you assign a non-primitive variable to another non-primitive variable, the two variables share the same data. That means, when you manipulate one you also manipulate the other. This can lead to confusion when you don’t keep it in mind. list_1 = [1,2,3] # a list is non-primitive list_2 = list_1 # here we assign the non-primitive list_1 to the variable list_2 list_2.append(4) # we add a fourth value, 4, to list_2 list_2 # list_2 is now [1,2,3,4] list_1 # BUT! list_1 is now also [1,2,3,4] # We can avoid this and extract the values from list_1 to create a completely new variable by using the .copy() function list_1 = [1,2,3] list_2 = list_1.copy() # we copy the values of list_1 to the new variable list_2 list_2.append(4) # we add a fourth value, 4, to the list list_2 list_2 # list_2 is now [1,2,3,4] list_1 # list_1 is still [1,2,3] . On the other hand when you assign a variable containing a primitive datatype to another variable, the value gets simply copied to the new variable. Here is an example: . a = 5 b = a # we assign the value of a to the variable b b = b + 1 # we increase the value of b by one b # b is now 6 a # a is still 5 . ",
    "url": "/New_BAI_DataAnalysis/python_1_basics.html#2-data-types-and-variables",
    
    "relUrl": "/python_1_basics.html#2-data-types-and-variables"
  },"7": {
    "doc": "1. BASICS OF PYTHON",
    "title": "3. Operators",
    "content": "An operator is something that allows you to interact with variables. Some very examples are mathematical operations or comparisons. 3.1 Arithmetic operations . Most operations are very intuitive. For example you can add numbers and also add words to concatenate them, but you can not subtract words from each other… Here is a list of operations: . a = 5 b = 10 word1 = \"Hi\" word2 = \"there\" # Airthmetic Operators: c = a + b # adding numbers concatenated_words = word1 + \" \" + word2 # adding words d = b - c # subtracting numbers e = a * b # multiplying numbers f = b / 5 # dividing numbers g = a ** 2 # Exponentation, this is a² h = 12 % 5 # Modulus, this returns the remaining amount after fitting one number into the other as many times as possible. Exercise . With what you know so far, grab the scores josefine scored in the last games and compute the average amount of goals per game she scores . Solution! scores = josefine[\"scores_last_games\"] total_scores = scores[0] + scores[1] + scores[2] + scores[3] mean_scores = total_scores / 4 . There are much better solutions to this, for example the iteration over all scores can be done with the built-in function sum() and the total number of score-values can be found using the len() function. A one-line solution could look like this: . mean_scores = sum(josefine[\"scores_last_games\"]) / len(josefine[\"scores_last_games\"]) . 3.2 Comparison operations . Comparison operations are used to compare values with each other in order to make decisions in your script. The output of a comparison is always a boolean value that is “True” if the comparison is evaluated as correct and “False” otherwise. goals_team1 = 5 goals_team2 = 2 goals_team1 &gt; goals_team2 # &gt; larger than goals_team1 &gt;= goals_team2 # &gt;= larger than or equal goals_team1 &lt; goals_team2 # &lt; smaller than goals_team1 &lt;= goals_team2 # &lt;= smaller than or equal goals_team1 == goals_team2 # == equal goals_team1 != goals_team2 # != not equal # you can also store the result in a variable: is_team1_winner = goals_team1 &gt; goals_team2 is_team2_winner = goals_team1 &lt; goals_team2 . 3.3 Logical operators . Logical operators can combine multiple comparisons. Namely there are three: and, or and not. The use of these is pretty intuitive. If we combine two comparisons with an “and”, the result is only True if all conditions hold. If we combine two comparisons with an “or”, the result is True if one of the conditions hold, even if the other is False. Not is a special case, that reverts the result. # Lets use a new example peter = { \"age\":24, \"height\":1.73, \"is_enrolled\": True } joana = { \"age\":25, \"height\":1.75, \"is_enrolled\": False } # Now we can do some comparisons: is_peter_taller_and_older_than_joana = peter[\"age\"] &gt; joana[\"age\"] and peter[\"height\"] &gt; joana[\"height\"] is_peter_not_enrolled = not peter[\"is_enrolled\"] is_joana_not_enrolled = not joana[\"is_enrolled\"] is_peter_or_joana_enrolled = peter[\"is_enrolled\"] or joana[\"is_enrolled\"] . 3.4 Identity and membership operators . The identitiy operator “is” is to check whether two objects are the same. On the other side, the membership operator “in” checks whether an object is contained within another object. Simple examples: . a = [1,2,3] # a simple list b = a # we assign a to b, remember non-primitive data types? a is b # What will be the result of this? 1 in a # we can test whether a contains a number 1 c = [a,b] # here we create a new list that contains the lists a and b a in b # now we can check whether one of the lists is within another list a in c . Code block in details in a notice . Exercise . Now you know all about operators. Try to use your knowledge and figure out what we test for in the following operations and what the result is: . joana = { \"enrolled\": True, \"grade_ecophysiology\": 1.3, \"grade_archery\": 1.3 } alfonso = { \"enrolled\": True, \"grade_ecophysiology\": 1.7, \"grade_archery\": 4.3 } legolas = { \"enrolled\": False, \"grade_ecophysiology\": 4.0, \"grade_archery\": 1.0 } # 1. a = (legolas[\"grade_ecophysiology\"] &lt; joana[\"grade_ecophysiology\"]) and (legolas[\"grade_ecophysiology\"] &lt; alfonso[\"grade_ecophysiology\"]) # 2. b = (legolas[\"grade_archery\"] &lt; joana[\"grade_archery\"]) and (legolas[\"grade_archery\"] &lt; alfonso[\"grade_archery\"]) # 3. c = not (legolas[\"grade_ecophysiology\"] &lt; joana [\"grade_ecophysiology\"]) or (alfonso[\"grade_ecophysiology\"] &lt; joana [\"grade_ecophysiology\"]) # 4. d = joana[\"enrolled\"] and alfonso[\"enrolled\"] and legolas[\"enrolled\"] # 5. e = alfonso[\"grade_ecophysiology\"] &gt; 4.0 or alfonso[\"grade_archery\"] &gt; 4.0 # 6. f = (alfonso[\"grade_ecophysiology\"] &gt; 4.0 or alfonso[\"grade_archery\"] &gt; 4.0) or (legolas[\"grade_ecophysiology\"] &gt; 4.0 or legolas[\"grade_archery\"] &gt; 4.0) or (joana[\"grade_ecophysiology\"] &gt; 4.0 or joana[\"grade_archery\"] &gt; 4.0) . Solution! . | Check 1 tests whether legolas is the best ecophysiologist. The result is False. | Check 2 tests whether legolas is the best archer. The result is True. | Check 3 tests whether legolas or alfonso are better ecophysiologists than joana. With the \"not\" in the beginning, the result is turned into whether Joana is better than any of the two. The result is True. | Check 4 tests whether everyone is enrolled. The result is False. Legolas is probably buisy somewhere else... | Check 5 tests whether Alfonso failed one of the exams with a grade higher than 4.0. The result is True. | Check 6 tests whether anyone failed one of the exams with a grade higher than 4.0. The result is True. | . ",
    "url": "/New_BAI_DataAnalysis/python_1_basics.html#3-operators",
    
    "relUrl": "/python_1_basics.html#3-operators"
  },"8": {
    "doc": "1. BASICS OF PYTHON",
    "title": "4. Conditionals and Loops",
    "content": "Conditionals and loops are constructs in your code that are often combined. A loop is used to do a certain task on many elements sequentially, a conditional uses a certain condition (or truth-evaluation) to decide whether a piece of code should be executed. 4.1 Conditionals . Remember how we talked about comparison, logical and identitiy and membership operators? They all result in a boolean, stating whether a condition is True or False. We can make use of that by utilizing conditionals. Here is a simple example: . is_peter_smart = True if is_peter_smart == True: print(\"Peter is smart\") . Notice how indentation plays a role here! We end the line of the if-check with a “:” and start the new line indented. Indented lines signal a code block, that always belongs to the previous statement that ended with a “:”. In the above example the print() command will be executed because the value of is_peter_smart is True. If we check for a boolean value (True or False) we can also leave the comparison operation out and ask very coloquially: . is_peter_smart = True if is_peter_smart: print(\"Peter is smart\") . We can also define a code that should be executed, ONLY if the if-check is evaluated as False. For that we use the keyword “else”. is_peter_smart = True if is_peter_smart: print(\"Peter is smart\") else: print(\"Peter is not smart\") . Finally, you can also chain if-checks by using the “elif” keyword. This stands for “else if”, meaning that “if the previous checks failed and this check is evaluated as True, run the code” . is_peter_smart = False is_peter_big = True if is_peter_smart: print(\"Peter is smart\") elif is_peter_big: print(\"Peter is big\") else: print(\"Peter is not smart and not big\") . 4.2 Loops . A loop is a structure that allows you to iteratively perform actions, either with several elements (e.g. stored in a list) or while a specific condition holds. These two types are called “for-loops” and “while-loops”. They always consist of two parts: The definition how and over what you want to iterate (or “loop”) and the actual action you want to perform. 4.2.1 The for-loop . The most “classical” loop is the for-loop. The syntax is, as often in Python, held very simple. Here is an easy example: . temperatures = [12,14,16,15,16,17,20,21] for temperature in temperatures: print(temperature) . Notice that in the definition of the loop, we define a new variable called “temperature”. This variable represents the element we are currently working on in each step of the loop. So in the first step, temperature is 12, in the next temperature is 14 and so on. There is one very handy built-in method that can give you both the value of the list-entry and its corresponding index, called enumerate(). You can put them both in variables by using a comma in the loop-definition. A quick demo: . temperatures = [12,14,16,15,16,17,20,21] hour_of_day = [8,9,10,11,12,13,14,15] # When using enumerate, each iteration we get the index and value of the current list entry. # So in the first loop index will be 0 and temperature 12, # next index will be 1 and temperature 14 and so on... for index, temperature in enumerate(temperatures): print(\"Temperature at \"+str(hour_of_day[index]) + \":00: \"+str(temperature) + \"°C\") . 4.2.2 The while-loop . A while loop is not used as often as a for-loop. In the definition you define a condition and “while” that condition holds, the loop is executed. Look at this example: . a = 1 while a &lt;= 10: print(a) a = a +1 . Can you guess what will be display? . Solution It will print the numbers 1 to 10, including 10 Warning . When you define a while-loop, always make sure that the condition will at some point be fullfilled. Otherwise it can easily happen that youre while-loop just keeps running endlessly! . a = 1 # This loop will run forever, because a will never be &gt; 10! while a &lt;= 10: print(a) . Exercise . Now you already know quite some tools for writing a Python script! Use your knowledge to complete the code below. The goal is to print the sentence \"{month} was a hot month\" whenever the mean monthly temperature is above two times the mean and \"{month} was a dry month\" whenever the precipitation was less than half of the mean. One tip: For the printing you can use formatted strings. They make inserting variables in a string much easier! just put an \"f\" in front of the string and insert the variable in curly braces {}. For example print(f\"Hello {name}\" would print \"Hello Peter\" if the variable name=Peter is defined. Here is your starter code: . months = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"Juli\",\"August\", \"September\", \"October\", \"November\", \"December\"] monthly_temperature = [4, 4, 7, 11, 15, 17, 28, 24, 27, 12, 7, 4] monthly_precipitation = [15, 40, 60, 75, 65, 32, 10, 80, 60, 70, 57, 100] mean_temperature = mean_precipitation = for ... in enumerate(...): if ...: ... if ...: ... Solution! months = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"Juli\",\"August\", \"September\", \"October\", \"November\", \"December\"] monthly_temperature = [4, 4, 7, 11, 15, 17, 28, 24, 27, 12, 7, 4] monthly_precipitation = [15, 40, 60, 75, 65, 32, 10, 80, 60, 70, 57, 100] mean_temperature = sum(monthly_temperature)/len(monthly_temperature) mean_precipitation = sum(monthly_precipitation)/len(monthly_precipitation) for index, month in enumerate(months): if monthly_temperature[index] &gt; 2*mean_temperature: print(f\"{month} was a hot month\") if monthly_precipitation[index] &lt; mean_precipitation/2: print(f\"{month} was a dry month\") . ",
    "url": "/New_BAI_DataAnalysis/python_1_basics.html#4-conditionals-and-loops",
    
    "relUrl": "/python_1_basics.html#4-conditionals-and-loops"
  },"9": {
    "doc": "1. BASICS OF PYTHON",
    "title": "5. Functions and Classes",
    "content": "Congratulations! You made it this far down, that means you have accquired knowledge of the basic building blocks of Python. You are now ready to go into two concepts that go beyond basic scripting (meaning, just putting all your code line by line into one file), and learn about the fundamental blocks that help strcuturing your program: Functions and Classes! . 5.1 Functions . Functions are constructs of own, separate blocks of code in your program which take care of certain tasks. They are super useful, because often you want to do the same operation many times in your code but don’t want to write the same code every time again. Just write your own function and call it whenever you need its expertise! Lets just look at a simple example: . def calculate_mean(list_of_values): n_samples = len(list_of_values) sum_of_values = sum(list_of_values) mean = sum_of_values/n_samples return mean . Pretty intuitive, right? . A function is always defined by starting with the keyword “def”, then we give it a name, calculate_mean in this case. Afterwards in the braces are the “arguments” that the function takes. Arguments are pieces of information from the outside code, which the function requires to work. Here it is the list_of_values the function shall calculate the mean value of. After the “:” we follow with the indented codeblock that belongs to the function. Here we do all the operations the function should do. Finally, we use the “return” keyword which ends the function and defines, which piece of information should be returned to the outside code. Important The variables which are defined inside a function are restricted to that function! The outside code won’t know of the variables n_samples or “mean” which are defined in the function. Calling the function would for example look like this: . monthly_temperature = [4, 4, 7, 11, 15, 17, 28, 24, 27, 12, 7, 4] mean_monthly_temperature = calculate_mean(monthly_temperature) . You do not have to return a value. You could also for example print something in the function and then return, without providing a value to return. In older versions of Python this was all there was to writing a function. However, nowadays you can add some additonal information to make it even easier for the next person or your future self to understand it. With some extra bits you can add the infos, what type of data you expect as an input to the function and what type of data it will output. This is generally a good thing to do and now considered best practice when writing functions. For the above code it would look like this: . def calculate_mean(list_of_values:list[float]) -&gt; int: n_samples = len(list_of_values) sum_of_values = sum(list_of_values) mean = sum_of_values/n_samples return mean . In the first line, after the list_of_values we write “:list[float]” to specify that we expect a list of float (floats actually imply integers, so we can use that to also accept integers). After the closing bracket we write “-&gt; int” which states that this function will return an integer value. Exercise 5.1.1 . As a first exercise, try to figure out what the output of the below function will be without executing it! . def square_value(value:int) -&gt; int: return value * value def divide_value_by(numerator:int, denominator:int) -&gt; int: return numerator / denominator a = square_value(2) b = square_value(a) c = divide_value_by(b,a) d = square_value(divide_value_by(c,2)) print(d) . Solution! The result is 4! . def square_value(value:int) -&gt; int: return value * value def divide_value_by(numerator:int, denominator:int) -&gt; int: return numerator / denominator a = square_value(2) # 2*2 = 4 b = square_value(a) # 4*4 = 16 c = divide_value_by(b,a) # 16/4 = 4 d = square_value(divide_value_by(c,2)) # square_value gets the output of divide_value(c,2) as argument. print(d) # so 4/2 is 2, thn 2*2 is 4 . Exercise 5.1.2 . Lets go for a bit more challenging of an exericse (I am sure you are ready for it!) There is a built-in function that allows the user to give an input through the command-line to the program. It is simply called \"input()\". E.g. \"testword = input()\" would stop the program and wait for the user to input something in the console and then press enter. Imagine you want a program in which you set a new password. Write a function that checks whether the new password is longer than 9 symbols and that returns the corresponding boolean. the function should also print that the password is too short if it is too short and that it is ok when it is ok. Use the returned boolean to keep asking for new input from the user while the word is less than 9 characters long . Here is some starter code: . def is_password_too_short(word:str, min_length:int)-&gt;bool: is_password_too_short = ... if ...: .... else: .... return ... password_is_bad = True while ...: print(\"Please enter your password:\") password = input() password_is_bad = ... Solution! def is_password_too_short(word:str, min_length:int)-&gt;bool: is_password_too_short = len(word) &lt; min_length if is_password_too_short: print(f\"Password has to be at least {min_length} characters long!\") else: print(\"New password set!\") return is_password_too_short password_is_bad = True while password_is_bad: print(\"Please enter your new password:\") password = input() password_is_bad = is_password_too_short(password, 8) . 5.2 Classes . Classes are the final fundamental building block of Python we will look at here. A class basically represents a blueprint of an object that has certain properties. As an example, when I am working with data on ecosystems it could be convenient to have an ecosystem class that includes information about the ecosystem type, the location as latitude and longitude, and some meteorological data. Lets look at an example: . class Ecosystem(): def __init__(self, id, IGBP_ecosystem_class, lat, lon, mean_annual_Tair, mean_annual_precip): self.id = id self.IGBP_ecosystem_class = IGBP_ecosystem_class self.lat = lat self.lon = lon self.mean_annual_Tair = mean_annual_Tair self.mean_annual_precip = mean_annual_precip . The definition of a class always begins with the class-keyword followed by the name of the class. It always has a first function called init() which is also called “constructor”. This method is used to create new instances of the class and assigns values to the class. The “self” keyword is in this context always used within a class to reference the class itself. Note, that “self” also has to be in the list of arguments for the function, but is does not get passed when you call the function. Many new words but stay with me, it is pretty simple when we look at an example, how we create a new instance: . # First we define the class class Ecosystem(): # see how the first argument here is \"self\" def __init__(self, id, IGBP_ecosystem_class, lat, lon, mean_annual_Tair, mean_annual_precip): self.id = id self.IGBP_ecosystem_class = IGBP_ecosystem_class self.lat = lat self.lon = lon self.mean_annual_Tair = mean_annual_Tair self.mean_annual_precip = mean_annual_precip # now we use that class to create a new ecosystem-object, # see how we have to provide every value defined in the constructor except for \"self\": amtsvenn = Ecosystem(id=\"amtsvenn\", IGBP_ecosystem_class = \"open shrublands\", lat = 52.176, lon = 6.955, mean_annual_Tair = 10.5, mean_annual_precip = 870) # Now you have stored all the info about amtsvenn in the \"amtsvenn\" # object and can access them whenever you want: print(amtsvenn.id) print(amtsvenn.IGBP_ecosystem_class) print(amtsvenn.lat) print(amtsvenn.lon) . Classes can not only comprise of the information associated with them but can also have methods associated specifically with them. For example we can create a function that prints all the information enclosed in the object. class Ecosystem(): def __init__(self, id, IGBP_ecosystem_class, lat, lon, mean_annual_Tair, mean_annual_precip): self.id = id self.IGBP_ecosystem_class = IGBP_ecosystem_class self.lat = lat self.lon = lon self.mean_annual_Tair = mean_annual_Tair self.mean_annual_precip = mean_annual_precip def print_ecosystem_information(self): print(\"=====================\") print(\"Ecosystem information\") print(f\"ID: {self.id}\") print(f\"IGBP ecosystem class: {self.IGBP_ecosystem_class}\") print(f\"Location (lat/lon): {self.lat}°/{self.lon}°\") print(f\"Mean annual air temperature: {self.mean_annual_Tair} °C\") print(f\"Mean annual precipitation: {self.mean_annual_precip} mm\") amtsvenn = Ecosystem(id=\"amtsvenn\", IGBP_ecosystem_class = \"open shrublands\", lat = 52.176, lon = 6.955, mean_annual_Tair = 10.5, mean_annual_precip = 870) # After creating the object we can use the classes functions like this: amtsvenn.print_ecosystem_information() . Exercise 5.2.1 . Lets do one exercise that can further show, why classes are great for creating reusable code. Try to write a function called \"Statistics\". This class will be a \"behavioural\" class, meaning it does not need to hold own data but rather holds some methods, that belong to the same topic. In that class, define functions that calculate the mean, the variance and the standard deviation of a given list. Then use that class to calculate these metrics of an arbitrary list. Hint: For the standard deviation you need to take the square root. You can do that with pythons built-in math module. You can use it like this: . import math math.sqrt(24) . Try to work out the solution yourself first! There is some starter code below, in case you get stuck though. Starter code class Statistics(): def calculate_mean(self, ...): ... def calculate_variance(self, ...): mean = ... squares = [] for value in values: squares.append(...) variance = ... return ... def calculate_stdev(self, ...): variance = ... stdev = .... return ... Solution! import math class Statistics(): def calculate_mean(self, values:list[float]): return sum(values)/len(values) def calculate_variance(self, values:list[float]): mean = self.calculate_mean(values) squares = [] for value in values: squares.append((value-mean)**2) variance = sum(squares) / (len(values)-1) return variance def calculate_stdev(self, values:list[float]): variance = self.calculate_variance(values) stdev = math.sqrt(variance) return stdev stat = Statistics() example_list = [1,2,3,4,5,5,6,7,123,1,1,4] mean = stat.calculate_mean(example_list) stdev = stat.calculate_stdev(example_list) variance = stat.calculate_variance(example_list) . We will not go deeper into classes here, but it is very important to understand the concept. Most Python packages are written in object-oriented style, which (in very simple terms) means that the methods are enclosed in classes. So knowing the basics makes it much easier to understand the following bits. ",
    "url": "/New_BAI_DataAnalysis/python_1_basics.html#5-functions-and-classes",
    
    "relUrl": "/python_1_basics.html#5-functions-and-classes"
  },"10": {
    "doc": "1. BASICS OF PYTHON",
    "title": "1. BASICS OF PYTHON",
    "content": " ",
    "url": "/New_BAI_DataAnalysis/python_1_basics.html",
    
    "relUrl": "/python_1_basics.html"
  },"11": {
    "doc": "2. DATA HANDLING AND VISUALIZATION",
    "title": "Of plots and pandas: Data handling and Visualization",
    "content": "In this second part of the course we will talk about how to handle, process and visualize data in Python. For that purpose we will make use of a few third-party libraries. NumPy and Pandas will help us store the data in array- and matrix-like structures (in Pandas more specifically Dataframes) and do some processing of the data. Pandas already has some visualization capabilities, but for nicer looks and configurability we will make use of the Plotly package. To underline that these are essential tools in Python, let me once again pull out the Stackoverflow 2023 survey: According to the ~3k respondants, Numpy, Pandas and Scikit-Learn (which we will use in the next lesson) are 3 of the 8 most used technologies in programming across all languages (disregarding web-technologies)! . For this part we will use some example data. It is a dataset from the german weather service DWD from the Ahaus Station (ID 7374) ranging from 1996 to 2023. Click here to download (25mb)…. Note The data is in .parquet-format. You may not have heard of it, but this is a very compressed and fast format. For example this dataset with 27 years worth of data, in Parquet this is 25mb of data, in .csv its 208mb. While you can not open .parquet directly in excel or a text editor like a .csv file, it is much much faster to load e.g. when using it in programming languages, which is exactly what we are going to do here. As a last note: NumPy is one of the older Python libraries and Pandas is actually built on top of it. However, because we work with example data and want to get hands-on as fast as possible, we will cover Pandas first and then go from there. Table of Contents . | Pandas | NumPy | Plotly | . ",
    "url": "/New_BAI_DataAnalysis/python_2_data_visualization.html#of-plots-and-pandas-data-handling-and-visualization",
    
    "relUrl": "/python_2_data_visualization.html#of-plots-and-pandas-data-handling-and-visualization"
  },"12": {
    "doc": "2. DATA HANDLING AND VISUALIZATION",
    "title": "0. Importing modules",
    "content": "Just a quick forword on importing libraries in Python. Pandas, Plotly and Numpy are all external libraries we need to import to our script in order to make them work. Usually we would also have to install them, but since we work in Anaconda, this is already taken care of for us! Very simply, to import a library you type “import” and then the respective name. Typically you want to give an “alias” to the package, which is basically a variable that you can then use to access all the methods in the package. For some packages there are long-standing standards of what names to use. For pandas for example this is “pd”: . import pandas as pd . You can also only import specific parts of a package, which can save memory. Going back to one exercise from the previous lesson, if you know that you will only use the sqrt function from the math package you can use the syntax . # Importing only a single function, squareroot from math import sqrt # Importing several functions, squareroot and greatest common divider from math import sqrt, gcd # theoretically you could also give an alias here from math import sqrt as squareroot # this does not make much sense though . ",
    "url": "/New_BAI_DataAnalysis/python_2_data_visualization.html#0-importing-modules",
    
    "relUrl": "/python_2_data_visualization.html#0-importing-modules"
  },"13": {
    "doc": "2. DATA HANDLING AND VISUALIZATION",
    "title": "1. Pandas",
    "content": "Pandas is around since 2008 and one of the most wiedely used tools for data analysis of all. The usage is all about two types of objects: The pandas Series and the Pandas DataFrame, where a Series is more or less one column of a dataframe (basically a vector). If you already worked with R, the concept of a DataFrame is not new to you. However for starters, a DataFrame is basically a table, in which each row has an index and each column has a label. Simple right? . (credit: https://www.geeksforgeeks.org/creating-a-pandas-dataframe/) . Creating DataFrames . Lets create a first little DataFrame. There are several ways to do it, one rather intuitive way is to use a dictionary. Think about it, a dictionary already has values which are labeled by keys. You can easily imagine this in a table-format: The keys will be the column-labels and the indices (row-labels) are by default just numbered. import pandas as pd # Note that we create an instance of the class \"DataFrame\" # Therefore we have to call the function pd.DataFrame(). Within # the brackets we then define a dictionary using the {}-style syntax values_column_1 = [2,4,6,8,10] values_column_2 = [3,6,9,12,15] df = pd.DataFrame({ \"column_1\": values_column_1, \"column_2\": values_column_2 }) . Another option to create a dataframe is of course to read in data. Lets go ahead and read the data from the german weather service that you can download above. Now we can use pandas built-in data-reader to directly create a DataFrame from the parquet-file: . # The path can either be the absolute path to the place where you saved the file # or the relative path, meaning the path relative to the place where your script is. # I'd recommend to create a subfolder where your script is called \"data\" and then # import the data from the path \"./data/ahaus_data_1996_2023.parquet\" df_dwd = pd.read_parquet('path_to_file') . Accessing rows and column . Once you createad a dataframe, you can access individual columns by using the column names. Either you can directly access them using brackets, or you use the built-in “.loc”-function. I would recommend getting used to the .loc right away, as it rules out some errors you can run into otherwise. With .loc you always have to provide first the rows you want to access and then the column, separated with a comma. If you want to get all rows, that is done using a colon (“:”) To get a list of all availabel columns you can simply type “df.columns” . # First we can take a look at the available columns df_dwd_columns = df_dwd.columns print(df_dwd_columns) # Then we can use the column names to extract a column # from the dataframe # Either you use only the column name in brackets: df_dwd[\"tair_2m_mean\"] # But even better: use the .loc function: df_dwd.loc[:,\"tair_2m_mean\"] # get all rows df_dwd.loc[20:50,\"tair_2m_mean\"] # get rows 20 to 50 df_dwd.loc[:20,\"tair_2m_mean\"] # get all rows up to 20 (including 20) df_dwd.loc[20:,\"tair_2m_mean\"] # get all rows after 20 (including 20) . Note that the .loc examples above all assume numeric index. But Pandas is not restricted to that! The index (or “row-label”) could also be something like “mean” or “standard-deviation”. Keep that in mind for the exercise below! . Built-in methods to describe the data . Pandas has a great set of convenience functions for us to look at and evaluate the data we have. | .info() gives us a summary of columns, number of non-null values and datatypes | .head() and .tail() show the first or last five rows of the dataframe | .describe() directly gives us some statistical measures (number of samples, mean, standard deviation, min, max and quantiles) Note that the output of .describe() is again a DataFrame, that you can save in a variable to evaluate it. There are also built-in methods that you can run directly on single columns. Examples of such functions are .mean(), .min(), .max() and .std(). | . Exercise . You already know, how to call a method that is attached to a class. With that knowledge, explore the Ahaus DWD dataset and figure out the mean, standard deviation, min and max for air temperature, precipitation height, air pressure and short wave radiation (SWIN) . Hint It may be that the output of the .describe() function has a pretty bad formatting with 5 decimal numbers or more. In that case you can change the formatting of the output using . df.describe().applymap('{:,.2f}'.format) . Solution! # There are lots of ways to complete this exercise. # You can use the above mentioned describe() method # First get the summary. Save the output of .describe() # in a new dataframe df_dwd_summary = df_dwd.describe().applymap('{:,.2f}'.format) # Then you can access values in that dataframe like this: tair_2m_mean = df_dwd_summary.loc[\"mean\", \"tair_2m_mean\"] tair_2m_min = df_dwd_summary.loc[\"min\", \"tair_2m_mean\"] # and so on... # You could also directly use the pandas built-in .min, .max, # .mean and .std methods. For example: tiar_2m_mean = df_dwd[\"tair_2m_mean\"].mean() tiar_2m_min = df_dwd[\"tair_2m_mean\"].min() # and so on... Challenge . There is a one-line solution to this task, that only grabs the values asked for in the exercise. I wouldn’t say that that would be the recommended solution for the sake of overview, but to fiddle around it is a good challenge. Hint: You can pass lists for the row- and column-labels to .loc . Solution # We can chain all the commands above to a one-line operation, meaning we # directly call .describe().map().loc[] on each others output. # By passing the list [\"mean\", \"std\", \"min\", \"max\"] as row-indices and # [\"tair_2m_mean\",\"precipitation\",\"SWIN\",\"pressure_air\"] as column-labels # we can directly access the range of values asked for in the exercise. df.describe().applymap('{:,.2f}'.format).loc[[\"mean\", \"std\", \"min\", \"max\"], [\"tair_2m_mean\",\"precipitation\",\"SWIN\",\"pressure_air\"]] . Datetime . Pandas has a specific datatype that is extremely useful when we are working with time series data (a s our example DWD dataset). It is called datetime64[ns] and allows us to do a range of super useful things like slicing based on dates or resampling from 10-minute to daily, weekly, monthly data and so on. With datetime-indices, handling timeseries gets so much more convenient. # get data newer than 31.12.2022 df_dwd[df_dwd[\"data_time\"] &gt; \"2022-12-31\"] # get only data from 2022 df_dwd[df_dwd[\"data_time\"].dt.year == 2022] # But wait! Its not working, is it? # Can you figure out why not? Remember the type() function! . Now, how do we get this to work for us? Well, the methods work with the datetime64 data type, so we need to change the “data_time” column type! Luckily, Pandas has a function for that. It is called to_datetime() and is part of the main library, so you call it as pd.to_datetime(). It takes the column you want to convert to datetime64 type as argument, tries to parse it to datetime64 and returns the result series. If it fails to parse, maybe because your data_time column is in a country-specific formatting, you can pass an additional “format” argument in which you provide the input format. But we will not cover it here, as the default should work for the DWD dataset. example_df = pd.DataFrame({ \"data_time\": [\"2022-01-01 01:00:00\",\"2022-01-01 12:00:00\", \"2022-01-02 01:00:00\", \"2022-01-02 12:00:00\", \"2022-01-03 01:00:00\", \"2022-01-03 12:00:00\"], \"values\" : [1,5,4,20,6,-10] }) type(example_df[\"data_time\"]) example_df[\"data_time\"] = pd.to_datetime(example_df[\"data_time\"]) . Exercise . 1. In your dataframe, turn the \"data_time\" column into a datetime64 type column. Then create dataframes for each season across all years, meaning one for spring, summer, autumn and winter each. The respective months are March to May, June to August, September to November and December to February. Compare the mean air temperature, precipitation and radiation between the different seasons. 2. Find the dates of the maximum temperatures measured in the dwd dataset. One hint: What we want to do here is to find those rows, where the value is one of a set of values. To do so you can use the built-in pandas function .isin(). An example: . # Here is an example series (representing a column of a dataframe) series = pd.Series([1,2,3,1,2,3,1,2,3]) # Wen want to extract the rows where the value is 1 or 3: desired_values = [1,3] series_ones_and_threes = series[series.isin(desired_values)] # Note that the indices in the extracted series are the ones from series, where the value is 1 or 3, # so it really represents an extracted subset of the original series . Solution! df_dwd[\"data_time\"] = pd.to_datetime(df_dwd[\"data_time\"]) df_dwd[\"data_time\"] = pd.to_datetime(df_dwd[\"data_time\"]) # First of all we create 4 dataframes, one for each season # We do it by accessing the numeric value of the months in the \"data_time\" # column. 1 refers to January and so on. With the .isin() method we extract # those rows where the values correspond to the numbering of the month df_dwd_summer = df_dwd.loc[df_dwd[\"data_time\"].dt.month.isin([6,7,8])] df_dwd_autumn = df_dwd.loc[df_dwd[\"data_time\"].dt.month.isin([9,10,11])] df_dwd_winter = df_dwd.loc[df_dwd[\"data_time\"].dt.month.isin([12,1,2])] df_dwd_spring = df_dwd.loc[df_dwd[\"data_time\"].dt.month.isin([3,4,5])] # To find the mean for each season we have a range of different options # how we want to get the means and compare them. I'll show three different # ways which are all valid. # We know we will want to do some operation on all of the 4 datasets, so it is # already a good idea to put them in a list. That way we can easily iterate over them seasonal_datasets = [df_dwd_spring, df_dwd_summer, df_dwd_autumn, df_dwd_winter] # Now one option would be to iterate over the datasets and print # the mean values of the desired columns: seasons = [\"spring\", \"summer\", \"autumn\", \"winter\"] for idx, df in enumerate(seasonal_datasets): print(\"----------\") print(seasons[idx]) print(\"----------\") print(f'mean Ta: {df[\"tair_2m_mean\"].mean()}') print(f'mean precipitation: {df[\"precipitation\"].mean()}') print(f'mean SWIN: {df[\"SWIN\"].mean()}') # This way we have the outputs grouped by seasons # Another option would be to iterate over the variables we want # to evaluate. Then we can print the variable values for each # season directly below each other: variables = [\"tair_2m_mean\", \"precipitation\", \"SWIN\"] for idx, variable in enumerate(variables): print(\"--------\") print(variable) print(\"--------\") for i, df in enumerate(seasonal_datasets): stats = df.describe() print(f\"{seasons[i]}: {stats.loc['mean', variable]}\") # Often times we don't even want to print the output but rather # just extract and keep it for later use, e.g. for visualizing it later. # So another option is to create a new dataframe that holds # the seasons as columns and variables as rows. That way we can # just look at the whole new dataframe and easily compare the values seasonal_df = pd.DataFrame(columns = seasons) for idx, df in enumerate([df_dwd_spring, df_dwd_summer, df_dwd_autumn, df_dwd_winter]): season = seasons[idx] seasonal_df.loc[\"Ta\", season] = df[\"tair_2m_mean\"].mean() seasonal_df.loc[\"Precip\", season] = df[\"precipitation\"].mean() seasonal_df.loc[\"SWIN\", season] = df[\"SWIN\"].mean() print(seasonal_df) . In this exercise we extracted seasonal information from 5-minute interval data. This type of frequency-conversion is something we do very often when working with time-series data. We also call this operation “resampling”. Pandas actually has a great convenience function, that makes resampling a breeze, utilizing the wonderful datetime64-format. The operation consists basically only of two function calls on the pandas dataframe. The first is “.resample()”. We must define the column that contains the datetimes with the “on” argument and our target frequency with the “rule” argument as a string. The most useful frequency specifiers are: . | “S”: seconds | “T” or “min”: minutes | “H”: hours | “D”: days All of these can be extended with numbers, such as “7D” for 7 days or “30min” for half-hourly values. | . Afterwards we also have to call a function that specifies how we want to resample. You see, if we change the frequency from 5 minute data to daily data, the daily value can be computed in different ways. For example for temperature it would make sense to use the daily mean value. For precipitation on the other hand it is probably more useful to get the daily sum, if we are interested in the amount of rain per day. That is why “.resample()” has to be followed by a function like “.mean()” or “.sum()”. Here is a full example: . example_data_time = pd.to_datetime([\"2022-01-01 01:00:00\",\"2022-01-01 12:00:00\", \"2022-01-02 01:00:00\", \"2022-01-02 12:00:00\", \"2022-01-03 01:00:00\", \"2022-01-03 12:00:00\"]) example_df = pd.DataFrame({ \"data_time\": example_data_time, \"values1\" : [1,5,4,20,6,-10], \"values2\" : [100,500,400,2000,600,-1000], }) df_daily_means = example_df.resample(rule=\"1D\", on=\"data_time\").mean() df_daily_sums = example_df.resample(rule=\"1D\", on=\"data_time\").sum() . I mentioned before that pandas itself already has some built-in plotting capabilities. I won’t go deep onto it, but it is definitely worth mentioning because you can use it to get a quick overview of data in a pandas dataframe. You can simply call the “.plot()” function on any dataframe. You can run this on the whole dataframe, which will plot all available columns, or you extract specific rows/columns with the methods we discussed before and then run “.plot()”: . # lets use the example_df from above: # First we plot both values1 and values2: example_df.plot(x=\"data_time\") # Note that we have to say that we want data_time on the # x-axis, because otherwise it will by default use a numeric # index on the x-axis and plot the data_time column against it # as well. You can try it out by leaving the x=... out # The figure should now pop up in the \"plots\" tab on the right # side of your Spyder window # Now we just plot values 1: exampl_df.plot(x=\"data_time\", y=\"values1\") . There is a lot more functionality, but I want to leave the pandas plotting with that, as we will dive deeper into plotting later in this course. Ok, we have covered quite some ground on handling pandas dataframes. We covered . | how to create dataframes | how to read data from .csv or .parquet files | how indexing works | how we get some descriptive information on the data | how to compute some informative values such as the min, max and mean of a series | even how datetime-indices work (honestly we just scratched the surface, but for an introduction course this is already quite advanced) | and how to resample time-series data to another frequency Finally I just want to give some “honorable mentions”, to tell you about functions with pandas that you will probably need at some point. No exercise here, I just want you to have heard of these: | . # 1. pd.concat(): # this function concatenates dataframes with matching columns or pandas series # meaning it simply glues one dataframe on the bottom of the next: df_1 = pd.Series([1,2,3]) df_2 = pd.Series([4,5,6]) df_3 = pd.concat([df_1, df_2]) # note that we have to put the two dataframes in a list # 2. pd.merge() # This functions combines dataframes based on common indices. # It is a rather complex function but this is a simple example # how to combine two dataframes that have overlapping indices: df_1 = pd.DataFrame( index = [1,2,3], data = { \"col_1\": [1,2,3], \"col_2\": [4,5,6] }) df_2 = pd.DataFrame( index = [3,4,5], data = { \"col_1\": [7,8,9], \"col_2\": [10,11,12] }) df_3 = df_1.merge(right=df_2, how = \"outer\") # 3. df.apply() # In the call to apply you can define a function that will be # executed on each element of the dataframe: df_1_plus_one = df_1.apply(lambda x: x+1) # don't worry about the \"lambda\", it simply creates # the variable \"x\" we can use for \"x+1\". x is only there # during the computation and then immediately vanishes again . ",
    "url": "/New_BAI_DataAnalysis/python_2_data_visualization.html#1-pandas",
    
    "relUrl": "/python_2_data_visualization.html#1-pandas"
  },"14": {
    "doc": "2. DATA HANDLING AND VISUALIZATION",
    "title": "2. A quick touch on Numpy",
    "content": "Many Python programmers and data scientists would probably shun me for not giving more time to numpy, but we want to get to the applications as fast as possible. However, if you want to know more you can . | download a little Numpy cheat sheet here | check out the official Numpy documentation | read about numpy at w3schools.com Numpy is like the grandmaster of handling data in Python. It has always been there, it can do everything, but it is not neccessarily pleasant to deal with. With Numpy you can create vectors and multi-dimensional matrices, do computations and much more. It is very lightweight (meaning it uses very little memory) and super fast. Actually, Pandas has Numpy as its underlying framework. Every column or row in a pandas datframe is actually a Numpy array with fancy extras. That makes Pandas slower than Numpy but also much more convenient to use. While we can do most of our analysis in this course only with Pandas, I think you should know about the basic functionality and the core uses of Numpy. So lets take a look at some simple structures and computations: | . 2.1. Numpy Arrays . The most used structure in Numpy are arrays. In contrast to normal Python lists, they are faster, they force the values to be homogeneous (e.g. no strings and integers mixed in a Numpy array), and with Numpy arrays you can compute some mathematical operations between arrays such as element-wise addtion, cross-products and so on. Additionally, numpy provides a range of functions you can run directly on arrays, such as .mean(), .min(), .max(), .median() and so on. Generally you can think of Numpy arrays/matrices vs Pandas Dataframes/Series as the difference between pure vectors or structures with pure numeric data in them vs. fully fledged and labeled tables. There are different ways to create Numpy arrays: . import numpy as np # a simple vector is created by calling np.array # with a list as argument: vector_1 = np.array([0,1,2,3,4,5]) # alternatively, you can directly create a vector # filled with zeros or ones providing a shape. # The shape has round brackets and defines the # dimensions of the data structure. For example # (2,3) will create a matrix with 2 rows and 3 columns vector_zeros = np.zeros(shape=(2,3)) vector_ones = np.ones(shape=(2,3)) # with np.random.rand() you can create a matrix with # random elements between 0 and 1, by multiplying it # you can get e.g. values between 0 and 10: vector_randoms_0_to_1 = np.random.rand(3,10) vector_randoms_0_to_10 = np.random.rand(3,10)*10 # You can then get individual elements from that 2-D # structure with indexing. For example to get the # the second element in the first column: vector_randoms_0_to_10[0,2] = 2 # You can find the shape of a numpy object with vector_zeros.shape() # Lastly you can create arrays with consecutive numbers with np.arange() # I takes a start, an end and an interval as arguments: range_10 = np.arange(0,10,1) range_10_halfsteps = np.arange(0,10,0.5) # The ranges are created including the first and excluding the # last number. 2.2. Useful Numpy functions . In addition to Numpys own data structures it provides a whole range of useful functions that can be used in other contexts as well. One function I probably use more than any other are np.floor(), np.ceil() and np.round(). These all round values. Floor returns the nearest lower integer, ceil the nearest upper integer and round rounds to a desired decimal point: . vector = np.array([1.1, 10.523124, 3.341]) vector_ceiled = np.ceil(vector) vector_floored = np.floor(vector) vector_rounded = np.round(vector, 2) . Numpy also provides some mathematical functions and constants. For example np.pi returns the value of pi, np.e returns Eulers number. Other mathematical operations include all angle computations such as np.sin(), np.cos() etc. These are all computed in radians, but you can turn them into degrees with np.degrees() . Exercise . Lets just do one quick exercise on numpy to get familiar. 1. Create a numpy array from 0 to 20 in steps of 0.1. 2. Compute the sin of the data, then compute the standard deviation of the sin data 3. Add some random noise to the data. To do so, use the np.random.rand(). The range of the noise should be between 0 and 0.5. Then compute the standard deviation of the noisy data. 4. Round the noisy values to 3 decimal places . Solution! vector = np.arange(0,21,.1) sin_vector = np.sin(vector) std_sin_vector = sin_vector.std() noisy_sin_vector = sin_vector + np.random.rand(len(sin_vector))*0.5 std_noisy_sin_vector = noisy_sin_vector.std() rounded_noisy_sin_vector = np.round(noisy_sin_vector, 3) . ",
    "url": "/New_BAI_DataAnalysis/python_2_data_visualization.html#2-a-quick-touch-on-numpy",
    
    "relUrl": "/python_2_data_visualization.html#2-a-quick-touch-on-numpy"
  },"15": {
    "doc": "2. DATA HANDLING AND VISUALIZATION",
    "title": "3. Data Visualization: Plotly",
    "content": "Finally! It is time to not only create endless boring arrays of numbers, but to mold them into beautiful, descriptive images that tell the story of what the data actually means. Because that is essentially what we are doing when plotting data. Nobody can look at a table of 100.000 rows and start talking about it, that is what we can achieve with data visualization. There are several libraries we could use for plotting in Python: . | Matplotlib: One of the most widely used frameworks. It is lightweight, built into Pandas but nobody really likes the syntax | Seaborn: A library built on top of Matplotlib. It makes the syntax quite a bit easier, provides nice out-of-the-box plot styles, but the documentation is a bit lacking and plots are not that easy to customize | Plotly: The solution we will be using here. Plotly is built on a Javascript library Plotly.js and therefor brings some unique features to the table. The syntax and strcuture is quite good to learn, it offers a load of customization. Additionally, it offers very nice interactivity with the plots which makes exploration of your data much easier | . Lucky for us, Plotly is already included in Anaconda, so we do not need to install it. Plotly provides two different approaches to plotting: . | Quick and easy plots with less customization using plotly express | fully fledged figures with full customization options using graphic-objects | . To get a good understanding of Plotly it makes sense to go from large to small, first looking at the general structure of Plotly figures and the way graphic_objects work. If you have a broad overview of these you can still learn about the quick-and-easy ways, but you will have a much easier time when you want to change something about the express solutions manually. ",
    "url": "/New_BAI_DataAnalysis/python_2_data_visualization.html#3-data-visualization-plotly",
    
    "relUrl": "/python_2_data_visualization.html#3-data-visualization-plotly"
  },"16": {
    "doc": "2. DATA HANDLING AND VISUALIZATION",
    "title": "Plotly - The modern plotting library",
    "content": "Where to find help . First of all lets gather some ressources. The two best places to find advice about any plotly-related questions are . | the official documentation at plotly.com | the plotly community forum | as always, Stackoverflow… | . The general structure of Plotly figures . First of all we need to go through a little bit of vocabulary to be able to talk about Plotly. In Plotly-world the whole image of a plot, including the axes, the data, the labels, the title and everything is called the “graph-object”. This is the top-level of every Plotly figure and it is also the name of the Python class, with which we build the plots. Within the graph-object there are two layers: One is the “data” layer with everything that is directly related to the displayed data. That is the data itself, the mode of repesentation in the graph for example the line (in a line-plot) or points (in a scatter-plot) and the styling such as the size or color of the line/points. In plotly, they also call the group of data-related attributes “traces”. Don’t ask me how they came up with it but we have to live with it… We will come back to that later! The second part of the figure is the “layout” layer. It includes everything that makes the graph besides the data itself, for example the axes, the titles on the axes, the title of the graph itself, the legend, colors, maybe a template and so on. In the image below I tried to highlight the areas including the “data” area in red and the “layout” related areas in green: . Lets dive into the code and create a first figure object. Its easy: . # Before we start, lets resample the dwd data down to daily values. # You will create quite some plots and plotting 27 years of 10-minute # data takes a bit of time. df_dwd_daily = df_dwd.resample(\"d\", on=\"data_time\").mean() # First import the graph_objects module from plotly. # We call it \"go\" because that is convention import plotly.graph_objects as go import plotly.io as pio pio.renderers.default = \"browser\" #This will force Plotly to open plots in your default web browser. # Then we create out figure like this: fig = go.Figure() # Check out what happens, when you print this # object with print(fig). You will see the structure # we talked about above! . Well, now we have a graph-object without any data. From printing the figure you can see that the “data” is an empty list. Lets change that and add some data from out dwd-dataset. To do so, we have to add a “trace” (remember how we introduced that above). We do that by calling the .add_trace() method on the figure. In the function the first thing we have to define is, what kind of graph we want to create. Otherwise the empty figure wouldn’t know whether it should become a scatter-plot, a histogram, a line-plot or anything else. We define the type of graph by giving an object of the graph-type we want to the “add_trace()” method. These objects are also included in our “graph-objects” (or “go”). Sounds complicated, but really it is not. Check this out: . # This is the bare figure fig = go.Figure() # Now we will add some data: fig.add_trace( # On fig we call the \"add_trace()\" method go.Scatter( # In the method we provide an object of type \"Scatter\" from \"go\" x = df_dwd_daily.index, # then, in go.Scatter we define, which data should be plotted y = df_dwd_daily[\"tair_2m_mean\"],# on the x- and on the y-axis ) ) # Now print the figure again and look the output # You will see that the \"data\" level now has the x- and y-data in it # Plotly has very nice interactivity. To open the graph # in an interactive browser-window type this: fig.show() # or you save the figure to an image like this: fig.write_image(\"daily_tair_2m_mean.png\") . Above we created a scatter-plot (every data point is a dot in the graph). But if you look at the plot, you’ll note that there is still a lot missing. Most importantly, it does not have axis-labels. We need to add those, so people know what is plotted here! Lets do it. Which part of the figure do you think we need to change to add axis-labels? . SolutionThe \"layout\" bit of the figure So lets see how we can change the layout of the figure! . # to get to the layout of the figure we have two options: # 1. The figure object \"fig\" has the \"layout\" property, # which has an \"xaxis\" property, which again has a \"title\" # property. We can go down this path manually like this: fig.layout.xaxis.title = \"Date\" fig.layout.yaxis.title = \"Tair [F]\" # 2. The second option is to use the \"update_layout() method. # This was was made to make styling more convenient. We can use # it to \"group\" our styling in a single function call. fig.update_layout( xaxis_title=\"Date\", # Note that we use an underscore yaxis_title=\"Tair [F]\" # \"_\" to grab the \"title\" property from \"xaxis\" ) # Now you'll see, that the labels are changed in the figure: fig.show() . This is pretty much the way you can change any attribute that is related to the layout of the figure. The only thing you have to figure out for whatever you want to change in your figure is, where the respective property lies. Is it part of the data or the layout layer? Which sub-layers are there? Sometimes you can figure it out by thinking about it, however you can always refer to the documentation and the hive-mind of the internet. Especially in the beginning you’ll need to google quite a bite, but once you get the hang of it, it is actually quite intuitive. Lets do some more styling. Above we created a scatter-plot. This is a time-series, so maybe a line-plot would be more appropriate… . Exercise 1 . Try to change the style of our plot above to a line-plot. To do so, you need to change the \"mode\" property which is part of the \"data\" layer, or \"trace\". You can change the trace just like we changed the \"layout\" above with a function called \"update_traces(). Challenge: Can you come up with two different ways to change the mode? . Solution! # Option 1: fig.update_traces( mode=\"lines\" ) # Option 2 (which you usually wouldn't use): fig.data[0].mode = \"lines\" # The trick is that we have to write # fig.data[0], because the \"data\" property is # a list! You can see that if you look at the # printed figures \"data\" property, it starts with a \"[\". # The reason is of course that you could plot several # lines within a single plot. This way you could style # them one-by-one. However, generally you use the # \"update_traces()\" method to apply styles that # are used for all plotted data and pass everything # else directly when you create the data with \"add_trace()\" . Exercise 2 . Now lets expand the plot a bit. Add two more lines to the plot, the tair_2m_min and tair_2m_max columns from our dwd data. You can simply add them to the existing plot with the \"add_trace()\" method. When calling add_trace(), try to directly change the mode to \"lines\". When adding the lines, also add the argument \"name\" to the add_trace() method. That defines, how the line will be reprented in the legend. Give appropriate names to the lines. Additionally, try to change the line style of the min and max temperature to \"dashed\". If you want, you can also change the colors of the lines. To do so, change the line_color property. To define the color you can use either a string in the form of \"rgb(0,0,0)\" where you have to replace the zeros with rgb values, or you use one of the pre-defined colors which you can also pass as string. You can find a list of available color-names here: . w3schools list of CSS colors… . Solution! fig = go.Figure() fig.add_trace( go.Scatter( y = df_dwd_daily[\"tair_2m_mean\"], name=\"Tair 2m\", line_color=\"black\" ) ) # after adding the first line we just keep adding # more lines. We can directly change the name, # line_dash and line_color attributes: fig.add_trace( go.Scatter( y = df_dwd_daily[\"tair_2m_min\"], name=\"Tair 2m min\", line_dash=\"dash\", line_color = \"lightblue\" ) ) fig.add_trace( go.Scatter( y = df_dwd_daily[\"tair_2m_max\"], name=\"Tair 2m max\", line_dash=\"dash\", line_color=\"lightcoral\" ) ) fig.update_layout(xaxis_title=\"Date\", yaxis_title=\"T2m [F]\") fig.show() . Great, you are on the best way to becoming a Data-Painting Plotly-Wizard! Of course there are not just simple line and scatter charts. There is a whole world of graphs to explore!. For now, lets look at just one more type of graph, a bar-chart. This is a common type of graph to compare measured amounts (as opposed to discrete values such as a temperature). Such a value would be our rainfall measurement! . Exercise . Go ahead and create a bar chart of the sum of daily rainfall. You can create a bar-chart just like we did with the scatter plots above, only that you call \"go.Bar\" instead of \"go.Scatter\" Remember that when resampling precipitation to daily values, you need to use the sum instead of the mean! . Solution! # First of all we grab daily precipitation data by # resampling with the \"sum\" aggregation function # Here I directly grabbed only the precipitation-column, # but you can also do it another way precip = df_dwd.resample(rule=\"d\", on=\"data_time\").sum()[\"precipitation\"] # Now we create the graph just like before: fig_precip = go.Figure() fig_precip.add_trace( go.Bar( # Here we simply use go.Bar instead of go.Scatter x=precip.index, y=precip # Note: when using a Series instead of a dataframe ) # I dont have to pass the column name, because I ) # only have one column anyways... fig_precip.update_layout( xaxis_title=\"Date\", yaxis_title=\"Rain amount, daily [mm]\" ) fig_precip.show() . Right on, this was quite a deep dive into the Plotly library! But if you followed all the way down here, you are on a very good way to become super proficient in plotting data in python! The skills you got from the exercise above should get you quite far in designing your own figures in the future. If it is all a bit much in the start, don’t worry! As time comes you will do things much faster. For now, keep trying, keep googling, consult the documentation and most importantly: be happy with the progress you make! . Before we finish the visualization exercises I want to show a few more very helpful things. Subplots . Often you want to create not just one but multiple plots in one figure, for example one big figure with a temperature plot on top and a precipitation plot on the bottom. This way readers can easily get an overview of the climate at the station. Creating such a “subplot” in Plotly is super easy! Instead of using go.Figure(), you use a different function to create your top-layer “graph-object”. The function we need is plotly.subplots.make_subplots(). In it we can define the number of rows and columns of figures we want to create with the “rows” and “cols” keywords. Think about the whole figure like a matrix. The figure on the top-left will be row 1, column 1, second on the left row 2, column 1 etc. Then whenever you are adding a new trace, you can define its position with the properties “row” and “col”: . # First we create the subplots graph_object. # To do so we have to import that specific method: from plotly.subplots import make_subplots # Now we create a subplot figure with two rows: fig_subplots = plotly.subplots.make_subplots(rows=2, cols=1) # Now we can start adding traces to the figure: fig_subplots.add_trace( go.Scatter( x=df_dwd_daily.index, y=df_dwd_daily.tair_2m_mean, name=\"Tair mean\", line_color = \"black\" ), row = 1, # here we define, where the figure should be col=1, ) fig_subplots.add_trace( go.Bar( x=precip.index, y=precip, name=\"precip\", marker_color=\"blue\" ), row = 2, # precipitation will be the lower plot col=1 ) fig_subplots.show() . As a little exercise, print the fig_subplots object from above and try to figure out how to change the y-axis titles on the first and the second plot. Solution fig_subplots.update_layout( xaxis2_title=\"Date\", yaxis_title=\"Tair 2m [F]\", yaxis2_title=\"Rain amount, daily [mm]\" ) . Templates . One very nice way to style your figure a bit differently than the default is to use Plotly templates! You can implement them in your figure simply by adding the template in the layout: . fig_subplots.update_layout( template=\"simple-white\" ) fig_subplots.show() . Looks nice right? There is a whole gallery of templates available on the website: Plotly template gallery… . Plotly Express . For now, we will leave it with that. But wait, I was talking about an easier way to create graphs before right? Yes, for “quick and dirty” graphs you can use the awesome plotly.express shortcut. With it you can create a bunch of graphs like the ones we talked about above without all the fuzz of graph_objects etc. All you need to do to create a scatter plot is . import plotly.express as px fig_express = px.scatter(df_dwd_daily, y=\"tair_2m_mean\") fig_express.show() # You can update the fig_express just the same as the output # of go.Figure(), with update_layout and all of its beauty. Plotly express is very well connected with Pandas. This enables you to display all data of a dataframe in one combined, interactive overview char, simply by passing a pandas dataframe into it: . import plotly.express as px fig_express = px.scatter(df_dwd_daily) fig_express.show() . Plotly express includes many other graph types as well. You can find a very nice documentation on the plotly website…. One very last very useful thing I want to mention here is the addition of trendlines in Plotly Express. It is a super handy feature that is only implemented in Plotly Express, not in plain Plotly. I will show you how to do it and how you can grab all information the trendline can give you, but I will not explain the statistics behind it here. It happens very often that you want to plot two variables together, to see whether they are related. In order to explore the relationship, you can fit a linear line to the data and look at the parameters of the line-equation. For example, if I would plot air temperature on the x-axis against the same air-temperature on the y-axis, that would result in a perfect fit, the line would have a slope of exactly 1, the y-axis-intercept would be 0 and the r_square value of the regression would be 1, indicating the perfect fit. In Plotly express you can simply add a trendline to the plot by adding the “trendline” argument to the function call: . import plotly.express as px fig_express = px.scatter(df_dwd_daily, x=\"pressure_air\", y=\"tair_2m_mean\", trendline=\"ols\") results = px.get_trendline_results() print(results) fig_express.show() . But the bare line itself is not too useful, we need to grab the coefficients of the line, so the slope and the intercept of the line-equation. You can grab the results from the figure using px.get_trendline_results(fig_express). Honestly, the actual parameters are a bit hidden inside the deeper objects, but you can extract each one anyways: . import plotly.express as px fig_express = px.scatter(df_dwd_daily, x=\"pressure_air\", y=\"tair_2m_mean\", trendline=\"ols\") results = px.get_trendline_results(fig_express) # To actually access the results of the regression we need to # access the first row of the px_fit_results: ols_results = results.px_fit_results.iloc[0] print(ols_results.summary()) # ols_results is an object of the type \"OLSResults\" # which comes from a different library, \"statsmodels\" (https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLSResults.html) # From it you can grab the coefficients and a bunch of statistical infos: slope = ols_results.params[1] intercept = ols_results.params[0] rsquared = ols_results.rsquared # etc... I won’t go much more into the details because you can easily look up more plotly express functions (and I do encourage you to do so because it is super handy!), but by now you know enough about Plotly to explore that yourself. Why did we go through all the fuzz of handling the traces and layout ourselves? Because express is limited! If you want to customize your plots in some way it does not support out-of-the-box, you will have to dive into the figure-structure sooner or later, and now you know how. Still it is encouraged (even by Plotly themselves) to make use of both: run plotly express for a base figure or for quick data exploration, and then style the figure the way you want with the in-depth methods. ",
    "url": "/New_BAI_DataAnalysis/python_2_data_visualization.html#plotly---the-modern-plotting-library",
    
    "relUrl": "/python_2_data_visualization.html#plotly---the-modern-plotting-library"
  },"17": {
    "doc": "2. DATA HANDLING AND VISUALIZATION",
    "title": "Congratulations",
    "content": "on finishing this plot about plotting plots with Plotly! What a journey. I hope you can take away the knowledge to navigate plotting in Python fairly well from now on! Enjoy your future data-adventures and I wish you happy coding! . ",
    "url": "/New_BAI_DataAnalysis/python_2_data_visualization.html#congratulations",
    
    "relUrl": "/python_2_data_visualization.html#congratulations"
  },"18": {
    "doc": "2. DATA HANDLING AND VISUALIZATION",
    "title": "2. DATA HANDLING AND VISUALIZATION",
    "content": " ",
    "url": "/New_BAI_DataAnalysis/python_2_data_visualization.html",
    
    "relUrl": "/python_2_data_visualization.html"
  },"19": {
    "doc": "3. INTERPOLATION AND GAP FILLING",
    "title": "Interpolation and Gap Filling",
    "content": "In this exercise we look at interpolation and the rather time-series specific topic of data-gap filling. In time-series data we often have gaps due to a variety of reasons. They can result from instrumental issues or maintenance times or unfavorable weather conditions which leads to data being discarded. These data gaps can be filled with statistical methods. In this lesson exercises are not completely separated from the content. Just follow along, grab the code and in some parts you will get snippets to run and fiddle with yourself. Before we start plotting data we will see, how we can deal with missing values which are already handled by the institution measuring the data, e.g. the DWD. For example it is common that the data is included with a specific placeholder value, which we first need to handle. ",
    "url": "/New_BAI_DataAnalysis/python_3_interpolation_gapfilling.html#interpolation-and-gap-filling",
    
    "relUrl": "/python_3_interpolation_gapfilling.html#interpolation-and-gap-filling"
  },"20": {
    "doc": "3. INTERPOLATION AND GAP FILLING",
    "title": "1. Loading and  converting data:",
    "content": "We will use some data I have prepared in a way that you might find it in an online data portal. Download the file here . To test some things we will work with the air temperature column “tair_2m_mean” here. There are several issues when we have a missing-data-placeholder like that. Try two things: . Exercise . Look at a quick express plot of the data. Is that a meaningful representation? Then try to resample this data to daily values. Plot the data, do the values make sense? . Solution! df_dwd = pd.read_parquet('./dwd_ahaus_1996_2023_missing_placeholders.parquet') df_dwd[\"data_time\"] = pd.to_datetime(df_dwd[\"data_time\"]) fig = px.scatter(df_dwd, x = 'data_time', y = \"tair_2m_mean\") fig.show() . Plotting the data with the missing value placeholder makes the data barely readable: . df_dwd_daily = df_dwd.resample(rule = 'D', on = 'data_time').mean() fig_daily = px.scatter(df_dwd_daily, x = df_dwd_daily.index, y = \"tair_2m_mean\") fig_daily.show() . When averaging the values, the -999.99 values are taken into account leading to unrealistic results: . Lets look at the first way to solve this issue. We have to find the rows, where the values are the placeholder value. You can identify these rows by grabbing the right-row indices of the dataframe with a condition: . # we find the indices in the column of air temperature, where # the values are -999.99: # | here we find the rows we need: | column we need| # | those where the row tair is -999.99| # | # v v indices_of_missing_values = df_dwd.loc[df_dwd[\"tair_2m_mean\"] == -999.99, \"tair_2m_mean\"].index . We can then go ahead and replace these values with a special type, a “NaN”-value. NaN stands for Not a Number and represents specifically missing numeric values. This object is provided by the numpy package and works nicely with pandas: . import numpy as np # do this if you have not already imported numpy df_dwd.loc[indices_of_missing_values, \"tair_2m_mean\"] = np.NaN . Now the irritating -999.99 values are replaced and you can easily plot and resample the data in a meaningful manner: . fig = px.scatter(df_dwd,x = 'data_time', y=\"tair_2m_mean\") fig.show() df_dwd_daily = df_dwd.resample(rule=\"d\", on=\"data_time\").mean() fig_daily = px.scatter(df_dwd_daily, x = df_dwd_daily.index, y=\"tair_2m_mean\") fig_daily.show() . ",
    "url": "/New_BAI_DataAnalysis/python_3_interpolation_gapfilling.html#1-loading-and--converting-data",
    
    "relUrl": "/python_3_interpolation_gapfilling.html#1-loading-and--converting-data"
  },"21": {
    "doc": "3. INTERPOLATION AND GAP FILLING",
    "title": "2. Gap Filling, interpolation and modelling",
    "content": "In the next part we will discuss how we can work with timeseries that have gaps of different sizes. This is a regular task when working with long-time observations and there are a couple of options, depending on what data is available to you and what is the final evaluation goal you have in mind. 2.1: Simple linear interpolation . You do basic interpolation in your every day live. You want to bake a cake and only find a receipe for an 8 person cake, but only 3 friends are coming over for cake time. In the receipe you have to use 1 kg of flour. Intuitively you can see that since you will only be four at the table, you can alter the receipe and only use 500 g of flour. And already did you do some interpolation! What you easily did right away in your head could be mathematically formulated as: y = 125 * x where y is the amount of flour in grams and x is the number of people eating cake. The formula for an interpolation between two points (x1,y1) and (x2,y2) at a specific point (xn, yn) is: . $$ yn = y1 + \\frac{(y_{2}-y_{1})}{(x_{2}-x_{1})} * (x_{n} - x_{1}) $$ We simply construct a straight line where y1 is our y-intercept, the slope is derived from the two points with the well known slope-formula . $$ m = (y2-y1)/(x2-x1) $$ and our x value on this constructed line is difference between the point we want to look at minus the starting point . Note that in this form of y = mx + b we only have one x which we use to explain our y-value. We have one “predictor”. Using only one predictor gives us a so called simple linear regression. This is a super simple form of interpolation and of course leaves a lot of information aside. Lets look at a simple example of how to actually do linear interpolation in Python: . First we create a data set to play with. We create a simple running index from 1 to 11 and some made up y-values. We make one array in which all values are present and a second in which some values are missing. : . index = [1,2,3,4,5,6,7,8,9,10,11] data = { \"full_data\" : [1,2,0,13,4,10,19,15,13,21,27], \"missing_data\" : [1,2,0,np.NaN,4,np.NaN,19,15,13,np.NaN,27] } data = pd.DataFrame(index = index, data = data) . lets take a quick look at the two datasets: . First we create as simple plot to look at the characteristics of the data. Lets make a quick little function to keep a bit of styling: . import plotly.express as px def scatter_plot_interp(data, columns:list[str], show=True): fig = px.scatter(data, y=columns) fig.update_traces(marker_size=10) fig.update_layout(template=\"simple_white\") if show: fig.show() return fig scatter_plot_interp(data, [\"full_data\", \"missing_data\"]) . Since the two plots overlay each other, you can see the “missing” values in blue and all the ones in the reduced dataset in red. To do a linear interpolation between each adjacent points you can use a numpy function, np.interp() or a built-in pandas method. You can find its documentation here. The function takes 3 main arguments: . | The x-coordinates for which the data shall be interpolated | The x-coordinates of the input data | The y-values of the input data | . The catch with the numpy function however is that the function will return NaN if there are NaN-values in the input arrays. The pandas function is a lot easier, but we will have to deal more with this problem of getting rid of NaN values later when we use other models, so we can practice getting rid of NaN data in our training data now anyways. Specifically we will do the following: . | find indices of NaN values | find indices of non-NaN-values | first argument is where to interpolate, so provide the indices of the NaN values | second and third arguments are the x and y values of the adjacent non-NaN values, so provide the index and the y-values at the non-NaN indices | . # 1. get indices of missing and present points: indices_of_missing_points = data.loc[data[\"missing_data\"].isna()].index indices_of_present_points = data.loc[data[\"missing_data\"].notna()].index # 2. interpolate missing values and store them in # a new column in the dataframe. We can either do this with # the numpy function np.interp: data.loc[indices_of_missing_points,\"interpolated_data\"]= np.interp(indices_of_missing_points, data.loc[indices_of_present_points,\"missing_data\"].index, data.loc[indices_of_present_points,\"missing_data\"]) # the pandas approach is much easier to use and is simply: data[\"interpolated_data\"] = data[\"missing_data\"].interpolate() . In this approach all we did was to draw straight lines between adjacent points. As you see, for the first point the prediction was rather poor, the other two where pretty well reconstructed: . fig = scatter_plot_interp(data, [\"full_data\", \"interpolated_data\"]) . However, with this approach we leave all the information the other points give us about the data aside. Imagine for example that you have a timeseries where you measure temperature at midnight and at 12AM. If one datapoint was missing, you would connect the two night time temperatures and interpolate the daytime temperature way off. A simple measure of how well our model performed is to look at the root mean squared error. $$ RMSE = \\sqrt{\\overline{(y[i] - ypred[i])^2}} $$ where y is the true value and ypred is the predicted y value. You simply compute the model error for each datapoint and square them to avoid counter balancing of negative and positive errors. Then you take the mean of these values and finally take the square root, to get back into your data range. Exercise . Use your knowledge of pandas and numpy to write a function that returns the RMSE . Solution! def get_RMSE(y_true, y_predicted): RMSE = np.sqrt(np.mean((y_true - y_predicted)**2)) return RMSE y_true = data.loc[indices_of_missing_points, \"full_data\"] y_predicted = data.loc[indices_of_missing_points, \"interpolated_data\"] RMSE = get_RMSE(y_true, y_predicted) . 2.2: Simple linear models . Another approach is be to create a linear model that builds not only on the two points adjacent to the one we want to know, but rather the whole of the dataset that we have available. So what we want to achieve, is to find a function that constructs our unknown data points based on the data we have available in the best possible way. That means, that we want to have as little errors in our model as possible. The error is usually measured as the “sum of squared errors” (SSE) which is the total of distances between true values and the predicted values. We square it to avoid negative and positive values counterbalancing each other. Looking at an array of n data points we can write . $$ SSE = \\sum_{i=1}^n (y(i) - b - m * x(i))^2 $$ y(i) is the true y value at the predicted point, b is the y-intercept of the linear model, m is the first coefficient of the linear model and x(i) is the x-value at the predicted point. Since we want to find the straight line, that MINIMIZES the SSE, we call a procedure like this a “minimization problem” and specifically the estimation of this line is called a “least squares estimation”. In the easiest way of fitting a linear model to such a dataset, it all depends on the mean of our dataset. To derive the model parameters we can use the following relations where we replace b with alpha and m with beta (as that is the general standard). Also we will now denote the predicted y-value with a ^ on top of that, which is the common standard in literature. Sometimes this is also referred to as y_hat. $$ \\hat{y}_{i} = \\alpha + \\beta * x_{i} $$ $$\\alpha = \\bar{y} - (m \\bar{x})$$ $$\\beta = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n (x_i - \\bar{x})^2}$$ If we would do it by hand, we would simply plug in all the numbers we have into the expression for beta and use the result to derive our alpha . But we are working with Python so we will now introduce an awesome modelling and machine-learning library called “scikit-learn”. Scikit-learn has a huge amount of model-packages available, from simple linear regression all the way advanced statistical regressions, classifications and analysis tools. The documentation is also quite nice and extensive! We will make use of scikit-learn to fit a simple linear regression model to our data. However to do so we need to some tweaking of our data. Especially two things are important: . | Scikit learn can not work with NaN-data. That means we need to filter these out of the data we feed to the linear model | The model needs two-dimensional data. A list like [1,2,3] is one-dimensional and does not work with creating linear models. Instead we have to bring it to a form of [[1],[2],[3]] etc. We can do this using the “reshape” function. | . from sklearn.linear_model import LinearRegression # Scikit learn is quite object oriented. That means, # we imported a Class called \"LinearRegression\", which contains # all the following functions to work with the model. # Step 1: instantiate the class linearModel = LinearRegression() # Step 2: Fit the model. This is the process of # feeding our known data to the model and tweaking the # parameters so that the prediction error gets minimized. # However, sklearn can not work with NaN values, so again # we need to leave them out in the fitting: # --- repetition from before: indices_of_present_points = data.loc[data[\"missing_data\"].notna()].index x = data.loc[indices_of_present_points,\"missing_data\"].index.values.reshape(-1,1) y = data.loc[indices_of_present_points,\"missing_data\"].values.reshape(-1,1) # --- fitting the model: linearModel.fit(x,y) # Step 3: Check how well our model performed! sklearn has an inbuilt # function for it called \"score\". It returns the R^2 value for # the true values and the values predicted by the model: linearModel.score(x,y) . We can obtain the paramters of the linear model, that scikit-learn has created for us: . m = linearModel.coef_ b = linearModel.intercept_ print(f\"Linear equation: {m}*x+{b}\") . The last thing left to do is to use this model to predict our missing values. All we need to do is use the models “predict()” function and give it the indices we want to prediction for: . # first we create a new column consisting of NaN values data[\"sklearn_prediction\"] = np.NaN # then we replace the values in the missing rows with our model prediction: linear_prediction = linearModel.predict(indices_of_missing_points.values.reshape(-1,1)) data.loc[indices_of_missing_points, \"sklearn_prediction\"] = linear_prediction . We can look at out interpolated values by plotting them as red dots together with our reduced dataset: . fig_linmod = scatter_plot_interp(data, [\"full_data\", \"sklearn_prediction\"], show=False) . As you can see, the linear model already performs a bit better than the simple linear interpolation does. We can visualize the linear regression line by predicting the full array of x-values and plotting the result as a line: . data[\"yhat_full\"] = linearModel.predict(data.index.values.reshape(-1,1)) fig_linmod.add_traces( go.Scatter( x = data.index, y = data[\"yhat_full\"], mode=\"lines\", name=\"linear regression line\" ) ) fig_linmod.show() . Finally we need to look at statistical metrics to find out, how well our linear model performed. Luckily we can easily get a whole range of such metrics from the sklearn.metrics package. Lets define a simple function to grab a bunch of metrics at once: . import sklearn.metrics as metrics def regression_results(y_true, y_pred): # Regression metrics mse=metrics.mean_squared_error(y_true, y_pred) median_absolute_error=metrics.median_absolute_error(y_true, y_pred) r2=metrics.r2_score(y_true, y_pred) print('r^2: ', round(r2,4)) print('MAE: ', round(median_absolute_error,4)) print('MSE: ', round(mse,4)) print('RMSE: ', round(np.sqrt(mse),4)) . A few things we can take from this are: a) r^2 is the ratio of the sum of squared errors divided by the sum of squared deviations from the mean. You can say that r^2 is a measure of how much of the variance in the original data is reflected by the model. In this case, as our model is just a line, the amount of variance captured in the model stems from the linear trend that is inherent in the original data. Whether an r^2 is reflective of a good correlation depends heavily on the application. If you are a social scientist and work on voter behaviour an r_square of 0.65 may be spectacularly good. If you want to calibrate your measurement device and the reference and measured values have an r^2 of less than 0.85 you might want to check it again… . b) The mean squared error (MSE) is exactly that: we calculate the distance from each datapoint to its predicted counterpart, and to avoid negative errors counterbalancing positive ones, we square them. Then we take the mean of all errors. Due to the squaring the errors get quite high and are not directly interpretable. That is why we take the square root of the squared errors and get to the “root mean square error” (RMSE). This is an error very often reported in model performance evaluation, also often used in scientific papers. c) Lastly the median absolute error (MAE) is a different performance metric that gets shown not as often, but is still very useful. For the MAE, we also calculate each error, take the absolute of it (make negative values positive) and then grab the median value, so the one that sits right in the middle of all datapoints. Because we take the median instead of the mean, this metric is insensitive to outliers. If we have very low errors, but then a few extremely high ones (or the other way around), the mean value can be skewed while the median would not change. We wont go much deeper into statistical metrics here. But as you can see, this model does represent certain characteristics of the data regarding its variance (judging by the rsquare of &gt; 0.8) but has a pretty high average error of more than 4 while we are in a domain of data that only reaches from 1 to 27. Part 2.3: Multiple linear models . Lets look at another way to make our models a bit more flexible So far we created a linear model with only one parameter. Obviously that did not catch all of the variance in our data. In reality we often have more data at hand which can help us explain the measure of interest. For example to fill gaps in temperature data instead of only using the indices to predict, we could add variables such as the incoming radiation or the relative humidity of the air. Lets continue working with the dwd data we used before. Load it just like we did in the previous exercises . When we try to simply interpolate with the pointwise linear interpolation, you will see that we get a pretty uninformed output. We will now create a more sophisticated model to reconstruct our missing data. However, this time we have a whole dataset of predictors to choose from. Since we want to fill a gap in temperature data, we need to find predictors that are well correlated with temperature. To figure out which ones are suitable we can make use of the correlation matrix. A correlation matrix is a normalized form of a covariance matrix. The values vary between -1 and 1. A value of 1 signals a perfect positive, -1 a perfect negative correlation. 0 means that the two variables are not correlated at all. With pandas you can get the full correlation matrix with all variables with the .corr() function: . # lets look at the correlation matrix df_dwd.corr() # you can plot and explore it with plotly. # The interactivity is really handy here: px.imshow(df_dwd.corr()).show() # To get all correlations with tair_2m_mean we have to index it: df_dwd.corr()[\"tair_2m_mean\"] . Try to figure out, which variables could be suitable to fill the gaps in our data from the below table. Now we can go ahead and start building our multivariate model! Let go! Before we really start plugging the data into the model we need to do a bit of preparation: Since the model has to be fit with data where all the predictors we want are present AND we have observation data of our target variable to train the model on, we first need to find that data. We can do that easily by dropping the rows, where these columns are na with “dropna()”: . df_dwd_noNA = df_dwd.loc[:,[\"data_time\",\"SWIN\",\"rH\",\"tair_2m_mean\"]].dropna() . Now we want to split these into the data we use as predictors (y) and the data we want to predict (x, also called the “predictand”): . x = df_dwd_noNA.loc[:,[\"SWIN\",\"rH\"]] y = df_dwd_noNA.loc[:,[\"tair_2m_mean\"]] . Finally one last very important step is that we need to split our available data into two parts: a training and a testing dataset. The training data will ONLY be used for creating (or “fitting”) the model. To test the performance of the model, we keep a fraction of the available data out of the training set. That way we can predict the testing data and compare it to the real results. We are working with some artificiallly created gaps in the data here, but in real life you would otherwise have no way to test, how well your model actually predicts data. Additionally to splitting the data, the training datasets also get shuffled. That makes the model more robust in extrapolating it to unknown data. It is extremely important to do this split, because you can never test a model on data that it has already seen during its training phase. That would skew your results and make it look better than it actually is. Luckily, because this is such a common task to do, scikit learn has us covered with a very simple function to do the splitting: . from sklearn.model_selection import train_test_split # creating train and test sets X_train, X_test, y_train, y_test = train_test_split( x, y, test_size=0.3, random_state=42) . Now that we have our final training and testing datasets ready for use, we can go ahead and fit our model! . linearModel = LinearRegression() # Only training data used for fitting: linearModel.fit(X_train,y_train) # Only testing data used for the score: linearModel.score(X_test,y_test) # You can plot the prediction for the testing period # as a scatter plot to get an idea of the spread # of the errors. Put true values on one axis and predicted on the other: y_hat_ml = linearModel.predict(X_test).reshape(1,-1)[0] px.scatter(x=y_hat_ml,y=y_test[\"tair_2m_mean\"]).show() . As you can see the score is roughly 0.31. That is not exactly great but does indicate a weak correlation between predicted and true values. Exercise . Do a linear interpolation and 1-D linear model prediction for this same data. Do any of them perform equally good or better than the multiple regression? . Solution! #------- interpolation: # in order to interpolate value-by-value we need to # first sort the previously randomized data: y_train_sorted = y_train.sort_index() y_test_sorted = y_test.sort_index() interpolated_data = np.interp( y_test_sorted.index, y_train_sorted.index, y_train_sorted[\"tair_2m_mean\"]) regression_results(y_test_sorted, interpolated_data) #-------- 1-D linear model: y = df_dwd_noNA.loc[:,[\"tair_2m_mean\"]].values.reshape(-1,1) x = df_dwd_noNA.index.values.reshape(-1,1) from sklearn.model_selection import train_test_split # creating train and test sets X_train, X_test, y_train, y_test = train_test_split( x, y, test_size=0.3, random_state=101) linearModel = LinearRegression() linearModel.fit(X_train,y_train) y_hat_linear = linearModel.predict(X_test) regression_results(y_test, y_hat_linear) . 2.4: Machine Learning approaches (example Random Forests) . We already covered quite a lot of ground on how to deal with missing data by . | cleaning the raw data | gap fill with interpolation, 1D-linear modeling and multiple linear regression | . In this final part we will take a quick look at a more sophisticated type of model, the Random Forests algorithm. Random Forest is a so-called decision-tree algorithm and can be counted to the broad category of “machine-learning” methods. The latter however is reeeeally a broad category, as it basically just describes that the machine works through a minimization procedure on such a large amount of data, that humans could not handle it manually, thus the machine is “learning” the optimization of the model and can make predictions from it. Random Forests has proven to be quite effective in gap-filling applications in a variety of contexts and is available as part of the scikit-learn package. The purpose is for you to get an idea, how to implement such a sophisticated method and to hopefully get you excited about machine learning! We will not go into the details of the actual method. Lets dive right in and load the random forest regressor from scikit-learn. We use the regressor because we work with time-series data. Random Forest also has a classification model, which is used for categorical data (for example image-recognition, predicting an animal type from its traits etc…) . The great thing about scikit-learn is that most of the models work in the exact same way, no matter whether it is a simple linear model or a complex machine-learning approach. For example to run a model with simple default setting all we have to do is the following: . from sklearn.ensemble import RandomForestRegressor # I set n_estimators to 12 for a quick initial fit # we will go into the parameters a bit more later! df_dwd_noNA = df_dwd.loc[:,[\"data_time\",\"SWIN\",\"rH\",\"tair_2m_mean\"]].dropna() x = df_dwd_noNA.loc[:,[\"SWIN\",\"rH\"]] y = df_dwd_noNA.loc[:,[\"tair_2m_mean\"]] X_train, X_test, y_train, y_test = train_test_split( x, y, test_size=0.3, random_state=42) rf_model = RandomForestRegressor(random_state=42, n_estimators=12) rf_model.fit(X_train, y_train) rf_model.score(X_test, y_test) y_hat_rf = rf_model.predict(X_test) regression_results(y_test, y_hat_rf) . The predicting performance is still not that great. However, with a machine-learning approach we can feed some more data into the model and see, whether it improves the model. I will only give a very brief intro to random forests here, no need to memorize that. If you are interested, you can also look the below youtube video for a very good short video on the method. https://www.youtube.com/watch?v=v6VJ2RO66Ag . Very generally speaking you can say that this algorithm looks at your data and the predictors and it picks a few of the predictors, leaving others out. With this reduced set it trains a model. That means, it tries to find out under which circumstances in the predictors, the data has a certain value. In random forests, many of those models are trained and compared. Each with different predictors and trained on different amounts and points of training data. After building the model, you can use it to predict unknown values. Therefore, the predictor data for these unknown datapoints is fed into each of these models and the combined output from all of them is evaluated as the final decision. Lets try adding some more of our weather-data into the model and see whether it improves the performance: . # Lets add the other weather-data columns into the predictor data as well. # First we find the rows where all the predictors data and our observations # are present: df_dwd_noNA = df_dwd.loc[:,[\"data_time\", \"SWIN\",\"rH\", \"pressure_air\", \"wind_speed\", \"precipitation\", \"tair_2m_mean\"]].dropna() # Now we split them into the x-values (predictors) and the y-values # (predictand or target variable) x = df_dwd_noNA.loc[:,[\"SWIN\",\"rH\", \"pressure_air\", \"wind_speed\", \"precipitation\"]] y = df_dwd_noNA.loc[:,[\"tair_2m_mean\"]] # Now we go an split the data into training and testing data: X_train, X_test, y_train, y_test = train_test_split( x, y, test_size=0.3, random_state=101) # Finally we do the full pipeline of # creating the model, fitting it, and scoring: rf_model = RandomForestRegressor(random_state=42, n_estimators=12) rf_model.fit(X_train, y_train) rf_model.score(X_test, y_test) y_hat_rf = rf_model.predict(X_test) regression_results(y_test, y_hat_rf) # Aha, the model performs a bit better. Finally, we will do some tweaking on the random forest paramters. Parameters are options given to the model, that define how it is set up. Here for example n_estimators is one parameter we gave to the model so far. Maybe we can make the model perform even a bit better by increasing that value. In order to do so, we better use hourly data, because as we make the model bigger, the time it takes to fit the model gets substanitally larger. Lets first aggregate the data like before: . # mean for most data: df_dwd_hourly_noNA = df_dwd_noNA.loc[:,[\"SWIN\",\"rH\", \"pressure_air\", \"wind_speed\",\"tair_2m_mean\", \"data_time\"]].resample(rule=\"1h\", on=\"data_time\").mean().dropna() # sum for precipitation data: df_dwd_hourly_noNA[\"precipitation\"] = df_dwd_noNA.loc[:,[\"precipitation\", \"data_time\"]].resample(rule=\"1h\", on=\"data_time\").sum().dropna() . Now we can run a new model on the hourly data and e.g. set the n_estimators to 50. Play around with the parameter a bit and see how the performance changes: . x_hourly = df_dwd_hourly_noNA.loc[:,[\"SWIN\",\"rH\", \"pressure_air\", \"wind_speed\", \"precipitation\"]] y_hourly = df_dwd_hourly_noNA.loc[:,[\"tair_2m_mean\"]] X_train, X_test, y_train, y_test = train_test_split( x_hourly, y_hourly, test_size=0.3, random_state=101) rf_model = RandomForestRegressor(random_state=42, n_estimators=50) rf_model.fit(X_train, y_train) rf_model.score(X_test, y_test) y_hat_rf = rf_model.predict(X_test) regression_results(y_test, y_hat_rf) # as you can see, the model performs yet another bit better. Exercise . Practice makes perfect! For the hourly data, see how the linear interpolation, 1D-linear model and multiple linear models perform compared to the random forest regression. Solution! #------- preparation of data: # mean for most data aggregation: df_dwd_hourly_noNA = df_dwd_noNA.loc[:,[\"SWIN\",\"rH\", \"pressure_air\", \"wind_speed\",\"tair_2m_mean\", \"data_time\"]].resample(rule=\"1h\", on=\"data_time\").mean().dropna() # sum for precipitation aggregation: df_dwd_hourly_noNA[\"precipitation\"] = df_dwd_noNA.loc[:,[\"precipitation\", \"data_time\"]].resample(rule=\"1h\", on=\"data_time\").sum().dropna() x_hourly = df_dwd_hourly_noNA.loc[:,[\"SWIN\",\"rH\", \"pressure_air\", \"wind_speed\", \"precipitation\"]] y_hourly = df_dwd_hourly_noNA.loc[:,[\"tair_2m_mean\"]] X_train, X_test, y_train, y_test = train_test_split( x_hourly, y_hourly, test_size=0.3, random_state=101) print(\"------- linear intrpolation:\") y_train_sorted = y_train.sort_index() y_test_sorted = y_test.sort_index() interpolated_data = np.interp( y_test_sorted.index, y_train_sorted.index, y_train_sorted[\"tair_2m_mean\"]) regression_results(y_test_sorted, interpolated_data) print(\"------- multiple linear regression:\") linearModel = LinearRegression() linearModel.fit(X_train,y_train) y_hat = linearModel.predict(X_test) errors = (y_test - y_hat).iloc[:,0].values regression_results(y_test, y_hat) print(\"------- random forest:\") rf_model = RandomForestRegressor(random_state=42, n_estimators=50) rf_model.fit(X_train, y_train.values.ravel()) rf_model.score(X_test, y_test) y_hat_rf = rf_model.predict(X_test) regression_results(y_test, y_hat_rf) . For hourly data the linear interpolation still performs best. However, the gaps we are interpolating thus far are rather small. As a last exercise, we will see how the methods perform for longer gaps. Therefore I create a gap in the hourly dataset of a full day. We will then see how the different methods perform in filling the gap: . df_dwd_hourly_noNA = df_dwd_noNA.loc[:,[\"SWIN\",\"rH\", \"pressure_air\", \"wind_speed\",\"tair_2m_mean\", \"data_time\"]].resample(rule=\"1h\", on=\"data_time\").mean().dropna() df_dwd_hourly_noNA[\"precipitation\"] = df_dwd_noNA.loc[:,[\"precipitation\", \"data_time\"]].resample(rule=\"1h\", on=\"data_time\").sum().dropna() # first lets create the 14-day long gap: # I first extract the indices of some single day and safe them indices_for_gap = df_dwd_hourly_noNA.iloc[505:529, :].index # Now I make a copy of the original data to not mess it up gapped_data_hourly = df_dwd_hourly_noNA.copy() # Then I set all the values for tair in these indices to NaN gapped_data_hourly.loc[indices_for_gap, \"tair_2m_mean\"] = np.NaN # Finally I can extract the predictor and predictand columns with these indices: x_hourly = gapped_data_hourly.loc[indices_for_gap,[\"SWIN\",\"rH\", \"pressure_air\", \"wind_speed\", \"precipitation\"]] y_true = df_dwd_hourly_noNA.loc[indices_for_gap, \"tair_2m_mean\"] #---- interpolation interpolated_data = gapped_data_hourly[\"tair_2m_mean\"].interpolate() regression_results(y_true, interpolated_data[indices_for_gap]) #---- multiple linear regression: y_hat_linear = linearModel.predict(x_hourly) regression_results(y_true, y_hat_linear) #---- Random Forest: y_hat_rf = rf_model.predict(x_hourly) regression_results(y_true, y_hat_rf) . Exercise . Play around with the length of the gap and observe how the performance of the different methods changes. Try to give an explanation and maybe formulate, when something like linear interpolation could be suitable and when it is better to rely on a more complex method. Solution! The Random Forest method works quite nicely on longer prediction windows. As long as there is a clear linear trend in the data, a simple interpolation might perform very well. However, if within a data gap a shift happens and for example a warm period comes around, the linear interpolation will quickly get worse in its prediction. As long as there is a clear linear trend in the data, a simple interpolation might perform very well. However, if within a data gap a shift happens and for example a warm period comes around, the linear interpolation can not capture that. Keep in mind that a model is by definition NEVER the actual truth. Its goal is to come as close as possible to the truth while often times drastically reducing the complexity of the issue. In a real world scenario we would not know that our final modelled data is not the same as the true data. Therefore all we can do is create and test our models to the best of our knowledge and be honest about what they are capable of doing and what their shortcomings are! . ",
    "url": "/New_BAI_DataAnalysis/python_3_interpolation_gapfilling.html#2-gap-filling-interpolation-and-modelling",
    
    "relUrl": "/python_3_interpolation_gapfilling.html#2-gap-filling-interpolation-and-modelling"
  },"22": {
    "doc": "3. INTERPOLATION AND GAP FILLING",
    "title": "3. INTERPOLATION AND GAP FILLING",
    "content": " ",
    "url": "/New_BAI_DataAnalysis/python_3_interpolation_gapfilling.html",
    
    "relUrl": "/python_3_interpolation_gapfilling.html"
  },"23": {
    "doc": "4. EXTREME VALUE DETECTION",
    "title": "Extreme value detection",
    "content": "In this exercise we will look at extreme values in meteorological data. First you will learn about different ways to define what is an “extreme” value (extreme relative to what?). Afterwards we will work with an example dataset and code the methods in Python to get some hands on experience. Table of Contents . | Material | Background | Methods and Implementation 3.1. Peak Over Threshold (POT) 3.2. Block Maxima (BM) 3.3. Moving Average (MA) | . ",
    "url": "/New_BAI_DataAnalysis/python_4_extreme_detection.html#extreme-value-detection",
    
    "relUrl": "/python_4_extreme_detection.html#extreme-value-detection"
  },"24": {
    "doc": "4. EXTREME VALUE DETECTION",
    "title": "1. Material",
    "content": "We will once again use the DWD dataset from the Ahaus station for this section. Below you can find the download again: Ahaus DWD meteo data (25mb)…. Literatur . We will have to do some plotting again, so it might be good for you to resample the data to daily data, just to reduce the size of the dataset a bit. By now you should know how to do it. Try to create a pandas Series with hourly air temperature data. If you are getting stuck, you can refer to the exercise before. ",
    "url": "/New_BAI_DataAnalysis/python_4_extreme_detection.html#1-material",
    
    "relUrl": "/python_4_extreme_detection.html#1-material"
  },"25": {
    "doc": "4. EXTREME VALUE DETECTION",
    "title": "2. Background",
    "content": "We will look at three different methods to determine extreme events from time series of meteorological data. The main difference between the methods is the way they define the reference, to which we compare a value to describe it as being “extreme” or not. Pause for a second and think about how you could describe what an extreme value is. There are several ways to think about extreme values. An extreme value can simply be the highest/lowest value in a finite set of data. Think for example about testing the highest speed that a car reaches on a test drive. Here the absolute peak value would be a reasonable value of interest. In meteorological time series we are often interested in a range of extreme values. In other words, we are interested in the values in the tails of the distribution of our sample data, which exceed a certain threshold. It is important to consider the distribution of our underlying dataset and the question we actually want to answer. Our example dataset comprises of air temperature data from 1996 to 2023. If we are interested in the extreme values with respect to this whole time period, we can simply look at the distribution of all the data, determine a threshold and see which datapoints are above the upper or below the lower threshold. However, we might also be interested in the months with extreme temperatures. Because our distribution includes winter and summer data, extreme temperatures in spring and autumn will probably not be considered in this approach. For these we would have to create data distributions of seasonal, monthly or even daily data to evaluate extreme events on the respective time scale. This will become more obvious when we look at the methods. Read More: Extreme value return periods Another approach is the evaluation of extreme values and their probabilities based on historical data. Relating these probabilities to the time series of the data produces \"return periods\", frequencies in which the extreme values are expected to occur. As an example, requirements for buildings often include a resistance to weather extremes with a certain return period. Making up a case, wind turbines would be built that they can withstand windspeeds with a \"return level\" in a \"return period\" of 1 in 10.000, meaning the chance that such a windspeed occurs in a year would be 0.01%. ",
    "url": "/New_BAI_DataAnalysis/python_4_extreme_detection.html#2-background",
    
    "relUrl": "/python_4_extreme_detection.html#2-background"
  },"26": {
    "doc": "4. EXTREME VALUE DETECTION",
    "title": "3. Methods and Implementation",
    "content": "Lets now look at three different methods to analyze extreme events in our sample dataset. We will talk about the reasoning and the implementations of the methods. We will then go through each method and implement the methods into functions, which you can then use to analyze your data. As a little preface we need to talk about a concept we will use for all methods: Quantiles. What are quantiles? . Quantiles are points in a dataset that divide the data into equal-sized groups. They’re incredibly useful for understanding how values in a dataset are spread out and for comparing different datasets. E.g.we see a normal distribution curve divided into four equal areas. Each area represents 25% of the total data. The lines dividing these areas are our quartiles: . | Q1: The first quartile (25th percentile) | Q2: The second quartile (50th percentile, also known as the median) | Q3: The third quartile (75th percentile) | . You can use the following function to visualize the quantiles of our dataset: . def visualize_quantiles(x:pd.Series, q_low:float, q_high:float): import scipy.stats as stats x_mean = x.mean() x_sd = x.std() y = stats.norm.pdf(x.sort_values(), x_mean, x_sd) # This function creates the y-values of the normal distribution given our data, the mean and the standard deviation qh = x.quantile(q_high) # here we calculate the higher quantile threshold ql = x.quantile(q_low) # here we calculate the lower quantile threshold fig = px.scatter(x=x.sort_values(),y=y) fig.add_trace( go.Scatter( x=[ql, ql], y = [0,max(y)], mode=\"lines\", name=f\"{q_low*100}% quantile\" ) ) fig.add_trace( go.Scatter( x=[qh, qh], y = [0,max(y)], mode=\"lines\", name=f\"{q_high*100}% quantile\" ) ) fig.show() get_quantiles(df_dwd_ta_hourly, 0.05, 0.95) . Alright, now that we layed out the basics, lets dive into theme methods! . 3.1 Peak Over Threshold (POT) . The first approach is the Point Over Threshold (POT) method. This is a very simple approach that looks at the whole dataset as one. We define fixed thresholds for the dataset, defining the upper and lower bounds above or below which values will be considered extreme. The boundaries are usually defined by the quantiles we provided as an argument to the function. Exercise . Lets try and code that method ourselves. It is actually not very difficult! Define a new function called \"peak_over_threshold()\". It needs to take a series of data as input and the quantile we want to use for extreme detection. Then we need to do the following operations: . | Find the upper and lower thresholds for what is to be defined as extreme, absed on the quantiles. To find these values you can use the handy Python function \"quantiles()\". Just call it on the input Series and provide the quantiles as argument as in \"X.quantiles(0.95)\". Remember: You want the upper **and lower** thresholds. Think about how you can get both. | Find those rows in the input series which are higher and lower than the upper and lower thresholds. You can get Series of booleans by comparing a pandas Series with a value. You can try it out, just type for example \"X &gt; 270\" if X is your Series. | Finally you want to create a dataframe, because of course you want to return the results of your extreme value detection. Create a dataframe with the input data and two new columns, one containing the booleans of your high extreme values and the other for the low extremes. | . A little hint: The description here is quite long but the code for this is actually quite short. Hint if you get stuck! You can generate a Series of boolean values that indicate whether a datapoint is above or below a value with a direct comparison such as . X_larger_than_280 = X &gt; 280 . Solution! def peak_over_threshold(X:pd.Series, prob): print(f'Extremes detection using peak over threshold method at: {prob} percentile') df = pd.DataFrame(index=X.index, data = { \"data\": X, \"extreme_low\": X &lt; X.quantile(q=1-prob), \"extreme_high\": X &gt; X.quantile(q=prob) }) return df . For this and the next methods it will be very handy to have a function that plots the data and the extreme highs and lows in separate colors. You can try to build a nice plotly figure yourself or you use the code I provide below. Plot function def plot_extremes(data:pd.DataFrame, extr_high_col:str, extr_low_col:str): extr_high_data = data.loc[data[extr_high_col]==True, \"data\"] extr_low_data = data.loc[data[extr_low_col]==True, \"data\"] fig = go.Figure() fig.add_traces( go.Scatter( x=data.index, y=data[\"data\"], mode=\"markers\", name=\"no extreme\", marker_color=\"black\", marker_size=5, ) ), fig.add_traces( go.Scatter( x = extr_high_data.index, y = extr_high_data, name=\"extr. high\", mode=\"markers\", marker_color='orange', marker_size=5, showlegend=True ) ) fig.add_traces( go.Scatter( x = extr_low_data.index, y = extr_low_data, name=\"extr. low\", mode=\"markers\", marker_color='LightSkyBlue', marker_size=5, showlegend=True ) ) fig.update_layout( template=\"simple_white\" ) fig.show() . Take a look at the output and the datapoints marked as extreme values. Evaluate the plot yourself. What is the reference for these extreme values? Which questions could you answer with this type of extreme detection, which not? . Exercise . Lets fiddle with the code for a bit. Change the prob parameter to 85, 75 and see how the output changes. How many extreme values do you expect when setting prob to 50? Think about it and then run the function with that quantile. 3.2. Block Maxima Method (BM) . The next method we are looking at is the “Block Maxima” method. As the name states, we are looking at a certain “block” of data and find the maxima based on the defined threshold of the values in this block. There are several ways we could define these reference blocks. For example we could look at every year individually and find the extreme values for these. Alternatively, we could create blocks from each week of the year across all years or for every month across all years. We could then find extremes based on the quantiles of the data for every wekk of the year and separate e.g. extreme values in spring and autumn from the overshadowing extreme values in winter and summer. In our example we will define the blocks as the values for each single day across all the years. The procedure is as follows: . Step 1 . In the first line we again create a dataframe with the data, a date column and then add a new column called “DOY” with a mutation, that gives each date a value of 1 to 365. We need this “day” value to group our data across the years by it. We can get this by grabbing the “day_of_year” property from our datetime-indices in Pandas. df_bm = pd.DataFrame(index=X.index, data={ \"doy\":X.index.day_of_year, \"data\":X.values, }) . Step 2 . Next we create the column “data_14d_ma”. This is the 15 day moving average around every day. Moving average means that the “window” of data we are calculating the mean from varies. df_bm[\"data_14d_ma\"] = X.rolling(window=14, min_periods=1, center=True).mean() . Through this, we accquire a smoothing of the daily temperature values and make the underlying dataset for our daily temperature distribution more broad. The reasoning is the following: We want to create a representative dataset for daily temperature values across the years. If we use the single day for each year, we have a dataset of 18 datapoints which can easily include heavy outliers. By using a moving average of 15 days we enhance our dataset for each day by a factor of 15 to 270 datapoints, still restricted to a pretty small time window. While it does reduce the impact of individual extremely hot or cold days, it is more likely to representatively capture the state of the atmosphere around the time of interest. Step 3 . Now that we have the smoothed data and our doy information we can go ahead and calculate the long-term mean for every day of the year. To do so, we use the pandas “groupby” function. This allows us to sample data based on common values in a column. E.g. for the day of the year, it will grab all values where the day of the year is 1 and calculate the mean for those, then for day 2 and so on. long_term_means = df_bm.groupby(\"doy\")[\"data_14d_ma\"].mean() . Now that we have those long term means, we can calculate the difference between every datapoint and the long-term mean that fits to its day of the year. To make it more clear, we can use the pandas “iterrows” function that allows us to loop through the rows of the dataframe. First we create a new column filled with zeros called “diff”. Then we go through the rows of the dataframe, grab that long-term mean value by its index that corresponds to the “doy” of the current row (done with long_term_means.index == df_bm.loc[row,”doy”]). Then we subtract that long-term mean from that corresponding datapoint. df_bm[\"diff\"] = np.zeros(len(df_bm)) for row, index in df_bm.iterrows(): ltm = long_term_means[long_term_means.index == df_bm.loc[row,\"doy\"]] diff = df_bm.loc[row,\"data\"] - ltm df_bm.loc[row, \"diff\"] = diff.values . Step 4 . One thing is still missing: the threshold to define our datapoint as extreme! In this approach we define the thresholds for something to be extreme based on the “diff” column. We want to find those values, where the deviation from the long-term mean for that specific day of the year is larger than usual. Makes sense right? Again we can use the quantiles function to find the extremes of the differences: . upper_thresh = df_bm[\"diff\"].quantile(prob) lower_thresh = df_bm[\"diff\"].quantile(1-prob) df_bm[\"extreme_high\"] = df_bm[\"diff\"] &gt; upper_thresh df_bm[\"extreme_low\"] = df_bm[\"diff\"] &lt; lower_thresh . Note: In the POT approach the quantiles where built from the whole dataset itself. Here, the quantiles are built from the array of deviations from the mean! Remember this in the exercise when you evaluate the results. Exercise . | Go ahead and built a function for the block maxima method. You already got all the building blocks. Put them together and add the right function definition and return statement. | After using POT and the BM, which method do you expect to yield more extreme values per year? How do you think the extremes of the two methods are different from each other? | To compare the outcomes of the two functions you can plot the distributions of the extreme values together. In the “visualize_quantiles” method above you already have a function given that creates a distribution. Write a new function that builds distributions of the extreme values for the different methods and creates a plot. This can well be done by first creating an empty figure object and then looping through the different extremes-dataframes, calculating the distributions for each and adding a new trace. After the loop you can call the “fig.show()” to display the figure. A starter code is given below. | . Starter Code ex. 3 def plot_extremes_distribution(dfs:list[pd.DataFrame], extr_high_col:str, extr_low_col:str, methods:list[str]): print(\"----\") print(\"Printing extremes\") colors = [\"red\", \"blue\", \"green\", \"purple\", \"lightblue\", \"coral\"] fig = go.Figure() for i,df in enumerate(dfs): method = methods[i] color = colors[i] #... calculate distributions and add new traces to the figure # You can nicely visualize the different methods by giving them the # same color but maybe differentiate low and high # extremes by using dashed and solid lines # Use the \"method\" variable to give the traces # labels (with the \"name\" parameter to tell them apart # in the legend) . Solution Ex. 1 # the full code for the block maxima method def block_maxima(X:pd.Series, prob:float): df_bm = pd.DataFrame(index=X.index, data={ \"doy\":X.index.day_of_year, \"data\":X.values, \"data_14d_ma\": X.rolling(window=14, min_periods=1, center=True).mean() }) long_term_means = df_bm.groupby(\"doy\")[\"data_14d_ma\"].mean() df_bm[\"diff\"] = np.zeros(len(X)) for row, index in df_bm.iterrows(): ltm = long_term_means[long_term_means.index == df_bm.loc[row,\"doy\"]] diff = df_bm.loc[row,\"data\"] - ltm df_bm.loc[row, \"diff\"] = diff.values upper_thresh = df_bm[\"diff\"].quantile(prob) lower_thresh = df_bm[\"diff\"].quantile(1-prob) df_bm[\"extreme_high\"] = df_bm[\"diff\"] &gt; upper_thresh df_bm[\"extreme_low\"] = df_bm[\"diff\"] &lt; lower_thresh return df_bm . Solution Ex. 3 def plot_extremes_distribution(dfs:list[pd.DataFrame], extr_high_col:str, extr_low_col:str, methods:list[str]): print(\"----\") print(\"Plotting extreme distributions\") colors = [\"red\", \"blue\", \"green\", \"purple\", \"lightblue\", \"coral\"] fig = go.Figure() for i,df in enumerate(dfs): method = methods[i] color = colors[i] extr_highs = df.loc[df[extr_high_col] == True, \"data\"] extr_lows = df.loc[df[extr_low_col] == True, \"data\"] y_high = stats.norm.pdf(extr_highs.sort_values(), extr_highs.mean(), extr_highs.std()) # This function creates the y-values of the normal distribution given our data, the mean and the standard deviation y_low = stats.norm.pdf(extr_lows.sort_values(), extr_lows.mean(), extr_lows.std()) # This function creates the y-values of the normal distribution given our data, the mean and the standard deviation fig.add_traces( go.Scatter( x=extr_highs.sort_values(), y=y_high, name = f\"{method} extreme highs\", mode = \"lines\", line_color = color ) ) fig.add_traces( go.Scatter( x=extr_lows.sort_values(), y=y_low, name=f\" {method} extreme lows\", mode = \"lines\", line_color = color, line_dash = \"dash\" ) ) fig.update_layout(template=\"simple_white\") # &lt;- not neccessary, I just like it!&gt; fig.show() . 3.3. Moving Average Method (MA) . The final method we will look at is the moving average method. As the name already states, here the extremes are detected on a more temporally constrained basis, the moving average around each datapoint. Take a look at the code block for the block-maxima method. Everything we need for the moving average method is already in there. This time, try to write the method all by yourself. It is really not hard. You just need to figure out, which data you need to subtract to get the “diff” column right. As a little hint: You don’t need the day_of_year information here anymore at all. Exercises . | Think about how using a smaller time reference window might affect the extreme value detection. Would you expect extreme values in this approach to be more or less frequent than in the block averaging method? Then run the detection function and save the output in a new variable. | You have now run all three methods. How do you think does the distribution look for the moving average? Use your plotting function from before to check your hypothesis. | Change the parameter rollmean_period of the extreme detection function with the MA method to 365 and pass that output to the plot_extremes() function. How do you explain the output in comparison to the other methods? | . ",
    "url": "/New_BAI_DataAnalysis/python_4_extreme_detection.html#3-methods-and-implementation",
    
    "relUrl": "/python_4_extreme_detection.html#3-methods-and-implementation"
  },"27": {
    "doc": "4. EXTREME VALUE DETECTION",
    "title": "4. EXTREME VALUE DETECTION",
    "content": " ",
    "url": "/New_BAI_DataAnalysis/python_4_extreme_detection.html",
    
    "relUrl": "/python_4_extreme_detection.html"
  },"28": {
    "doc": "3. FLUX CALCULATION",
    "title": "Fluxes Calculation",
    "content": "In this tutorial, we’re going to analyze the data you collected on your field trip to the Lüner forest! Your instruments measured raw gas concentrations, but as ecologists, we need to turn that into gas fluxes. Why? Because fluxes represent a rate—the speed at which gases are being exchanged. With CO₂ fluxes, we can estimate crucial metrics like ecosystem respiration (RECO) and net ecosystem exchange (NEE). With fluxes of a potent greenhouse gas like Nitrous Oxide (N₂O), we can understand a key part of the nitrogen cycle. This guide will walk you through the entire process: from cleaning the raw concentration data, to calculating meaningful fluxes, and finally to comparing the results between different land cover types. Notice: . In the following sections, we will start using new functions and libraries that we haven’t introduced yet. Don’t worry or feel overwhelmed! This is a normal part of learning to code. For each new tool we use, I will: &gt;Briefly explain what it is and why we are using it. Provide a link to its official documentation if you’re curious and want to learn more. Think of it as adding new tools to your data analysis toolbox. We’ll introduce &gt;them one at a time, right when we need them. Table of Contents . | Read in and Merge Data Files | Loading and Exploring Raw Data | Filtering and Cleaning | Understanding the Data Pattern | Calculating Flux for a Single Plot | Automating Calculations for all plots | Comparing Results | . ",
    "url": "/New_BAI_DataAnalysis/python_4_flux_calculation.html#fluxes-calculation",
    
    "relUrl": "/python_4_flux_calculation.html#fluxes-calculation"
  },"29": {
    "doc": "3. FLUX CALCULATION",
    "title": "1.Read in and Merge Data Files",
    "content": "Different from the simple CSV files we might have worked with before, the raw data from the gas analyzer is more complex. When you open the file, you’ll see it contains two parts: A metadata header: This block at the top contains useful information about the measurement (like timezone, device model, etc.), but we don’t need it for our flux calculations. The data block: This is the core data we need, with columns for date, time, and gas concentrations. Our first challenge is to programmatically read only the data block and ignore the metadata. To do this, we’ll need the pandas library for creating our DataFrame and the io library, we need to import them. import pandas as pd import io . Our strategy will be to read the file line-by-line, find the start of the data, and then pass only those lines to pandas. 1.1 Reading and Parsing the File . First, we read the entire file into a single string, and then split that string into a list of individual lines. This gives us the flexibility to find our data “landmarks.” . # Read in raw data as a string with open(\"./BAI_StudyProject_LuentenerWald/raw_data/TG20-01072-2025-08-15T110000.data.txt\") as f: file_content = f.read() # Split the string into a list of lines. # '\\n' is the special character for a newline. lines = file_content.strip().split('\\n') . Next, we need to find the exact line that contains our column headers. Looking at the file, we know this line always starts with the word DATAH. We can write a short command to find the index of that line. # This code searches through our list 'lines' and gets the index of the first line that starts with 'DATAH' header_index = next(i for i, line in enumerate(lines) if line.startswith('DATAH')) # The actual data starts 2 lines after the header line (to skip the \"DATAU\" units line) data_start_index = header_index + 2 # Now we can grab the headers themselves from that line. The values are separated by tabs ('\\t'). headers = lines[header_index].split('\\t') . 1.2 Using io.StringIO to Read Our Cleaned Data . The pd.read_csv() function is built to read from a file. We don’t have a clean file; we have a list of Python strings (lines) that we’ve already processed. So, how do we make pandas read from our list? We use io.StringIO to trick pandas. It takes our cleaned-up data lines and presents them to pandas as if they were a file stored in the computer’s memory. Info: The Python io module helps us manage data streams. io.StringIO specifically allows us to treat a regular text string as a &gt;file. This is incredibly useful when you need to pass text data to a function that expects a file, just like we’re doing &gt;with pd.read_csv(). # Join our data lines back into a single string, separated by newlines data_string = '\\n'.join(lines[data_start_index:]) # Read the data string into a DataFrame df_raw = pd.read_csv( io.StringIO(data_string), # Treat our string as a file sep='\\t', # Tell pandas the data is separated by tabs header=None, # We are providing the headers ourselves, so there isn't one in the data names=headers, # Use the 'headers' list we extracted earlier na_values='nan' # Recognize 'nan' strings as missing values ) . 1.3 Data Formatting . The last step is to tidy up the DataFrame. We will: Remove the useless DATAH column. Combine the separate DATE and TIME columns into a single Timestamp object. This is crucial for time-series analysis. Set this new Timestamp as the DataFrame’s index, which makes plotting and selecting data by time much easier. # Drop the first column which is just the 'DATAH' label if 'DATAH' in df_raw.columns: df_raw = df_raw.drop(columns=['DATAH']) # Combine 'DATE' and 'TIME' into a proper Timestamp and set it as the index if 'DATE' in df_raw.columns and 'TIME' in df_raw.columns: df_raw['Timestamp'] = pd.to_datetime(df_raw['DATE'] + ' ' + df_raw['TIME']) df_raw = df_raw.drop(columns=['DATE', 'TIME']) df_raw = df_raw.set_index('Timestamp') print(\"Data loaded and formatted successfully!\") df_raw.head() . Great! Now, we have successfully read in and formatted our raw data. However, think about our field campaigns. We went out several times and generate a new data file for each trip. If we wanted to analyze all of them, we would have to copy and paste our loading code multiple times. To avoid repetition and make our code cleaner and more reliable, it’s a best practice to wrap a reusable process into a function. Let’s turn our loading and cleaning steps into a function called load_raw_data. Exercise . Try to write this function yourself based on the code snippets we created for data loading! Tip: The function will need to accept one argument: the filepath of the file you want to open. Solution! Note: how it’s the exact same logic as before, just defined within a def block. def load_raw_data(filepath: str) -&gt; pd.DataFrame: \"\"\" Loads raw data from a text file, remove metadata, and returns a DataFrame. Parameters: - filepath (str): The path to the input data file. Returns: - pd.DataFrame: A cleaned DataFrame with a DatetimeIndex. \"\"\" with open(filepath) as f: file_content = f.read() lines = file_content.strip().split('\\n') header_index = next(i for i, line in enumerate(lines) if line.startswith('DATAH')) data_start_index = header_index + 2 headers = lines[header_index].split('\\t') df_raw = pd.read_csv( io.StringIO('\\n'.join(lines[data_start_index:])), sep='\\t', header=None, names=headers, na_values='nan' ) if 'DATAH' in df_raw.columns: df_raw = df_raw.drop(columns=['DATAH']) if 'DATE' in df_raw.columns and 'TIME' in df_raw.columns: df_raw['Timestamp'] = pd.to_datetime(df_raw['DATE'] + ' ' + df_raw['TIME']) df_raw = df_raw.drop(columns=['DATE', 'TIME']) df_raw = df_raw.set_index('Timestamp') print(\"Raw data loaded and cleaned successfully.\") return df_raw . Now that we have our powerful load_raw_data function, we can easily handle data from multiple field trips. Instead of copying code, we can simply call our function in a loop. First, we create a list of all the file paths we want to load. Then, we can loop through this list, call our function for each path, and store the resulting DataFrames in a new list. # First, let's list all the files we want to load. # Make sure the file paths are complete and correct. base_path = \"./BAI_StudyProject_LuentenerWald/raw_data/\" file_names = [ 'TG20-01072-2025-08-15T110000.data.txt', 'TG20-01072-2025-08-16T110000.data.txt' # A hypothetical second file ] # Create the full file paths full_file_paths = [base_path + name for name in file_names] # Create an empty list to hold the loaded DataFrames raw_data_list = [] # Loop through each path, load the data, and append it to our list for path in full_file_paths: df = load_raw_data(path) raw_data_list.append(df) print(f\"\\nSuccessfully loaded {len(raw_data_list)} data files.\") . The loop above is clear and correct. However, a more concise way to write this in Python is with a list comprehension. It achieves the exact same result in a single, readable line: . raw_data_list = [load_raw_data(path) for path in full_file_paths] . For our flux calculations to be accurate, we need more than just gas concentrations. The Ideal Gas Law, which is the basis of the calculation, requires the ambient air temperature and air pressure at the time of each measurement. We will use the same workflow as before: load each file and then combine them. Exercise . You have two Excel files containing air temperature and two files for air pressure. Create lists of the file paths for the temperature and pressure data. Load each Excel file into a pandas DataFrame. Try using a list comprehension as we learned before! . Click here for the solution! # We assume the base path is the same as before base_path = \"./BAI_StudyProject_LuentenerWald/raw_data/\" # --- Load Air Temperature Data --- file_names_Ta = [ 'air_temperature_2025-08-15.xlsx', 'air_temperature_2025-08-16.xlsx' ] full_file_paths_Ta = [base_path + name for name in file_names_Ta] ta_data_list = [pd.read_excel(path) for path in full_file_paths_Ta] print(f\"Successfully loaded {len(ta_data_list)} air temperature files.\") # --- Load Air Pressure Data --- file_names_Pa = [ 'air_pressure_2025-08-15.xlsx', 'air_pressure_2025-08-16.xlsx' ] full_file_paths_Pa = [base_path + name for name in file_names_Pa] pa_data_list = [pd.read_excel(path) for path in full_file_paths_Pa] print(f\"Successfully loaded {len(pa_data_list)} air pressure files.\") . 1.4 Concatenating and Merging All Data . Now that we have all our data loaded, we need to combine it into one master DataFrame for analysis. This involves two steps: Concatenate: Stacking the files of the same type together (e.g., all gas files into one, all temperature files into one). Merge: Joining the different datasets (gas, temperature, and pressure) together based on their common timestamp. Concatenating the Datasets . First, let’s use pd.concat() to combine the lists of DataFrames we created. After combining, we must format the Timestamp column and set it as the index, just as we did before. # --- Concatenate and Clean Gas Data --- df_gas = pd.concat(raw_data_list) # Assumes raw_data_list is from the previous step # --- Concatenate and Clean Temperature Data --- df_Ta = pd.concat(ta_data_list) df_Ta['Timestamp'] = pd.to_datetime(df_Ta['Timestamp']) df_Ta = df_Ta.set_index('Timestamp') # --- Concatenate and Clean Pressure Data --- df_Pa = pd.concat(pa_data_list) df_Pa['Timestamp'] = pd.to_datetime(df_Pa['Timestamp']) df_Pa = df_Pa.set_index('Timestamp') print(\"--- Gas DataFrame Info ---\") df_gas.info() print(\"\\n--- Temperature DataFrame Info ---\") df_Ta.info() print(\"\\n--- Pressure DataFrame Info ---\") df_Pa.info() . Merging Gas and Auxiliary Data . Finally, we need to combine our df_gas, df_Ta, and df_Pa DataFrames. We want to add the temperature and pressure columns to the gas data, matching them by the nearest timestamp. The gas analyzer records data every second, while the weather station might only record every minute. A simple merge would leave many empty rows. The perfect tool for this is pd.merge_asof(). It performs a “nearest-neighbor” merge, which is ideal for combining time-series data with different frequencies. # First, merge the two auxiliary datasets together df_aux = pd.merge_asof(left=df_Ta, right=df_Pa, on='Timestamp', direction='nearest') # Now, merge the gas data with the combined auxiliary data. # We use direction='backward' to find the most recent weather data for each gas measurement. df_raw = pd.merge_asof( left=df_gas, right=df_aux, on='Timestamp', direction='backward' ) print(\"\\n--- Final Merged DataFrame ---\") display(df_raw.head()) df_raw.info() . Brilliant! You now have a single, clean DataFrame called df_final that contains everything you need: the high-frequency gas concentrations and the corresponding temperature and pressure for each measurement point. We are now fully prepared to move on to the flux calculation. ",
    "url": "/New_BAI_DataAnalysis/python_4_flux_calculation.html#1read-in-and-merge-data-files",
    
    "relUrl": "/python_4_flux_calculation.html#1read-in-and-merge-data-files"
  },"30": {
    "doc": "3. FLUX CALCULATION",
    "title": "2. Visualizing and Cleaning the Data",
    "content": "Now that we have a single, merged DataFrame, our next step is to inspect the data quality. Raw sensor data from the field is almost never perfect. Visualizing it is the best way to diagnose issues like noise, drift, or outliers before we attempt any calculations. For this, we’ll use plotly, a powerful library for creating interactive plots. 2.1 Creating a Reusable Plotting Function with Plotly . Just as we did with data loading, we’ll be plotting our time-series data multiple times. To make this efficient and keep our plots looking consistent, let’s create a dedicated function. This function will take a DataFrame and some plot details as input and generate an interactive plot. Exercise . The plooting function is partly providing in the following, now finish the function! . def plot_time_series(df, y_column, title, mode='lines'): \"\"\" Generates an interactive time-series plot using Plotly. Parameters: - df (pd.DataFrame): DataFrame with a DatetimeIndex. - y_column (str): The name of the column to plot on the y-axis. - title (str): The title for the plot. - mode (str): Plotly mode ('lines', 'markers', or 'lines+markers'). \"\"\" fig = go.Figure() fig.add_trace(...) # Update layout for a clean look fig.update_layout( ... ) fig.show() . Here is the solution! import plotly.graph_objects as go import plotly.io as pio # This setting forces Plotly to open plots in your default web browser, # which can be more stable in some environments. pio.renderers.default = \"browser\" def plot_time_series(df, y_column, title, mode='lines'): \"\"\" Generates an interactive time-series plot using Plotly. This function will automatically try to set a 'Timestamp' column as the index if the existing index is not a datetime type. Parameters: - df (pd.DataFrame): DataFrame to plot. - y_column (str): The name of the column to plot on the y-axis. - title (str): The title for the plot. - mode (str): Plotly mode ('lines', 'markers', or 'lines+markers'). \"\"\" # --- Input Validation and Auto-Correction --- # It's good practice to work on a copy inside a function to avoid # changing the user's original DataFrame unexpectedly. df_plot = df.copy() if not pd.api.types.is_datetime64_any_dtype(df_plot.index): print(\"Note: The DataFrame index is not a DatetimeIndex.\") # Attempt to fix the issue by finding a 'Timestamp' column if 'Timestamp' in df_plot.columns: print(\"--&gt; Found a 'Timestamp' column. Attempting to set it as the index.\") df_plot['Timestamp'] = pd.to_datetime(df_plot['Timestamp']) # CRITICAL: You must re-assign the variable to save the change. df_plot = df_plot.set_index('Timestamp') else: # If we can't fix it automatically, then we raise an error. raise TypeError( \"The DataFrame index is not a DatetimeIndex and a 'Timestamp' column was not found. \" \"Please set a DatetimeIndex before plotting.\" ) # --- Plotting --- # By this point, df_plot is guaranteed to have a valid DatetimeIndex. fig = go.Figure() fig.add_trace(go.Scatter( x=df_plot.index, y=df_plot[y_column], mode=mode, name=y_column )) # Update layout for a clean, professional look fig.update_layout( title=title, xaxis_title='Time', yaxis_title=f'{y_column} Concentration (ppb)', template='plotly_white', title_font=dict(size=24), xaxis=dict(tickfont=dict(size=14), title_font=dict(size=16)), yaxis=dict(tickfont=dict(size=14), title_font=dict(size=16)) ) fig.show() . 2.2 Visualizing the Raw Gas Data . Now, let’s use our new function to look at the raw N₂O data from our combined file. You can zoom and pan on the plot to inspect the noisy areas. # Call our function to plot the raw 'N2O' column plot_time_series(df_final, y_column='N2O', title='Raw N2O Concentration Over Time') . As you can see from the plot, the raw data is very noisy. There are several negative values and some extremely large spikes. These are physically impossible and are likely due to sensor errors or electrical interference. We cannot calculate meaningful fluxes from this data without cleaning it first. 2.3 Filtering with a Quantile Filter . To remove these outliers, we’ll use a simple but effective quantile filter. This method is robust because the extreme values we want to remove have very little influence on the calculation of percentiles. We will calculate the 10th and 90th percentiles of the N₂O concentration and discard any data points that fall outside this range. # Calculate the 10th and 90th percentiles p_10 = df_final.N2O.quantile(0.10) p_90 = df_final.N2O.quantile(0.90) print(f\"Filtering data to keep N2O concentrations between {p_10:.2f} and {p_90:.2f} ppb.\") # Apply the filter to create a new, clean DataFrame # .copy() is used here to avoid a SettingWithCopyWarning from pandas df_filtered = df_final[(df_final.N2O &gt;= p_10) &amp; (df_final.N2O &lt;= p_90)].copy() # Visualize the filtered data using our function again, this time using 'markers' plot_time_series(df_filtered, y_column='N2O', title='Filtered N2O Concentration Over Time', mode='markers') . This looks much better! The noise is gone, and a clear, meaningful pattern has emerged. 2.4 Understanding the Data Pattern . The filtered data shows a repeating pattern which is the signature of the static chamber method: Baseline (Ambient Air): The long, relatively flat periods show the baseline N₂O concentration in the ambient air. Concentration Increase (Chamber Closed): The sections where the concentration rises steadily and linearly are the actual measurements. This occurs when the chamber is placed over the soil, trapping the gases being emitted. The rate of this increase is what we will use to calculate the flux. Sudden Drop (Chamber Opened): The sharp vertical drops occur when a measurement is finished, and the chamber is lifted from the ground, exposing the sensor to ambient air again. Leveling Off: If a chamber is left on the ground for too long, the gas concentration inside can build up, altering the pressure gradient between the soil and the chamber air. This can cause the rate of increase to slow down and “level off.” For this reason, it’s crucial to use only the initial, linear part of the increase for our flux calculation. ",
    "url": "/New_BAI_DataAnalysis/python_4_flux_calculation.html#2-visualizing-and-cleaning-the-data",
    
    "relUrl": "/python_4_flux_calculation.html#2-visualizing-and-cleaning-the-data"
  },"31": {
    "doc": "3. FLUX CALCULATION",
    "title": "3. Calculating Flux for a Single Measurement",
    "content": "After loading and filtering our raw data and getting an overview of the patterns, it’s time to calculate the fluxes. Excited? In this section, we will focus on the data for a single measurement period to understand the process in detail. We’ll break it down into a few key steps: . | Review the flux calculation formula to see what components we need. | Define the metadata (chamber dimensions, etc.) for our specific plot. | Isolate the data for a specific time window and visualize it. | Perform a linear regression on the concentration data to get the rate of change. | Combine all the pieces to calculate the final flux. | . 3.1 The Flux Calculation Formula . First, let’s have a look on the fomula of flux calculation. ​ \\(\\text{Flux Rate (molar)} = \\frac{\\frac{\\Delta C}{t} \\cdot V \\cdot p}{R \\cdot (T_{c} + 273.15) \\cdot A}\\) ​ . Where: . ΔC/t: The rate of change of the gas concentration in ppm/s (this will be the slope from our regression). V: The total volume of the chamber headspace (m³). p: The air pressure in Pascals (Pa) during measurement. R: The ideal gas constant (8.314 J K⁻¹ mol⁻¹). T_c: The air temperature in Celsius (°C). A: The surface area covered by the chamber (m²). To understand this fomula, we need to figure out the meaning of ‘flux’. In the context of climate change, greenhouse gas flux specifically refers to the exchange of greenhouse gases (GHGs) like carbon dioxide (CO₂), methane (CH₄), and nitrous oxide (N₂O) between different parts of the Earth system (https://climate.sustainability-directory.com/term/greenhouse-gas-fluxes/#:~:text=In%20the%20context%20of%20climate,parts%20of%20the%20Earth%20system). Under the context of this analysis, ‘flux’ means gases exchange between soil and our measurement chamber. You might ask, “Doesn’t the rate of concentration change, ΔC/t (in ppb/s), already represent this flux?” Actually, ΔC/t is the raw evidence of a flux, but it is not a standardized, comparable measurement. It only describes what’s happening inside our specific chamber, under the specific conditions of that one measurement. We are not able to compare flux by simply comparing change rate of gas concentration. Under different temperature and pressure, gas molar density vary, the amount of gas molecular can be different even the gas volume is the same. Therefore, we need to utilize Gas Law (PV = nRT) to calculate the amount of molecular. Besides, a chamber covering a large area of soil will naturally capture more gas than one covering a small area. To make the measurement independent of our chamber’s specifications, we must divide by the soil Area (A) it covers. By applying the full formula, We convert our raw observation (ΔC/t) into a robust, standardized unit: micromoles per square meter per second (µmol m⁻² s⁻¹). To better understand the above fomula, it can be arranged into the following: . \\[\\text{Flux Rate (molar)} = ( \\frac{\\Delta C}{t} \\right) \\cdot ( \\frac{p \\cdot V} {R \\cdot (T_C + 273.15)} \\right) \\cdot ( \\frac{1}{A} \\right)\\] Now, it is clear that the fomula only contains three components: Flux = slope * gas_mole * A⁻¹ . Okay, lets create a function of flux calculation based on the fomula for later use. Exercise . The function calculate_flux is provided below but is not complete. It is your task to finish the function based on the formula. # Define key physical constants R = 8.314 # Ideal gas constant (J K⁻¹ mol⁻¹) def calculate_flux(slope_ppb_s, temp_k, pressure_pa, volume, area): \"\"\" Calculates N2O flux. Parameters: - slope_ppb_s (float): Rate of change in ppb/s. - temperature (float): Temperature, assumed to be in Celsius or Kelvin. - pressure (float): Pressure, assumed to be in Pascals (Pa) or hectopascals (hPa). - volume (float): Chamber volume, assumed to be in cubic meters (m³) or Liters (L). - area (float): Chamber area, assumed to be in square meters (m²) or square cm (cm²). \"\"\" # Convert slope from ppb/s to ppm/s for the formula ppm_per_second = ... # Calculate molar density of air (n/V = P/RT) in mol/m³ gas_model = ... # Calculate the flux in µmol m⁻² s⁻¹ # The 1e6 converts from mol to µmol flux = ... return flux . Solution! Here is the completed function: . # Define key physical constants R = 8.314 # Ideal gas constant (J K⁻¹ mol⁻¹) def calculate_flux(slope_ppb_s, temp_k, pressure_pa, volume, area): \"\"\" Calculates N2O flux. Parameters: - slope_ppb_s (float): Rate of change in ppb/s. - temperature (float): Temperature, assumed to be in Celsius or Kelvin. - pressure (float): Pressure, assumed to be in Pascals (Pa) or hectopascals (hPa). - volume (float): Chamber volume, assumed to be in cubic meters (m³) or Liters (L). - area (float): Chamber area, assumed to be in square meters (m²) or square cm (cm²). \"\"\" # Convert slope from ppb/s to ppm/s for the formula ppm_per_second = slope_ppb_s / 1000.0 # Calculate molar density of air (n/V = P/RT) in mol/m³ gas_mole = (pressure_pa * volume)/ (R * temp_k) # Calculate the flux in µmol m⁻² s⁻¹ # The 1e6 converts from mol to µmol flux = ppm_per_second * gas_mole / area * 1e6 return flux . 3.2 Isolating and Visualizing the Measurement Data . Let’s use an example time period of measurement: 2025-08-15 12:04:00 to 2025-08-15 12:10:00. We’ll slice our df_filtered DataFrame to get only the data within this window and then plot it to get a closer look. # Define the start and end times for our measurement window start_time = '2025-08-15 12:04:00' end_time = '2025-08-15 12:09:30' # Select the data for this specific time window measurement_data = df_filtered[(df_filtered.index &gt;= start_time) &amp; (df_filtered.index &lt; end_time)] # Use our plotting function to visualize this specific period plot_time_series( measurement_data, y_column='N2O', title=f'N2O Concentration for Plot {plot_metadata[\"plot_id\"]}', mode='markers' ) . As you can see from the plot, the data in our 5-minute window 2025-08-15 12:04:00 - 2025-08-15 12:09:30 contains more than just the measurement itself. We can identify three distinct phases: . Pre-measurement Baseline: A flat period at the beginning. This is when the sensor was measuring ambient air before the chamber was placed on the collar. The Measurement (Linear Increase): This is the part we want. The chamber is sealed, and N₂O from the soil is accumulating, causing a steady, linear increase in concentration. Post-measurement Drop: The sharp, sudden drop at the end. This occurred when the chamber was lifted, and the sensor was exposed to ambient air again. Our flux calculation relies on the slope (ΔC/t) from a linear regression. If we include the flat baseline or the sharp drop in our regression, the line of best fit will not represent the true rate of accumulation, leading to a highly inaccurate flux calculation. Therefore, To get an accurate flux, visual inspection is necessary to include only the linear increase phase. Zoom in on the interactive Plotly graph. We can see that the clean, linear increase happens approximately between 12:05:30 and 12:09:00. Exercise . Try to slice the dataframe based on your refined time window, and plot it to see our refined result. Solution! Here is the completed function: . # Define the refined, visually inspected time window start_mea = '2025-08-15 12:05:30' end_mea = '2025-08-15 12:09:00' # Create a new DataFrame with data only from this refined window # We use .copy() to create a completely new object for the regression measurement_data = df_filtered[df_filtered.index &gt; start_mea &amp; df_filtered.index &lt; end_mea].copy() # Visualize the refined data to confirm our selection plot_time_series( regression_data, y_column='N2O', title=f'Refined Regression Window for Plot {plot_metadata[\"plot_id\"]}', mode='markers' ) . Great! This plot shows the clear, linear increase in N₂O concentration after the chamber was placed on the collar. This is the exact data we need for our regression. 3.3 Linear Regression to derive the rate of gas concentration change . Now, as we talked before, we will fit a linear line to these data points. The slope of that line is the dC/dt (rate of change) that we need for our flux formula. As we expect the unit of our regression slope to be ppb/s our x-axis needs to be seconds elapsed (it means the seconds passed compared to the start of the measurement) instead of a timestamp. So, our first step is to create a new column, elapsed_seconds. from scipy import stats measurement_data = measurement_data.copy() # Create an 'elapsed_seconds' column for the regression # First, we get the start time of the measurement start_timestamp = measurement_data.index.min() # Then we get the time difference for each time point and the start of measurement, and use function total_seconds convert this time difference into seconds measurement_data['elapsed_seconds'] = (measurement_data.index - start_timestamp).total_seconds() . Then, we are going to actually fit the regression using scipy library. R2 represents the strength of the relationship that we detected. In here, we are going to use r2 = 0.7 as a threshold. If R2 of a regression is lower than 0.7, the change of gas concentration as time is not significant enough to be recognize as a flux (no flux is detected from the data), otherwise a gas flux can be deceted from the data. # Perform the linear regression using SciPy slope, intercept, r_value, p_value, std_err = stats.linregress( x=measurement_data['elapsed_seconds'], y=measurement_data['N2O'] ) # The R-squared value tells us how well the line fits the data (a value &gt; 0.7 is good!) r_squared = r_value**2 print(f\"--- Regression Results ---\") print(f\"Slope (dC/dt): {slope:.4f} ppb/s\") print(f\"R-squared: {r_squared:.4f}\") . 3.4 Visualizing the Fit and Final Calculation . It’s always good practice to visualize the regression line against the data to confirm the fit is good. # --- Visualize the regression line --- fig = go.Figure() # Add the raw data points fig.add_trace(go.Scatter(x=measurement_data['elapsed_seconds'], y=measurement_data['N2O'], mode='markers', name='Raw Data')) # Add the fitted regression line fig.add_trace(go.Scatter(x=measurement_data['elapsed_seconds'], y=intercept + slope * measurement_data['elapsed_seconds'], mode='lines', name='Fitted Line', line=dict(color='red'))) fig.update_layout(title=f'Linear Regression for Plot {plot_metadata[\"plot_id\"]} (R²={r_squared:.2f})', xaxis_title='Elapsed Time (s)', yaxis_title='N2O Concentration (ppb)', template='plotly_white') fig.show() . Good! If the regression is well fitted into our data, we are able to calculate the flux now! Before we call the calculate_flux function, there are still some steps to go. First, we need to get the average chamber air temperature and air pressure during the measurement and convert them into desired unit respectively (K for air temperature and Pa for air pressure). The unit conversion is very important, as when we wrote the calculate_flux function, we assumed units of our inputs. The mismatch of units will introduce systematic errors and leading to inaccuracy. # try to get the average air temperature and air temperature value, don't forget the unit conversion. avg_temp_c = avg_pressure_pa = . Solution! # Get the average temperature and pressure during the measurement avg_temp_c = measurement_data['T_air'].mean() + 273.15 # convert from °C to K avg_pressure_pa = measurement_data['P_air'].mean() * 100 # Assuming pressure is in hPa, convert to Pa . Then, we still need the total volume of the chamber headspace (m³) and the surface area covered by the chamber (m²). As they are independent of time and the same for all plots, we can simply define them as constants. # --- Finally, Calculate the Flux! --- VOLUME = 0.126 # AREA = 0.13 # the radias of the collar ring is 0.2m, so the area is 0.2*0.2*PI . Finally, call our calculate_flux function and we can get the result! . # Now try to call the function using all the inputs we have and print out to check the result. flux_N2O = ... print(...) . Solution! # Now we have all the pieces! Let's call our function. flux_N2O = calculate_flux( slope_ppb_s=slope, temp_k=avg_temp_k, pressure_pa=avg_pressure_pa, volume=VOLUME, area=AREA ) print(f\"\\n--- Final Flux Calculation ---\") print(f\"Average Temperature: {avg_temp_c:.2f} °C\") print(f\"Average Pressure: {avg_pressure_pa:.2f} Pa\") print(f\"Calculated N₂O Flux: {flux_N2O:.5f} µmol m⁻² s⁻¹\") . Brilliant! Now you successfuly turn the raw gas concentration data into gas fulx! . Challenge . Our current calculate_flux function works well, but it has a hidden weakness. It assumes the units of the inputs are correct. For example, it blindly assumes the temp_k argument is already in Kelvin. What if a user accidentally passes in a temperature in Celsius? The function would run without an error but produce a wildly incorrect result. Code that relies on such hidden assumptions is sometimes called “hard-coded.” A much better practice is to write more flexible code that can handle different situations or at least warn the user when something is wrong. Task: upgrade the calculate_flux function to be more robust. It should: . | Add Unit Checks: Check the input values to make a reasonable guess about their units. | Perform Automatic Unit Conversion: If it detects a value in a common but incorrect unit (like Celsius for temperature), it should automatically convert it to the required unit (Kelvin). | Raise Errors: If a value is completely outside a plausible range, it should stop and raise an error with a helpful message. | . Tip: You can determine units by checking the physical range of a variable. For example, for a terrestrial field measurement, if a temperature value is between -50 and 50, it’s almost certainly Celsius. If it’s between 223 and 323, it’s likely already in Kelvin. Solution! # Define key physical constants R = 8.314 # Ideal gas constant (J K⁻¹ mol⁻¹) def calculate_flux(slope_ppb_s, temperature, pressure, volume, area): \"\"\" Calculates N2O flux with extensive unit checks and auto-conversion for all inputs. Parameters: - slope_ppb_s (float): Rate of change in ppb/s. - temperature (float): Temperature, assumed to be in Celsius or Kelvin. - pressure (float): Pressure, assumed to be in Pascals (Pa) or hectopascals (hPa). - volume (float): Chamber volume, assumed to be in cubic meters (m³) or Liters (L). - area (float): Chamber area, assumed to be in square meters (m²) or square cm (cm²). \"\"\" # --- 1. Input Validation and Unit Conversion --- # Check Temperature (Celsius vs. Kelvin) if -50 &lt;= temperature &lt;= 50: print(f\"Note: Temperature ({temperature}) detected as Celsius. Converting to Kelvin.\") temp_k = temperature + 273.15 elif 223 &lt;= temperature &lt;= 323: temp_k = temperature else: raise ValueError(f\"Temperature value ({temperature}) is outside a plausible range.\") # Check Pressure (Pascals vs. hPa) if 800 &lt;= pressure &lt;= 1100: print(f\"Note: Pressure ({pressure}) detected as hPa/mbar. Converting to Pascals.\") pressure_pa = pressure * 100 elif 80000 &lt;= pressure &lt;= 110000: pressure_pa = pressure else: raise ValueError(f\"Pressure value ({pressure}) is outside a plausible range.\") # Check Volume (m³ vs. Liters) if 10000 &lt;= volume &lt;= 2000000 print(f\"Note: Volume ({volume}) detected as cm³. Converting to m³.\") volume_m3 = volume / 1e6 elif 10 &lt;= volume &lt;= 2000: # Plausible range for Liters print(f\"Note: Volume ({volume}) detected as Liters. Converting to m³.\") volume_m3 = volume / 1000.0 elif 0.01 &lt;= volume &lt;= 2: # Plausible range for m³ volume_m3 = volume else: raise ValueError(f\"Volume value ({volume}) is outside a plausible range for m³ or Liters.\") # Check Area (m² vs. cm²) if 100 &lt;= area &lt;= 20000: # Plausible range for cm² print(f\"Note: Area ({area}) detected as cm². Converting to m².\") area_m2 = area / 10000.0 elif 1 &lt;= area &lt;= 200: # Plausible range for dm² area_m2 = area / 100.0 print(f\"Note: Area ({area}) detected as dm². Converting to m².\") elif 0.01 &lt;= area &lt;= 2: # Plausible range for m² area_m2 = area else: raise ValueError(f\"Area value ({area}) is outside a plausible range for m², dm² or cm².\") # --- 2. Core Calculation --- v_over_a = volume_m3 / area_m2 ppm_per_second = slope_ppb_s / 1000.0 molar_density = pressure_pa / (R * temp_k) flux = ppm_per_second * molar_density * v_over_a * 1e6 return flux . Info: Python (raise keywords) is used to raise exceptions or errors. The raise keyword raises an error and stops the control flow of the program. It is used to bring up the current exception in an exception handler (an exception handler indicates the error type) so that it can be handled further up the call stack. The basic way to raise an exception is . raise Exception ('...') # In here, Exception is an exception handler (it is actually a function), which indicate a general exception. It takes a string used to reminder # users what errors happen in here and the potential reasons. In the ‘solution’, we used the handler ValueError to indicate the input value is outside a plausible range, and pass a string showing to users to further explain the error. ",
    "url": "/New_BAI_DataAnalysis/python_4_flux_calculation.html#3-calculating-flux-for-a-single-measurement",
    
    "relUrl": "/python_4_flux_calculation.html#3-calculating-flux-for-a-single-measurement"
  },"32": {
    "doc": "3. FLUX CALCULATION",
    "title": "4. Automating gas flux calculation",
    "content": "4.1 Store and structure measurement info . The first and crucial step of automation is to store the key information (metadata) for each measurement in a structured way that a program can loop through. For this, we will use a Python dictionary. The dictionary keys will be our data “columns” (e.g., ‘plot_id’, ‘land_use’), and the values will be lists containing the data for each plot. Now, there is an issue: we take multiple measurements at the same plot, perhaps on different days or at different times. How can we store this information efficiently? We can store the multiple start and end times for a single plot as a single string, with each timestamp separated by a semicolon (;). Of course, the order of the multiple starttime and endtime for a plot should match. By doing this, we can keep the metadata table concise and still tell our program to perform multiple calculations for that plot. # --- Create Metadata --- measurement_info = { 'plot_id': [1, 2], 'land_use': ['forest', 'forest'], 'start_time': ['2025-08-15 12:06:00; 2025-08-15 12:14:00', '2025-08-15 12:13:00'], 'end_time': ['2025-08-15 12:09:00; 2025-08-15 12:17:00', '2025-08-15 12:18:30'], } metadata_df = pd.DataFrame(measurement_info) . 4.2 Automation Calculation . Now we can build a for loop that iterates through each row (Each row contains infomation for a single plot) of our metadata_df. In here, we are going to use ‘iterrow()’ to iterate through metadata_df. ‘iterrow()’ is a method of data frame object, it generates an iterator object of the DataFrame, allowing us to iterate each row in the DataFrame. Each iteration produces an index object and a row object (a Pandas Series object). Inside the loop, we will split start_time and end_time string for each plot using ‘split()’ method and build a inner loop to iterate all measurements for the plot. results = [] # Create a empty list we can use to store all calculated fluxes. for index, row in metadata_df.iterrows(): start_times = row['start_time'].strip().split(';') # Handle potential multiple times end_times = row['end_time'].strip().split(';') # Handle potential multiple times for start_time, end_time in zip(start_times, end_times): start_time = pd.to_datetime(start_time.strip()) end_time = pd.to_datetime(end_time.strip()) measurement_date = f'{start_time.year}-{start_time.month:02d}-{start_time.day:02d}' . Within the inner loop, we will perform the exact same steps we did manually in the last section. However, there is one key difference in the visual inspection step. In the manual section, we looked at the plot and then assigned our refined start and end times into variables. To keep the program continuing without needing to stop and edit the script each time, we will use the built-in input() function. This will pause the script, show us a plot, and allow us to enter our refined time window directly into the terminal before the program continues. ## step 1: Visual inspection ## # Select the data for this specific time window measurement_data = df_filtered[(df_filtered.index &gt;= start_time) &amp; (df_filtered.index &lt; end_time)] # Plot the raw data for visual inspection plot_time_series(measurement_data, y_column='N2O', title=f'N2O Concentration Over Time}', mode='markers') # Mannually selcect the start and end time for regression start_mea = input(\"Enter the start time for regression (YYYY-MM-DD HH:MM:SS): \").strip() end_mea = input(\"Enter the end time for regression (YYYY-MM-DD HH:MM:SS): \").strip() # Use the original start and end time if no input is given if not start_mea: start_mea = start_time if not end_mea: end_mea = end_time start_time = pd.to_datetime(start_mea) end_time = pd.to_datetime(end_mea) measurement_data = measurement_data[(measurement_data.index &gt;= start_time) &amp; (measurement_data.index &lt;= end_time)] ## step 2: Linear regression ## # Ensure there is enough data to perform a regression if len(measurement_data) &lt; 10: print(f\"Skipping plot {row['plot_id']} on {measurement_date} due to insufficient data.\") continue # Create an 'elapsed_seconds' column for the regression measurement_data['elapsed_seconds'] = (measurement_data.index - start_time).total_seconds() # Perform linear regression: N2O concentration vs. time slope, intercept, r_value, p_value = stats.linregress( x=measurement_data['elapsed_seconds'], y=measurement_data['N2O'] ) # --- Quality Control (QC) --- # We only accept measurements with a good linear fit and a positive slope r_squared = r_value**2 # plot the regression line fig, ax = plt.subplots(layout='constrained', figsize=(10, 5)) ax.scatter(measurement_data['elapsed_seconds'], measurement_data['N2O'], label='N2O Concentration (ppb)') ax.plot(measurement_data['elapsed_seconds'], intercept + slope * measurement_data['elapsed_seconds'], 'r', label='Fitted line') ax.set_xlabel('Elapsed Time (s)') ax.set_ylabel('N2O Concentration (ppb)') ax.set_title(f'Linear Regression for Plot {row[\"plot_id\"]} (R²={r_squared:.2f})') plt.legend() plt.show() if r_squared &lt; 0.70 or p_value &gt; 0.05: flux_umol_m2_s = 0 # Set flux to 0 if QC fails qc_pass = False else: qc_pass = True ## step 3: Flux Calculation Formula ## # This formula converts the rate of change in concentration (slope) to a flux rate. # It corrects for ambient pressure and temperature. temp_k = measurement_data['temperature'].mean() + 273.15 # Convert °C to Kelvin pressure_pa = measurement_data['pressure'].mean() * 100 # Convert hPa to Pascals flux_umol_m2_s = calculate_flux(slope, temp_k, pressure_pa, VOLUME, AREA) . At the end of the iteration, we need to save the results of each calculation. Only the flux value is not enough, we also need to save its metadata (e.g., plot_id, ‘land_use’), which are essential for flux analysis and visualization we are going to do later. # Store the results results.append({ 'plot_id': row['plot_id'], 'land_use': row['land_use'], 'measurement_date': measurement_date, 'slope_ppb_s': slope, 'r_squared': r_squared, 'p_value': p_value, 'qc_pass': qc_pass, 'N2O_flux_umol_m2_s': flux_umol_m2_s }) # Convert the results list to a final DataFrame flux_results_df = pd.DataFrame(results) print(\"\\nFlux calculation complete:\") print(flux_results_df) . 4.3 Flux comparison . # --- Visualization --- plt.figure(figsize=(10, 7)) sns.boxplot(data=flux_results_df, x='land_use', y='N2O_flux_umol_m2_s', palette='viridis') sns.stripplot(data=flux_results_df, x='land_use', y='N2O_flux_umol_m2_s', color='black', size=8, jitter=True, alpha=0.7) plt.title('N₂O Flux by Land Use Type', fontsize=16) plt.xlabel('Land Use', fontsize=12) plt.ylabel('N₂O Flux (µmol m⁻² s⁻¹)', fontsize=12) plt.grid(axis='y', linestyle='--', alpha=0.7) plt.show() . ",
    "url": "/New_BAI_DataAnalysis/python_4_flux_calculation.html#4-automating-gas-flux-calculation",
    
    "relUrl": "/python_4_flux_calculation.html#4-automating-gas-flux-calculation"
  },"33": {
    "doc": "3. FLUX CALCULATION",
    "title": "3. FLUX CALCULATION",
    "content": " ",
    "url": "/New_BAI_DataAnalysis/python_4_flux_calculation.html",
    
    "relUrl": "/python_4_flux_calculation.html"
  }
}
