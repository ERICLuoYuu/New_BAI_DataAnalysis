{"0": {
    "doc": "Home",
    "title": "Why You Should Learn Python?",
    "content": "Generally speaking Python is a general-purpose, high-level interpreted programming language. What does that mean? . It means that it can be used for almost any kind of application you can achieve with any programming language. The “high-level” refers so to speak to the “distance” from the hardware in the way you use the language. The most extreme other side of the spectrum would be Assembly, where you directly control the processor and memory of the PC. Python lets you speak in simple terms and the computer understands what you want. In contrast to many other languages such as Java or C you don’t even have to compile your code. Compiling is a process in which your written code gets translated into something the machine can understand. In Java for example your workflow is always write code -&gt; compile program -&gt; run program. Python is interpreted which means that whenever you execute your code, it gets internally compiled and directly executed. That takes some responsibility of our shoulders. Not to forget: Python is one of the most widely used programming languages of all. The below graphic is from the 2023 Stackoverflow software developer survey. Pythons ranking is especially impressive as the other top languages are HTML, CSS and Javascript which power the majority of the modern internet while not being very widely used in other contexts. The data science platform Kaggle conducted a similar study in 2022 but specifically for data science and machine learning engineers . In the plot below you can clearly see that Python is the most widely used technology, way before R (R even decreasing in use) and SQL. So with all this said, get ready to join us in Python heaven! (credits: https://xkcd.com/353/) . ",
    "url": "/New_BAI_DataAnalysis/#why-you-should-learn-python",
    
    "relUrl": "/#why-you-should-learn-python"
  },"1": {
    "doc": "Home",
    "title": "Home",
    "content": " ",
    "url": "/New_BAI_DataAnalysis/",
    
    "relUrl": "/"
  },"2": {
    "doc": "0. INSTALLATION",
    "title": "Installing Python",
    "content": "For this course we will use a very handy tool called Anaconda. It is basically Python in a box, meaning that it creates a closed environment on your PC which already has Python and a lot of extra packages as well as additional software such as a code editor installed on it. This makes the installation as easy as it gets. The downside is that the installer is quite large (&gt;800mb). To get started, just go to Link: the Anaconda website… and download the version for your operating system. Simply follow the download instructions and leave all the buttons as they are by default. Once the installation is finished, search for the program “Anaconda Navigator” and open it. Once it opens you are presented with the main window of your Anaconda environment . In the “Home” screen you see a bunch of different programs that can run within Anaconda. The one we will use most is “Spyder” (definitely usable by people with Arachnophobia!). Make sure that Spyder is installed, if it is not click on the button to install it. Spyder is a code editor for Python which has some handy extensions, such as line-by-line execution and nicely viewable variables and tables during execution of code. But we will come back to that later… . Additionally there is a window called “Environnments”. Click on it and you will be presented with a table of two columns. The left side shows you environments (red box). Anaconda lets you create multiple environments (again, basically separete Python installations which are secluded from each other). This can be useful if e.g. you need functionality of very specific versions of packages for some program, but the same package in a different version in another program. To avoid the different versions clashing you can put them in different environments. On the right side (blue box) you can see the packages which are installed in an environment. Python has some internal core functionalities, but there are many many (many many many…) additional packages created by the community which unlock Pythons full potential. Some of the most widely used pacakges are e.g. numpy, pandas or scikit-learn. Notice: Try searching for them in the environment using the search-box on the top right and see whether they are already installed! . If a package is missing and you want to install it, select the dropdown menu on the top that by default says “installed” and swithc to “Not installed”. Then use the search box again to find the package you want to install. Notice: Try installing the package “xgboost” in this way. Once you have finished all of this you should be good to go! To verify that everything works go to the “Home” tab and start Spyder. Time for your first line of code! In the open Editor you will have to save the file in order to run it. Then copy and paste the following code by clicking the button on the top right in the box below. Insert it into spyder. Crypticlist = ['⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀', ' ⣀⣤⣤⠤⠐⠂⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡌⡦⠊⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡀⣼⡊⢀⠔⠀⠀⣄⠤⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣤⣤⣄⣀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣶⠃⠉⠡⡠⠤⠊⠀⠠⣀⣀⡠⠔⠒⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⣾⣿⢟⠿⠛⠛⠁', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣼⡇⠀⠀⠀⠀⠑⠶⠖⠊⠁⠀⠀⠀⡀⠀⠀⠀⢀⣠⣤⣤⡀⠀⠀⠀⠀⠀⢀⣠⣤⣶⣿⣿⠟⡱⠁⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢰⣾⣿⡇⠀⢀⡠⠀⠀⠀⠈⠑⢦⣄⣀⣀⣽⣦⣤⣾⣿⠿⠿⠿⣿⡆⠀⠀⢀⠺⣿⣿⣿⣿⡿⠁⡰⠁⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣾⣿⣿⣧⣠⠊⣠⣶⣾⣿⣿⣶⣶⣿⣿⠿⠛⢿⣿⣫⢕⡠⢥⣈⠀⠙⠀⠰⣷⣿⣿⣿⡿⠋⢀⠜⠁⠀⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠠⢿⣿⣿⣿⣿⣰⣿⣿⠿⣛⡛⢛⣿⣿⣟⢅⠀⠀⢿⣿⠕⢺⣿⡇⠩⠓⠂⢀⠛⠛⠋⢁⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠀', '⠘⢶⡶⢶⣶⣦⣤⣤⣤⣤⣤⣀⣀⣀⣀⡀⠀⠘⣿⣿⣿⠟⠁⡡⣒⣬⢭⢠⠝⢿⡡⠂⠀⠈⠻⣯⣖⣒⣺⡭⠂⢀⠈⣶⣶⣾⠟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀', '⠀⠀⠙⠳⣌⡛⢿⣿⣿⣿⣿⣿⣿⣿⣿⣻⣵⣨⣿⣿⡏⢀⠪⠎⠙⠿⣋⠴⡃⢸⣷⣤⣶⡾⠋⠈⠻⣶⣶⣶⣷⣶⣷⣿⣟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠈⠛⢦⣌⡙⠛⠿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡀⠀⠀⠩⠭⡭⠴⠊⢀⠀⠀⠀⠀⠀⠀⠀⠀⠈⣿⣿⣿⣿⣿⡇⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠈⠙⠓⠦⣄⡉⠛⠛⠻⢿⣿⣿⣿⣷⡀⠀⠀⠀⠀⢀⣰⠋⠀⠀⠀⠀⠀⣀⣰⠤⣳⣿⣿⣿⣿⣟⠑⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠓⠒⠒⠶⢺⣿⣿⣿⣿⣦⣄⣀⣴⣿⣯⣤⣔⠒⠚⣒⣉⣉⣴⣾⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠛⠹⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣭⣉⣉⣤⣿⣿⣿⣿⣿⣿⡿⢀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⠟⡁⡆⠙⢶⣀⠀⢀⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⣴⣶⣾⣿⣟⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠿⢛⣩⣴⣿⠇⡇⠸⡆⠙⢷⣄⠻⣿⣦⡄⠀⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣼⣿⣿⣿⣿⣿⣿⣿⣎⢻⣿⣿⣿⣿⣿⣿⣿⣭⣭⣭⣵⣶⣾⣿⣿⣿⠟⢰⢣⠀⠈⠀⠀⠙⢷⡎⠙⣿⣦⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣼⣿⣿⣿⣿⣿⣿⣿⣿⡟⣿⡆⢻⣿⣿⣿⣿⣿⣿⣿⣿⣿⠿⠿⠟⠛⠋⠁⢀⠇⢸⡇⠀⠀⠀⠀⠈⠁⠀⢸⣿⡆⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡜⡿⡘⣿⣿⣿⣿⣿⣶⣶⣤⣤⣤⣤⣤⣤⣤⣴⡎⠖⢹⡇⠀⠀⠀⠀⠀⠀⠀⠀⣿⣷⡄⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣦⡀⠘⢿⣿⣿⣿⣿⣿⣿⣿⣿⠿⠿⠛⠋⡟⠀⠀⣸⣷⣀⣤⣀⣀⣀⣤⣤⣾⣿⣿⣿⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⣸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣭⣓⡲⠬⢭⣙⡛⠿⣿⣿⣶⣦⣀⠀⡜⠀⠀⣰⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⢀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣭⣛⣓⠶⠦⠥⣀⠙⠋⠉⠉⠻⣄⣀⣸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⣼⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣶⣆⠐⣦⣠⣷⠊⠁⠀⠀⡭⠙⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡆⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⢠⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⢉⣛⡛⢻⡗⠂⠀⢀⣷⣄⠈⢆⠉⠙⠻⢿⣿⣿⣿⣿⣿⠇⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠘⣿⣿⡟⢻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡟⣉⢁⣴⣿⣿⣿⣾⡇⢀⣀⣼⡿⣿⣷⡌⢻⣦⡀⠀⠈⠙⠛⠿⠏⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠙⢿⣿⡄⠙⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠿⠛⠛⠛⢯⡉⠉⠉⠉⠉⠛⢼⣿⠿⠿⠦⡙⣿⡆⢹⣷⣤⡀⠀⠀⠀⠀⠀⠀⠀', '⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠘⠿⠄⠈⠻⠿⠿⠿⠿⠿⠿⠛⠛⠿⠛⠉⠁⠀⠀⠀⠀⠀⠀⠻⠿⠿⠿⠿⠟⠉⠀⠀⠤⠴⠶⠌⠿⠘⠿⠿⠿⠿⠶⠤⠀'] for line in Crypticlist: print(line) . Now its time to run your first script! You can run all of your script by clicking the green arrow on top (red circle) or you can run the code line-by-line with the green arrow with the line-symbol next to it (white circle). Once you ran the code you will see the variables created in the list on the top right (see picture below) and you will see the output of your code in the bottom right in the console (see picture below). May the code be with you! . ",
    "url": "/New_BAI_DataAnalysis/python_0_installation.html#installing-python",
    
    "relUrl": "/python_0_installation.html#installing-python"
  },"3": {
    "doc": "0. INSTALLATION",
    "title": "0. INSTALLATION",
    "content": " ",
    "url": "/New_BAI_DataAnalysis/python_0_installation.html",
    
    "relUrl": "/python_0_installation.html"
  },"4": {
    "doc": "1. BASICS OF PYTHON",
    "title": "The Basics of Python",
    "content": "This interactive tutorial will get you started on your data-exploration road and make you familiar with some core concepts of Python programming and data analysis. Notice: In all following sections I will insert some code snippets. You are very much encouraged to copy and paste them with the button on the top right and run them in your IDE (e.g. Spyder). Table of Contents . | General stuff about Python | Data Types and Variables | Operators | Loops and Conditionals | Functions and Classes | . Notice that it is not at all expected that you learn all these things and they are burnt into your brain (!!!!!). It is more of a broad intrdocution to all the basics so you have herd of them, but programmers do look up stuff all the time! So don’t worry if it is a lot of input right now, just try to understand the concepts and you can always come back and find help in here, in the internet or from me directly. Here are some useful ressources to look things up: . Link: w3schools.com: Tutorials on many topics where you can quickly look up things… Link: geeks4geeks.com: Another nice overview of many functionalities of Python (requires login)… Geeksforgeeks requires you to make an account or use e.g. a google login, but it features many tutorials, project ideas, quizzes and so on on many programming languages and general topics such as Machine Learning, Data Visualization, Data Science, Web Development and many more Link: Pandas cheat sheet: Later on we will use the library “Pandas” (so cute!) for data handling. A nice cheat sheet is provided by the developers… . ",
    "url": "/New_BAI_DataAnalysis/python_1_basics.html#the-basics-of-python",
    
    "relUrl": "/python_1_basics.html#the-basics-of-python"
  },"5": {
    "doc": "1. BASICS OF PYTHON",
    "title": "1. General things about Python",
    "content": "I quickly want to highlight some things which are special about Python compared to many other programming languages and that one needs to get used to. Python is indentation sensitive . In Python it matters, how far you indent your lines, meaning how much space you have at the beginning of a line. As an example this will work: . a = 5 b = 1 . but this will throw an error: . a = 5 b = 5 . will result in an error: . File \"&lt;stdin&gt;\", line 1 b = 5 IndentationError: unexpected indent . Variables . Generally in Python variables are created by assigning a value to them with an equal sign, just like we did above. Theire output can be shown by just typing the variable: . a = 5 a 5 . Comments . Comments are lines in the code that are not executed and are there for documentation. For now it is a good idea to use comments in your code to keep track of what is happening where. Single line comments are always created with an ‘#’. Everything after that symbol in the line is not executed. Multi-line comments can be written by enclosing them in three ‘: . # first I create a single line comment, this is not executed a = 5 # this line is executed, but the comment gets ignored ''' Now I write a multi-line comment I can continue the comment on the next line b = 5 &lt;-- this is ignored ''' . Python is 0-indexed . In Python, the first index in for example a list always has the number 0! This takes some time to get used to, especially if you come from e.g. R which has 1-based indexing, but most programming lanuge handle indexing like that and it is worth getting used to it. I won’t go into why it is handled like that but there are many discussions on the internet about it, feel free to dive in if you feel like diving into a rabbit-hole ;) . Separators . In Python separators for decimal numbers are ALWAYS dots! Commas are used e.g. to separete variables from each other or entries in a list . correct_decimal = 2.5 correct_decimal 2.5 . Naming of variables, functions, and anything at all . This is not Python-specific but a very important note! Always use descriptive names for variables, functions or anything that you give a name! Especially in scientific programming you see it time and time again that people name variables and functions using abbreviations that just came to their mind. This makes code much, much harder to read and to use by other people or your own future self. It happens so often that people look back at what they wrote 3 weeks ago and do not understand half of it because they did not give descriptive names. You can also use comments to document your code a bit, but that always takes up extra space, often does not look good because you barely keep the same formatting throughout the code and gives the next user more work to do when trying to understand the code. Just making the code explain itself is the best solution of all. Here is a very simple example: . # Bad code with abbreviations # it requires the user to interprete the variables and look at # used functions to understand what this even does l = [1,5,12,17,18,14,11] n = len(l) s = sum(l) m = s/n # Fixing it with comments # With comments we require the user to read all the extra text to # 1) understand what the data is # 2) understand what is calculated l = [1,5,12,17,18,14,11] # a list of temperature values n = len(l) # get total number of samples s = sum(l) # get total sum of samples m = s/n # calculate the mean value # This gets so much easier to read when using declarative naming. # You dont even have to look into the function to understand what it is doing: monthly_temperature = [1,5,12,17,18,14,11] number_of_samples = len(monthly_temperature) sum_of_samples = sum(monthly_temperature) mean_monthly_temperature = sum_of_samples/number_of_samples # This does not mean that naming has to replace comments completely # (although some people argue like that). It is still alright to use comments # to clarify parts of your code, just try keep it to a minimum and make the # code as self-explanatory as possible! . ",
    "url": "/New_BAI_DataAnalysis/python_1_basics.html#1-general-things-about-python",
    
    "relUrl": "/python_1_basics.html#1-general-things-about-python"
  },"6": {
    "doc": "1. BASICS OF PYTHON",
    "title": "2. Data Types and Variables",
    "content": "Python knows different types of data. A number is a different kind of variable than a word. That helps organizing the variables and defines, which operations are possible with which data. For example, computing the mean of a word would be difficult, just as translating a number to all-uppercase letters… . In Python you dont have to define the data type yourself because Python is smart and finds the type of data on its own. For example when we define a number Python will understand and give it the type “int” or “float”, which means “integer” or “floating point number” (decimal) We will not cover all data types as we probably won’t need all of them for our purpose. However these ones are important: . Primitive Datatypes Primitive datatypes are simple constructs that consists basically of one chunk of information, e.g. a number or a word: . | int: Integer, a number without a floating point | float: Floating point number, a number with decimals | str: A string of characters, e.g. letters, words and sentences | bool: Boolean, a value that can only be True or False. This helps us make decisions in our code | . Non-Primitive Datatypes Non-primitive datatypes consist of aggregations of primitive datatypes. A list for example holds several numbers or words or something else . | list: An ordered sequence of data, for example [1,2,3] is a list where each of the entries have a specific position and the entries can be accessed by indices | dict: A non-ordered mapping that consists of keys and values. That simply means, we can not get entries from the dictionary by indices (e.g. the 0th entry in a dictionary) but instead grab data from the dictionary by using the key. Imagine it like a digital telephone-book. The comparison does not hold completely because in theory a telephone book is ordered, but you would never search the 5001231th entry in a telephone book. Instead you would search the phone number of Mr. Smith”, so you go to the “key” Mr. Smith and get the “value” 0251/1234567. | . Lets look at some examples for data types: . # Primitive datatypes: letter_a = \"a\" # &lt;-- a string name = \"Josefine\" # &lt;-- a longer string age = 24 # &lt;-- an integer total_playtime = 354.5 # &lt;-- a float is_injured = False # &lt;-- a boolean # Non-Primitive datatypes: # list: # a list is alwasys enclosed by brackets # and the items are separeted with commas: scores_last_games = [5,3,0,1] # To access the values we can use the index, for example scores_last_games[0] # &lt;-- gets the first entry scores_last_games[2] # &lt;-- gets the 3rd entry scores_last_games[-1] # &lt;-- gets the last entry scores_last_games[-2] # &lt;-- gets the second last entry # dictionary: # is always enclosed by {}, # and has the structure \"key\":value, lines are separeted by a comma. josefine = { \"age\":age, \"total_playtime\":total_playtime, \"is_injured\":is_injured, \"scores_last_games\":scores_last_games } # Now the values of the dictionary can be accessed using the key like this: josefine[\"age\"] 24 # new entries can be added by assigning a value to a new key: josefine[\"trikot_number\"] = 9 . You can always find the type of a variable by using the type() function (more on functions later): . type(name) type(age) type(total_playtime) type(is_injured) type(scores_last_games) type(josefine) . It is possible to change the type of a variable, but only if Python is able to understand what the outcome should be. The functions to do that have the same name as the target data type, for example int() or str(): . int(\"10\") # &lt;-- this works str(500) # &lt;-- this works float(500) #&lt;-- this works float(\"500.5\") #&lt;-- this works float(\"abc\") #&lt;-- this won't work, how should you translate a word to a number? . One last thing is important to note. When you assign a non-primitive variable to another non-primitive variable, the two variables share the same data. That means, when you manipulate one you also manipulate the other. This can lead to confusion when you don’t keep it in mind. list_1 = [1,2,3] # a list is non-primitive list_2 = list_1 # here we assign the non-primitive list_1 to the variable list_2 list_2.append(4) # we add a fourth value, 4, to list_2 list_2 # list_2 is now [1,2,3,4] list_1 # BUT! list_1 is now also [1,2,3,4] # We can avoid this and extract the values from list_1 to create a completely new variable by using the .copy() function list_1 = [1,2,3] list_2 = list_1.copy() # we copy the values of list_1 to the new variable list_2 list_2.append(4) # we add a fourth value, 4, to the list list_2 list_2 # list_2 is now [1,2,3,4] list_1 # list_1 is still [1,2,3] . On the other hand when you assign a variable containing a primitive datatype to another variable, the value gets simply copied to the new variable. Here is an example: . a = 5 b = a # we assign the value of a to the variable b b = b + 1 # we increase the value of b by one b # b is now 6 a # a is still 5 . ",
    "url": "/New_BAI_DataAnalysis/python_1_basics.html#2-data-types-and-variables",
    
    "relUrl": "/python_1_basics.html#2-data-types-and-variables"
  },"7": {
    "doc": "1. BASICS OF PYTHON",
    "title": "3. Operators",
    "content": "An operator is something that allows you to interact with variables. Some very examples are mathematical operations or comparisons. 3.1 Arithmetic operations . Most operations are very intuitive. For example you can add numbers and also add words to concatenate them, but you can not subtract words from each other… Here is a list of operations: . a = 5 b = 10 word1 = \"Hi\" word2 = \"there\" # Airthmetic Operators: c = a + b # adding numbers concatenated_words = word1 + \" \" + word2 # adding words d = b - c # subtracting numbers e = a * b # multiplying numbers f = b / 5 # dividing numbers g = a ** 2 # Exponentation, this is a² h = 12 % 5 # Modulus, this returns the remaining amount after fitting one number into the other as many times as possible. Exercise . With what you know so far, grab the scores josefine scored in the last games and compute the average amount of goals per game she scores . Solution! scores = josefine[\"scores_last_games\"] total_scores = scores[0] + scores[1] + scores[2] + scores[3] mean_scores = total_scores / 4 . There are much better solutions to this, for example the iteration over all scores can be done with the built-in function sum() and the total number of score-values can be found using the len() function. A one-line solution could look like this: . mean_scores = sum(josefine[\"scores_last_games\"]) / len(josefine[\"scores_last_games\"]) . 3.2 Comparison operations . Comparison operations are used to compare values with each other in order to make decisions in your script. The output of a comparison is always a boolean value that is “True” if the comparison is evaluated as correct and “False” otherwise. goals_team1 = 5 goals_team2 = 2 goals_team1 &gt; goals_team2 # &gt; larger than goals_team1 &gt;= goals_team2 # &gt;= larger than or equal goals_team1 &lt; goals_team2 # &lt; smaller than goals_team1 &lt;= goals_team2 # &lt;= smaller than or equal goals_team1 == goals_team2 # == equal goals_team1 != goals_team2 # != not equal # you can also store the result in a variable: is_team1_winner = goals_team1 &gt; goals_team2 is_team2_winner = goals_team1 &lt; goals_team2 . 3.3 Logical operators . Logical operators can combine multiple comparisons. Namely there are three: and, or and not. The use of these is pretty intuitive. If we combine two comparisons with an “and”, the result is only True if all conditions hold. If we combine two comparisons with an “or”, the result is True if one of the conditions hold, even if the other is False. Not is a special case, that reverts the result. # Lets use a new example peter = { \"age\":24, \"height\":1.73, \"is_enrolled\": True } joana = { \"age\":25, \"height\":1.75, \"is_enrolled\": False } # Now we can do some comparisons: is_peter_taller_and_older_than_joana = peter[\"age\"] &gt; joana[\"age\"] and peter[\"height\"] &gt; joana[\"height\"] is_peter_not_enrolled = not peter[\"is_enrolled\"] is_joana_not_enrolled = not joana[\"is_enrolled\"] is_peter_or_joana_enrolled = peter[\"is_enrolled\"] or joana[\"is_enrolled\"] . 3.4 Identity and membership operators . The identitiy operator “is” is to check whether two objects are the same. On the other side, the membership operator “in” checks whether an object is contained within another object. Simple examples: . a = [1,2,3] # a simple list b = a # we assign a to b, remember non-primitive data types? a is b # What will be the result of this? 1 in a # we can test whether a contains a number 1 c = [a,b] # here we create a new list that contains the lists a and b a in b # now we can check whether one of the lists is within another list a in c . Code block in details in a notice . Exercise . Now you know all about operators. Try to use your knowledge and figure out what we test for in the following operations and what the result is: . joana = { \"enrolled\": True, \"grade_ecophysiology\": 1.3, \"grade_archery\": 1.3 } alfonso = { \"enrolled\": True, \"grade_ecophysiology\": 1.7, \"grade_archery\": 4.3 } legolas = { \"enrolled\": False, \"grade_ecophysiology\": 4.0, \"grade_archery\": 1.0 } # 1. a = (legolas[\"grade_ecophysiology\"] &lt; joana[\"grade_ecophysiology\"]) and (legolas[\"grade_ecophysiology\"] &lt; alfonso[\"grade_ecophysiology\"]) # 2. b = (legolas[\"grade_archery\"] &lt; joana[\"grade_archery\"]) and (legolas[\"grade_archery\"] &lt; alfonso[\"grade_archery\"]) # 3. c = not (legolas[\"grade_ecophysiology\"] &lt; joana [\"grade_ecophysiology\"]) or (alfonso[\"grade_ecophysiology\"] &lt; joana [\"grade_ecophysiology\"]) # 4. d = joana[\"enrolled\"] and alfonso[\"enrolled\"] and legolas[\"enrolled\"] # 5. e = alfonso[\"grade_ecophysiology\"] &gt; 4.0 or alfonso[\"grade_archery\"] &gt; 4.0 # 6. f = (alfonso[\"grade_ecophysiology\"] &gt; 4.0 or alfonso[\"grade_archery\"] &gt; 4.0) or (legolas[\"grade_ecophysiology\"] &gt; 4.0 or legolas[\"grade_archery\"] &gt; 4.0) or (joana[\"grade_ecophysiology\"] &gt; 4.0 or joana[\"grade_archery\"] &gt; 4.0) . Solution! . | Check 1 tests whether legolas is the best ecophysiologist. The result is False. | Check 2 tests whether legolas is the best archer. The result is True. | Check 3 tests whether legolas or alfonso are better ecophysiologists than joana. With the \"not\" in the beginning, the result is turned into whether Joana is better than any of the two. The result is True. | Check 4 tests whether everyone is enrolled. The result is False. Legolas is probably buisy somewhere else... | Check 5 tests whether Alfonso failed one of the exams with a grade higher than 4.0. The result is True. | Check 6 tests whether anyone failed one of the exams with a grade higher than 4.0. The result is True. | . ",
    "url": "/New_BAI_DataAnalysis/python_1_basics.html#3-operators",
    
    "relUrl": "/python_1_basics.html#3-operators"
  },"8": {
    "doc": "1. BASICS OF PYTHON",
    "title": "4. Conditionals and Loops",
    "content": "Conditionals and loops are constructs in your code that are often combined. A loop is used to do a certain task on many elements sequentially, a conditional uses a certain condition (or truth-evaluation) to decide whether a piece of code should be executed. 4.1 Conditionals . Remember how we talked about comparison, logical and identitiy and membership operators? They all result in a boolean, stating whether a condition is True or False. We can make use of that by utilizing conditionals. Here is a simple example: . is_peter_smart = True if is_peter_smart == True: print(\"Peter is smart\") . Notice how indentation plays a role here! We end the line of the if-check with a “:” and start the new line indented. Indented lines signal a code block, that always belongs to the previous statement that ended with a “:”. In the above example the print() command will be executed because the value of is_peter_smart is True. If we check for a boolean value (True or False) we can also leave the comparison operation out and ask very coloquially: . is_peter_smart = True if is_peter_smart: print(\"Peter is smart\") . We can also define a code that should be executed, ONLY if the if-check is evaluated as False. For that we use the keyword “else”. is_peter_smart = True if is_peter_smart: print(\"Peter is smart\") else: print(\"Peter is not smart\") . Finally, you can also chain if-checks by using the “elif” keyword. This stands for “else if”, meaning that “if the previous checks failed and this check is evaluated as True, run the code” . is_peter_smart = False is_peter_big = True if is_peter_smart: print(\"Peter is smart\") elif is_peter_big: print(\"Peter is big\") else: print(\"Peter is not smart and not big\") . 4.2 Loops . A loop is a structure that allows you to iteratively perform actions, either with several elements (e.g. stored in a list) or while a specific condition holds. These two types are called “for-loops” and “while-loops”. They always consist of two parts: The definition how and over what you want to iterate (or “loop”) and the actual action you want to perform. 4.2.1 The for-loop . The most “classical” loop is the for-loop. The syntax is, as often in Python, held very simple. Here is an easy example: . temperatures = [12,14,16,15,16,17,20,21] for temperature in temperatures: print(temperature) . Notice that in the definition of the loop, we define a new variable called “temperature”. This variable represents the element we are currently working on in each step of the loop. So in the first step, temperature is 12, in the next temperature is 14 and so on. There is one very handy built-in method that can give you both the value of the list-entry and its corresponding index, called enumerate(). You can put them both in variables by using a comma in the loop-definition. A quick demo: . temperatures = [12,14,16,15,16,17,20,21] hour_of_day = [8,9,10,11,12,13,14,15] # When using enumerate, each iteration we get the index and value of the current list entry. # So in the first loop index will be 0 and temperature 12, # next index will be 1 and temperature 14 and so on... for index, temperature in enumerate(temperatures): print(\"Temperature at \"+str(hour_of_day[index]) + \":00: \"+str(temperature) + \"°C\") . 4.2.2 The while-loop . A while loop is not used as often as a for-loop. In the definition you define a condition and “while” that condition holds, the loop is executed. Look at this example: . a = 1 while a &lt;= 10: print(a) a = a +1 . Can you guess what will be display? . Solution It will print the numbers 1 to 10, including 10 Warning . When you define a while-loop, always make sure that the condition will at some point be fullfilled. Otherwise it can easily happen that youre while-loop just keeps running endlessly! . a = 1 # This loop will run forever, because a will never be &gt; 10! while a &lt;= 10: print(a) . Exercise . Now you already know quite some tools for writing a Python script! Use your knowledge to complete the code below. The goal is to print the sentence \"{month} was a hot month\" whenever the mean monthly temperature is above two times the mean and \"{month} was a dry month\" whenever the precipitation was less than half of the mean. One tip: For the printing you can use formatted strings. They make inserting variables in a string much easier! just put an \"f\" in front of the string and insert the variable in curly braces {}. For example print(f\"Hello {name}\" would print \"Hello Peter\" if the variable name=Peter is defined. Here is your starter code: . months = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"Juli\",\"August\", \"September\", \"October\", \"November\", \"December\"] monthly_temperature = [4, 4, 7, 11, 15, 17, 28, 24, 27, 12, 7, 4] monthly_precipitation = [15, 40, 60, 75, 65, 32, 10, 80, 60, 70, 57, 100] mean_temperature = mean_precipitation = for ... in enumerate(...): if ...: ... if ...: ... Solution! months = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"Juli\",\"August\", \"September\", \"October\", \"November\", \"December\"] monthly_temperature = [4, 4, 7, 11, 15, 17, 28, 24, 27, 12, 7, 4] monthly_precipitation = [15, 40, 60, 75, 65, 32, 10, 80, 60, 70, 57, 100] mean_temperature = sum(monthly_temperature)/len(monthly_temperature) mean_precipitation = sum(monthly_precipitation)/len(monthly_precipitation) for index, month in enumerate(months): if monthly_temperature[index] &gt; 2*mean_temperature: print(f\"{month} was a hot month\") if monthly_precipitation[index] &lt; mean_precipitation/2: print(f\"{month} was a dry month\") . ",
    "url": "/New_BAI_DataAnalysis/python_1_basics.html#4-conditionals-and-loops",
    
    "relUrl": "/python_1_basics.html#4-conditionals-and-loops"
  },"9": {
    "doc": "1. BASICS OF PYTHON",
    "title": "5. Functions and Classes",
    "content": "Congratulations! You made it this far down, that means you have accquired knowledge of the basic building blocks of Python. You are now ready to go into two concepts that go beyond basic scripting (meaning, just putting all your code line by line into one file), and learn about the fundamental blocks that help strcuturing your program: Functions and Classes! . 5.1 Functions . Functions are constructs of own, separate blocks of code in your program which take care of certain tasks. They are super useful, because often you want to do the same operation many times in your code but don’t want to write the same code every time again. Just write your own function and call it whenever you need its expertise! Lets just look at a simple example: . def calculate_mean(list_of_values): n_samples = len(list_of_values) sum_of_values = sum(list_of_values) mean = sum_of_values/n_samples return mean . Pretty intuitive, right? . A function is always defined by starting with the keyword “def”, then we give it a name, calculate_mean in this case. Afterwards in the braces are the “arguments” that the function takes. Arguments are pieces of information from the outside code, which the function requires to work. Here it is the list_of_values the function shall calculate the mean value of. After the “:” we follow with the indented codeblock that belongs to the function. Here we do all the operations the function should do. Finally, we use the “return” keyword which ends the function and defines, which piece of information should be returned to the outside code. Important The variables which are defined inside a function are restricted to that function! The outside code won’t know of the variables n_samples or “mean” which are defined in the function. Calling the function would for example look like this: . monthly_temperature = [4, 4, 7, 11, 15, 17, 28, 24, 27, 12, 7, 4] mean_monthly_temperature = calculate_mean(monthly_temperature) . You do not have to return a value. You could also for example print something in the function and then return, without providing a value to return. In older versions of Python this was all there was to writing a function. However, nowadays you can add some additonal information to make it even easier for the next person or your future self to understand it. With some extra bits you can add the infos, what type of data you expect as an input to the function and what type of data it will output. This is generally a good thing to do and now considered best practice when writing functions. For the above code it would look like this: . def calculate_mean(list_of_values:list[float]) -&gt; int: n_samples = len(list_of_values) sum_of_values = sum(list_of_values) mean = sum_of_values/n_samples return mean . In the first line, after the list_of_values we write “:list[float]” to specify that we expect a list of float (floats actually imply integers, so we can use that to also accept integers). After the closing bracket we write “-&gt; int” which states that this function will return an integer value. Exercise 5.1.1 . As a first exercise, try to figure out what the output of the below function will be without executing it! . def square_value(value:int) -&gt; int: return value * value def divide_value_by(numerator:int, denominator:int) -&gt; int: return numerator / denominator a = square_value(2) b = square_value(a) c = divide_value_by(b,a) d = square_value(divide_value_by(c,2)) print(d) . Solution! The result is 4! . def square_value(value:int) -&gt; int: return value * value def divide_value_by(numerator:int, denominator:int) -&gt; int: return numerator / denominator a = square_value(2) # 2*2 = 4 b = square_value(a) # 4*4 = 16 c = divide_value_by(b,a) # 16/4 = 4 d = square_value(divide_value_by(c,2)) # square_value gets the output of divide_value(c,2) as argument. print(d) # so 4/2 is 2, thn 2*2 is 4 . Exercise 5.1.2 . Lets go for a bit more challenging of an exericse (I am sure you are ready for it!) There is a built-in function that allows the user to give an input through the command-line to the program. It is simply called \"input()\". E.g. \"testword = input()\" would stop the program and wait for the user to input something in the console and then press enter. Imagine you want a program in which you set a new password. Write a function that checks whether the new password is longer than 9 symbols and that returns the corresponding boolean. the function should also print that the password is too short if it is too short and that it is ok when it is ok. Use the returned boolean to keep asking for new input from the user while the word is less than 9 characters long . Here is some starter code: . def is_password_too_short(word:str, min_length:int)-&gt;bool: is_password_too_short = ... if ...: .... else: .... return ... password_is_bad = True while ...: print(\"Please enter your password:\") password = input() password_is_bad = ... Solution! def is_password_too_short(word:str, min_length:int)-&gt;bool: is_password_too_short = len(word) &lt; min_length if is_password_too_short: print(f\"Password has to be at least {min_length} characters long!\") else: print(\"New password set!\") return is_password_too_short password_is_bad = True while password_is_bad: print(\"Please enter your new password:\") password = input() password_is_bad = is_password_too_short(password, 8) . 5.2 Classes . Classes are the final fundamental building block of Python we will look at here. A class basically represents a blueprint of an object that has certain properties. As an example, when I am working with data on ecosystems it could be convenient to have an ecosystem class that includes information about the ecosystem type, the location as latitude and longitude, and some meteorological data. Lets look at an example: . class Ecosystem(): def __init__(self, id, IGBP_ecosystem_class, lat, lon, mean_annual_Tair, mean_annual_precip): self.id = id self.IGBP_ecosystem_class = IGBP_ecosystem_class self.lat = lat self.lon = lon self.mean_annual_Tair = mean_annual_Tair self.mean_annual_precip = mean_annual_precip . The definition of a class always begins with the class-keyword followed by the name of the class. It always has a first function called init() which is also called “constructor”. This method is used to create new instances of the class and assigns values to the class. The “self” keyword is in this context always used within a class to reference the class itself. Note, that “self” also has to be in the list of arguments for the function, but is does not get passed when you call the function. Many new words but stay with me, it is pretty simple when we look at an example, how we create a new instance: . # First we define the class class Ecosystem(): # see how the first argument here is \"self\" def __init__(self, id, IGBP_ecosystem_class, lat, lon, mean_annual_Tair, mean_annual_precip): self.id = id self.IGBP_ecosystem_class = IGBP_ecosystem_class self.lat = lat self.lon = lon self.mean_annual_Tair = mean_annual_Tair self.mean_annual_precip = mean_annual_precip # now we use that class to create a new ecosystem-object, # see how we have to provide every value defined in the constructor except for \"self\": amtsvenn = Ecosystem(id=\"amtsvenn\", IGBP_ecosystem_class = \"open shrublands\", lat = 52.176, lon = 6.955, mean_annual_Tair = 10.5, mean_annual_precip = 870) # Now you have stored all the info about amtsvenn in the \"amtsvenn\" # object and can access them whenever you want: print(amtsvenn.id) print(amtsvenn.IGBP_ecosystem_class) print(amtsvenn.lat) print(amtsvenn.lon) . Classes can not only comprise of the information associated with them but can also have methods associated specifically with them. For example we can create a function that prints all the information enclosed in the object. class Ecosystem(): def __init__(self, id, IGBP_ecosystem_class, lat, lon, mean_annual_Tair, mean_annual_precip): self.id = id self.IGBP_ecosystem_class = IGBP_ecosystem_class self.lat = lat self.lon = lon self.mean_annual_Tair = mean_annual_Tair self.mean_annual_precip = mean_annual_precip def print_ecosystem_information(self): print(\"=====================\") print(\"Ecosystem information\") print(f\"ID: {self.id}\") print(f\"IGBP ecosystem class: {self.IGBP_ecosystem_class}\") print(f\"Location (lat/lon): {self.lat}°/{self.lon}°\") print(f\"Mean annual air temperature: {self.mean_annual_Tair} °C\") print(f\"Mean annual precipitation: {self.mean_annual_precip} mm\") amtsvenn = Ecosystem(id=\"amtsvenn\", IGBP_ecosystem_class = \"open shrublands\", lat = 52.176, lon = 6.955, mean_annual_Tair = 10.5, mean_annual_precip = 870) # After creating the object we can use the classes functions like this: amtsvenn.print_ecosystem_information() . Exercise 5.2.1 . Lets do one exercise that can further show, why classes are great for creating reusable code. Try to write a function called \"Statistics\". This class will be a \"behavioural\" class, meaning it does not need to hold own data but rather holds some methods, that belong to the same topic. In that class, define functions that calculate the mean, the variance and the standard deviation of a given list. Then use that class to calculate these metrics of an arbitrary list. Hint: For the standard deviation you need to take the square root. You can do that with pythons built-in math module. You can use it like this: . import math math.sqrt(24) . Try to work out the solution yourself first! There is some starter code below, in case you get stuck though. Starter code class Statistics(): def calculate_mean(self, ...): ... def calculate_variance(self, ...): mean = ... squares = [] for value in values: squares.append(...) variance = ... return ... def calculate_stdev(self, ...): variance = ... stdev = .... return ... Solution! import math class Statistics(): def calculate_mean(self, values:list[float]): return sum(values)/len(values) def calculate_variance(self, values:list[float]): mean = self.calculate_mean(values) squares = [] for value in values: squares.append((value-mean)**2) variance = sum(squares) / (len(values)-1) return variance def calculate_stdev(self, values:list[float]): variance = self.calculate_variance(values) stdev = math.sqrt(variance) return stdev stat = Statistics() example_list = [1,2,3,4,5,5,6,7,123,1,1,4] mean = stat.calculate_mean(example_list) stdev = stat.calculate_stdev(example_list) variance = stat.calculate_variance(example_list) . We will not go deeper into classes here, but it is very important to understand the concept. Most Python packages are written in object-oriented style, which (in very simple terms) means that the methods are enclosed in classes. So knowing the basics makes it much easier to understand the following bits. ",
    "url": "/New_BAI_DataAnalysis/python_1_basics.html#5-functions-and-classes",
    
    "relUrl": "/python_1_basics.html#5-functions-and-classes"
  },"10": {
    "doc": "1. BASICS OF PYTHON",
    "title": "1. BASICS OF PYTHON",
    "content": " ",
    "url": "/New_BAI_DataAnalysis/python_1_basics.html",
    
    "relUrl": "/python_1_basics.html"
  },"11": {
    "doc": "2. DATA HANDLING AND VISUALIZATION",
    "title": "Of plots and pandas: Data handling and Visualization",
    "content": "In this second part of the course we will talk about how to handle, process and visualize data in Python. For that purpose we will make use of a few third-party libraries. NumPy and Pandas will help us store the data in array- and matrix-like structures (in Pandas more specifically Dataframes) and do some processing of the data. Pandas already has some visualization capabilities, but for nicer looks and configurability we will make use of the Plotly package. To underline that these are essential tools in Python, let me once again pull out the Stackoverflow 2023 survey: According to the ~3k respondants, Numpy, Pandas and Scikit-Learn (which we will use in the next lesson) are 3 of the 8 most used technologies in programming across all languages (disregarding web-technologies)! . For this part we will use some example data. It is a dataset from the german weather service DWD from the Ahaus Station (ID 7374) ranging from 1996 to 2023. Click here to download (25mb)…. Note The data is in .parquet-format. You may not have heard of it, but this is a very compressed and fast format. For example this dataset with 27 years worth of data, in Parquet this is 25mb of data, in .csv its 208mb. While you can not open .parquet directly in excel or a text editor like a .csv file, it is much much faster to load e.g. when using it in programming languages, which is exactly what we are going to do here. As a last note: NumPy is one of the older Python libraries and Pandas is actually built on top of it. However, because we work with example data and want to get hands-on as fast as possible, we will cover Pandas first and then go from there. Table of Contents . | Pandas | NumPy | Plotly | . ",
    "url": "/New_BAI_DataAnalysis/python_2_data_visualization.html#of-plots-and-pandas-data-handling-and-visualization",
    
    "relUrl": "/python_2_data_visualization.html#of-plots-and-pandas-data-handling-and-visualization"
  },"12": {
    "doc": "2. DATA HANDLING AND VISUALIZATION",
    "title": "0. Importing modules",
    "content": "Just a quick forword on importing libraries in Python. Pandas, Plotly and Numpy are all external libraries we need to import to our script in order to make them work. Usually we would also have to install them, but since we work in Anaconda, this is already taken care of for us! Very simply, to import a library you type “import” and then the respective name. Typically you want to give an “alias” to the package, which is basically a variable that you can then use to access all the methods in the package. For some packages there are long-standing standards of what names to use. For pandas for example this is “pd”: . import pandas as pd . You can also only import specific parts of a package, which can save memory. Going back to one exercise from the previous lesson, if you know that you will only use the sqrt function from the math package you can use the syntax . # Importing only a single function, squareroot from math import sqrt # Importing several functions, squareroot and greatest common divider from math import sqrt, gcd # theoretically you could also give an alias here from math import sqrt as squareroot # this does not make much sense though . ",
    "url": "/New_BAI_DataAnalysis/python_2_data_visualization.html#0-importing-modules",
    
    "relUrl": "/python_2_data_visualization.html#0-importing-modules"
  },"13": {
    "doc": "2. DATA HANDLING AND VISUALIZATION",
    "title": "1. Pandas",
    "content": "Pandas is around since 2008 and one of the most wiedely used tools for data analysis of all. The usage is all about two types of objects: The pandas Series and the Pandas DataFrame, where a Series is more or less one column of a dataframe (basically a vector). If you already worked with R, the concept of a DataFrame is not new to you. However for starters, a DataFrame is basically a table, in which each row has an index and each column has a label. Simple right? . (credit: https://www.geeksforgeeks.org/creating-a-pandas-dataframe/) . Creating DataFrames . Lets create a first little DataFrame. There are several ways to do it, one rather intuitive way is to use a dictionary. Think about it, a dictionary already has values which are labeled by keys. You can easily imagine this in a table-format: The keys will be the column-labels and the indices (row-labels) are by default just numbered. import pandas as pd # Note that we create an instance of the class \"DataFrame\" # Therefore we have to call the function pd.DataFrame(). Within # the brackets we then define a dictionary using the {}-style syntax values_column_1 = [2,4,6,8,10] values_column_2 = [3,6,9,12,15] df = pd.DataFrame({ \"column_1\": values_column_1, \"column_2\": values_column_2 }) . Another option to create a dataframe is of course to read in data. Lets go ahead and read the data from the german weather service that you can download above. Now we can use pandas built-in data-reader to directly create a DataFrame from the parquet-file: . # The path can either be the absolute path to the place where you saved the file # or the relative path, meaning the path relative to the place where your script is. # I'd recommend to create a subfolder where your script is called \"data\" and then # import the data from the path \"./data/ahaus_data_1996_2023.parquet\" df_dwd = pd.read_parquet('path_to_file') . Accessing rows and column . Once you createad a dataframe, you can access individual columns by using the column names. Either you can directly access them using brackets, or you use the built-in “.loc”-function. I would recommend getting used to the .loc right away, as it rules out some errors you can run into otherwise. With .loc you always have to provide first the rows you want to access and then the column, separated with a comma. If you want to get all rows, that is done using a colon (“:”) To get a list of all availabel columns you can simply type “df.columns” . # First we can take a look at the available columns df_dwd_columns = df_dwd.columns print(df_dwd_columns) # Then we can use the column names to extract a column # from the dataframe # Either you use only the column name in brackets: df_dwd[\"tair_2m_mean\"] # But even better: use the .loc function: df_dwd.loc[:,\"tair_2m_mean\"] # get all rows df_dwd.loc[20:50,\"tair_2m_mean\"] # get rows 20 to 50 df_dwd.loc[:20,\"tair_2m_mean\"] # get all rows up to 20 (including 20) df_dwd.loc[20:,\"tair_2m_mean\"] # get all rows after 20 (including 20) . Note that the .loc examples above all assume numeric index. But Pandas is not restricted to that! The index (or “row-label”) could also be something like “mean” or “standard-deviation”. Keep that in mind for the exercise below! . Built-in methods to describe the data . Pandas has a great set of convenience functions for us to look at and evaluate the data we have. | .info() gives us a summary of columns, number of non-null values and datatypes | .head() and .tail() show the first or last five rows of the dataframe | .describe() directly gives us some statistical measures (number of samples, mean, standard deviation, min, max and quantiles) Note that the output of .describe() is again a DataFrame, that you can save in a variable to evaluate it. There are also built-in methods that you can run directly on single columns. Examples of such functions are .mean(), .min(), .max() and .std(). | . Exercise . You already know, how to call a method that is attached to a class. With that knowledge, explore the Ahaus DWD dataset and figure out the mean, standard deviation, min and max for air temperature, precipitation height, air pressure and short wave radiation (SWIN) . Hint It may be that the output of the .describe() function has a pretty bad formatting with 5 decimal numbers or more. In that case you can change the formatting of the output using . df.describe().applymap('{:,.2f}'.format) . Solution! # There are lots of ways to complete this exercise. # You can use the above mentioned describe() method # First get the summary. Save the output of .describe() # in a new dataframe df_dwd_summary = df_dwd.describe().applymap('{:,.2f}'.format) # Then you can access values in that dataframe like this: tair_2m_mean = df_dwd_summary.loc[\"mean\", \"tair_2m_mean\"] tair_2m_min = df_dwd_summary.loc[\"min\", \"tair_2m_mean\"] # and so on... # You could also directly use the pandas built-in .min, .max, # .mean and .std methods. For example: tiar_2m_mean = df_dwd[\"tair_2m_mean\"].mean() tiar_2m_min = df_dwd[\"tair_2m_mean\"].min() # and so on... Challenge . There is a one-line solution to this task, that only grabs the values asked for in the exercise. I wouldn’t say that that would be the recommended solution for the sake of overview, but to fiddle around it is a good challenge. Hint: You can pass lists for the row- and column-labels to .loc . Solution # We can chain all the commands above to a one-line operation, meaning we # directly call .describe().map().loc[] on each others output. # By passing the list [\"mean\", \"std\", \"min\", \"max\"] as row-indices and # [\"tair_2m_mean\",\"precipitation\",\"SWIN\",\"pressure_air\"] as column-labels # we can directly access the range of values asked for in the exercise. df.describe().applymap('{:,.2f}'.format).loc[[\"mean\", \"std\", \"min\", \"max\"], [\"tair_2m_mean\",\"precipitation\",\"SWIN\",\"pressure_air\"]] . Datetime . Pandas has a specific datatype that is extremely useful when we are working with time series data (a s our example DWD dataset). It is called datetime64[ns] and allows us to do a range of super useful things like slicing based on dates or resampling from 10-minute to daily, weekly, monthly data and so on. With datetime-indices, handling timeseries gets so much more convenient. # get data newer than 31.12.2022 df_dwd[df_dwd[\"data_time\"] &gt; \"2022-12-31\"] # get only data from 2022 df_dwd[df_dwd[\"data_time\"].dt.year == 2022] # But wait! Its not working, is it? # Can you figure out why not? Remember the type() function! . Now, how do we get this to work for us? Well, the methods work with the datetime64 data type, so we need to change the “data_time” column type! Luckily, Pandas has a function for that. It is called to_datetime() and is part of the main library, so you call it as pd.to_datetime(). It takes the column you want to convert to datetime64 type as argument, tries to parse it to datetime64 and returns the result series. If it fails to parse, maybe because your data_time column is in a country-specific formatting, you can pass an additional “format” argument in which you provide the input format. But we will not cover it here, as the default should work for the DWD dataset. example_df = pd.DataFrame({ \"data_time\": [\"2022-01-01 01:00:00\",\"2022-01-01 12:00:00\", \"2022-01-02 01:00:00\", \"2022-01-02 12:00:00\", \"2022-01-03 01:00:00\", \"2022-01-03 12:00:00\"], \"values\" : [1,5,4,20,6,-10] }) type(example_df[\"data_time\"]) example_df[\"data_time\"] = pd.to_datetime(example_df[\"data_time\"]) . Exercise . 1. In your dataframe, turn the \"data_time\" column into a datetime64 type column. Then create dataframes for each season across all years, meaning one for spring, summer, autumn and winter each. The respective months are March to May, June to August, September to November and December to February. Compare the mean air temperature, precipitation and radiation between the different seasons. 2. Find the dates of the maximum temperatures measured in the dwd dataset. One hint: What we want to do here is to find those rows, where the value is one of a set of values. To do so you can use the built-in pandas function .isin(). An example: . # Here is an example series (representing a column of a dataframe) series = pd.Series([1,2,3,1,2,3,1,2,3]) # Wen want to extract the rows where the value is 1 or 3: desired_values = [1,3] series_ones_and_threes = series[series.isin(desired_values)] # Note that the indices in the extracted series are the ones from series, where the value is 1 or 3, # so it really represents an extracted subset of the original series . Solution! df_dwd[\"data_time\"] = pd.to_datetime(df_dwd[\"data_time\"]) df_dwd[\"data_time\"] = pd.to_datetime(df_dwd[\"data_time\"]) # First of all we create 4 dataframes, one for each season # We do it by accessing the numeric value of the months in the \"data_time\" # column. 1 refers to January and so on. With the .isin() method we extract # those rows where the values correspond to the numbering of the month df_dwd_summer = df_dwd.loc[df_dwd[\"data_time\"].dt.month.isin([6,7,8])] df_dwd_autumn = df_dwd.loc[df_dwd[\"data_time\"].dt.month.isin([9,10,11])] df_dwd_winter = df_dwd.loc[df_dwd[\"data_time\"].dt.month.isin([12,1,2])] df_dwd_spring = df_dwd.loc[df_dwd[\"data_time\"].dt.month.isin([3,4,5])] # To find the mean for each season we have a range of different options # how we want to get the means and compare them. I'll show three different # ways which are all valid. # We know we will want to do some operation on all of the 4 datasets, so it is # already a good idea to put them in a list. That way we can easily iterate over them seasonal_datasets = [df_dwd_spring, df_dwd_summer, df_dwd_autumn, df_dwd_winter] # Now one option would be to iterate over the datasets and print # the mean values of the desired columns: seasons = [\"spring\", \"summer\", \"autumn\", \"winter\"] for idx, df in enumerate(seasonal_datasets): print(\"----------\") print(seasons[idx]) print(\"----------\") print(f'mean Ta: {df[\"tair_2m_mean\"].mean()}') print(f'mean precipitation: {df[\"precipitation\"].mean()}') print(f'mean SWIN: {df[\"SWIN\"].mean()}') # This way we have the outputs grouped by seasons # Another option would be to iterate over the variables we want # to evaluate. Then we can print the variable values for each # season directly below each other: variables = [\"tair_2m_mean\", \"precipitation\", \"SWIN\"] for idx, variable in enumerate(variables): print(\"--------\") print(variable) print(\"--------\") for i, df in enumerate(seasonal_datasets): stats = df.describe() print(f\"{seasons[i]}: {stats.loc['mean', variable]}\") # Often times we don't even want to print the output but rather # just extract and keep it for later use, e.g. for visualizing it later. # So another option is to create a new dataframe that holds # the seasons as columns and variables as rows. That way we can # just look at the whole new dataframe and easily compare the values seasonal_df = pd.DataFrame(columns = seasons) for idx, df in enumerate([df_dwd_spring, df_dwd_summer, df_dwd_autumn, df_dwd_winter]): season = seasons[idx] seasonal_df.loc[\"Ta\", season] = df[\"tair_2m_mean\"].mean() seasonal_df.loc[\"Precip\", season] = df[\"precipitation\"].mean() seasonal_df.loc[\"SWIN\", season] = df[\"SWIN\"].mean() print(seasonal_df) . In this exercise we extracted seasonal information from 5-minute interval data. This type of frequency-conversion is something we do very often when working with time-series data. We also call this operation “resampling”. Pandas actually has a great convenience function, that makes resampling a breeze, utilizing the wonderful datetime64-format. The operation consists basically only of two function calls on the pandas dataframe. The first is “.resample()”. We must define the column that contains the datetimes with the “on” argument and our target frequency with the “rule” argument as a string. The most useful frequency specifiers are: . | “S”: seconds | “T” or “min”: minutes | “H”: hours | “D”: days All of these can be extended with numbers, such as “7D” for 7 days or “30min” for half-hourly values. | . Afterwards we also have to call a function that specifies how we want to resample. You see, if we change the frequency from 5 minute data to daily data, the daily value can be computed in different ways. For example for temperature it would make sense to use the daily mean value. For precipitation on the other hand it is probably more useful to get the daily sum, if we are interested in the amount of rain per day. That is why “.resample()” has to be followed by a function like “.mean()” or “.sum()”. Here is a full example: . example_data_time = pd.to_datetime([\"2022-01-01 01:00:00\",\"2022-01-01 12:00:00\", \"2022-01-02 01:00:00\", \"2022-01-02 12:00:00\", \"2022-01-03 01:00:00\", \"2022-01-03 12:00:00\"]) example_df = pd.DataFrame({ \"data_time\": example_data_time, \"values1\" : [1,5,4,20,6,-10], \"values2\" : [100,500,400,2000,600,-1000], }) df_daily_means = example_df.resample(rule=\"1D\", on=\"data_time\").mean() df_daily_sums = example_df.resample(rule=\"1D\", on=\"data_time\").sum() . I mentioned before that pandas itself already has some built-in plotting capabilities. I won’t go deep onto it, but it is definitely worth mentioning because you can use it to get a quick overview of data in a pandas dataframe. You can simply call the “.plot()” function on any dataframe. You can run this on the whole dataframe, which will plot all available columns, or you extract specific rows/columns with the methods we discussed before and then run “.plot()”: . # lets use the example_df from above: # First we plot both values1 and values2: example_df.plot(x=\"data_time\") # Note that we have to say that we want data_time on the # x-axis, because otherwise it will by default use a numeric # index on the x-axis and plot the data_time column against it # as well. You can try it out by leaving the x=... out # The figure should now pop up in the \"plots\" tab on the right # side of your Spyder window # Now we just plot values 1: exampl_df.plot(x=\"data_time\", y=\"values1\") . There is a lot more functionality, but I want to leave the pandas plotting with that, as we will dive deeper into plotting later in this course. Ok, we have covered quite some ground on handling pandas dataframes. We covered . | how to create dataframes | how to read data from .csv or .parquet files | how indexing works | how we get some descriptive information on the data | how to compute some informative values such as the min, max and mean of a series | even how datetime-indices work (honestly we just scratched the surface, but for an introduction course this is already quite advanced) | and how to resample time-series data to another frequency Finally I just want to give some “honorable mentions”, to tell you about functions with pandas that you will probably need at some point. No exercise here, I just want you to have heard of these: | . # 1. pd.concat(): # this function concatenates dataframes with matching columns or pandas series # meaning it simply glues one dataframe on the bottom of the next: df_1 = pd.Series([1,2,3]) df_2 = pd.Series([4,5,6]) df_3 = pd.concat([df_1, df_2]) # note that we have to put the two dataframes in a list # 2. pd.merge() # This functions combines dataframes based on common indices. # It is a rather complex function but this is a simple example # how to combine two dataframes that have overlapping indices: df_1 = pd.DataFrame( index = [1,2,3], data = { \"col_1\": [1,2,3], \"col_2\": [4,5,6] }) df_2 = pd.DataFrame( index = [3,4,5], data = { \"col_1\": [7,8,9], \"col_2\": [10,11,12] }) df_3 = df_1.merge(right=df_2, how = \"outer\") # 3. df.apply() # In the call to apply you can define a function that will be # executed on each element of the dataframe: df_1_plus_one = df_1.apply(lambda x: x+1) # don't worry about the \"lambda\", it simply creates # the variable \"x\" we can use for \"x+1\". x is only there # during the computation and then immediately vanishes again . ",
    "url": "/New_BAI_DataAnalysis/python_2_data_visualization.html#1-pandas",
    
    "relUrl": "/python_2_data_visualization.html#1-pandas"
  },"14": {
    "doc": "2. DATA HANDLING AND VISUALIZATION",
    "title": "2. A quick touch on Numpy",
    "content": "Many Python programmers and data scientists would probably shun me for not giving more time to numpy, but we want to get to the applications as fast as possible. However, if you want to know more you can . | download a little Numpy cheat sheet here | check out the official Numpy documentation | read about numpy at w3schools.com Numpy is like the grandmaster of handling data in Python. It has always been there, it can do everything, but it is not neccessarily pleasant to deal with. With Numpy you can create vectors and multi-dimensional matrices, do computations and much more. It is very lightweight (meaning it uses very little memory) and super fast. Actually, Pandas has Numpy as its underlying framework. Every column or row in a pandas datframe is actually a Numpy array with fancy extras. That makes Pandas slower than Numpy but also much more convenient to use. While we can do most of our analysis in this course only with Pandas, I think you should know about the basic functionality and the core uses of Numpy. So lets take a look at some simple structures and computations: | . 2.1. Numpy Arrays . The most used structure in Numpy are arrays. In contrast to normal Python lists, they are faster, they force the values to be homogeneous (e.g. no strings and integers mixed in a Numpy array), and with Numpy arrays you can compute some mathematical operations between arrays such as element-wise addtion, cross-products and so on. Additionally, numpy provides a range of functions you can run directly on arrays, such as .mean(), .min(), .max(), .median() and so on. Generally you can think of Numpy arrays/matrices vs Pandas Dataframes/Series as the difference between pure vectors or structures with pure numeric data in them vs. fully fledged and labeled tables. There are different ways to create Numpy arrays: . import numpy as np # a simple vector is created by calling np.array # with a list as argument: vector_1 = np.array([0,1,2,3,4,5]) # alternatively, you can directly create a vector # filled with zeros or ones providing a shape. # The shape has round brackets and defines the # dimensions of the data structure. For example # (2,3) will create a matrix with 2 rows and 3 columns vector_zeros = np.zeros(shape=(2,3)) vector_ones = np.ones(shape=(2,3)) # with np.random.rand() you can create a matrix with # random elements between 0 and 1, by multiplying it # you can get e.g. values between 0 and 10: vector_randoms_0_to_1 = np.random.rand(3,10) vector_randoms_0_to_10 = np.random.rand(3,10)*10 # You can then get individual elements from that 2-D # structure with indexing. For example to get the # the second element in the first column: vector_randoms_0_to_10[0,2] = 2 # You can find the shape of a numpy object with vector_zeros.shape() # Lastly you can create arrays with consecutive numbers with np.arange() # I takes a start, an end and an interval as arguments: range_10 = np.arange(0,10,1) range_10_halfsteps = np.arange(0,10,0.5) # The ranges are created including the first and excluding the # last number. 2.2. Useful Numpy functions . In addition to Numpys own data structures it provides a whole range of useful functions that can be used in other contexts as well. One function I probably use more than any other are np.floor(), np.ceil() and np.round(). These all round values. Floor returns the nearest lower integer, ceil the nearest upper integer and round rounds to a desired decimal point: . vector = np.array([1.1, 10.523124, 3.341]) vector_ceiled = np.ceil(vector) vector_floored = np.floor(vector) vector_rounded = np.round(vector, 2) . Numpy also provides some mathematical functions and constants. For example np.pi returns the value of pi, np.e returns Eulers number. Other mathematical operations include all angle computations such as np.sin(), np.cos() etc. These are all computed in radians, but you can turn them into degrees with np.degrees() . Exercise . Lets just do one quick exercise on numpy to get familiar. 1. Create a numpy array from 0 to 20 in steps of 0.1. 2. Compute the sin of the data, then compute the standard deviation of the sin data 3. Add some random noise to the data. To do so, use the np.random.rand(). The range of the noise should be between 0 and 0.5. Then compute the standard deviation of the noisy data. 4. Round the noisy values to 3 decimal places . Solution! vector = np.arange(0,21,.1) sin_vector = np.sin(vector) std_sin_vector = sin_vector.std() noisy_sin_vector = sin_vector + np.random.rand(len(sin_vector))*0.5 std_noisy_sin_vector = noisy_sin_vector.std() rounded_noisy_sin_vector = np.round(noisy_sin_vector, 3) . ",
    "url": "/New_BAI_DataAnalysis/python_2_data_visualization.html#2-a-quick-touch-on-numpy",
    
    "relUrl": "/python_2_data_visualization.html#2-a-quick-touch-on-numpy"
  },"15": {
    "doc": "2. DATA HANDLING AND VISUALIZATION",
    "title": "3. Data Visualization: Plotly",
    "content": "Finally! It is time to not only create endless boring arrays of numbers, but to mold them into beautiful, descriptive images that tell the story of what the data actually means. Because that is essentially what we are doing when plotting data. Nobody can look at a table of 100.000 rows and start talking about it, that is what we can achieve with data visualization. There are several libraries we could use for plotting in Python: . | Matplotlib: One of the most widely used frameworks. It is lightweight, built into Pandas but nobody really likes the syntax | Seaborn: A library built on top of Matplotlib. It makes the syntax quite a bit easier, provides nice out-of-the-box plot styles, but the documentation is a bit lacking and plots are not that easy to customize | Plotly: The solution we will be using here. Plotly is built on a Javascript library Plotly.js and therefor brings some unique features to the table. The syntax and strcuture is quite good to learn, it offers a load of customization. Additionally, it offers very nice interactivity with the plots which makes exploration of your data much easier | . Lucky for us, Plotly is already included in Anaconda, so we do not need to install it. Plotly provides two different approaches to plotting: . | Quick and easy plots with less customization using plotly express | fully fledged figures with full customization options using graphic-objects | . To get a good understanding of Plotly it makes sense to go from large to small, first looking at the general structure of Plotly figures and the way graphic_objects work. If you have a broad overview of these you can still learn about the quick-and-easy ways, but you will have a much easier time when you want to change something about the express solutions manually. ",
    "url": "/New_BAI_DataAnalysis/python_2_data_visualization.html#3-data-visualization-plotly",
    
    "relUrl": "/python_2_data_visualization.html#3-data-visualization-plotly"
  },"16": {
    "doc": "2. DATA HANDLING AND VISUALIZATION",
    "title": "Plotly - The modern plotting library",
    "content": "Where to find help . First of all lets gather some ressources. The two best places to find advice about any plotly-related questions are . | the official documentation at plotly.com | the plotly community forum | as always, Stackoverflow… | . The general structure of Plotly figures . First of all we need to go through a little bit of vocabulary to be able to talk about Plotly. In Plotly-world the whole image of a plot, including the axes, the data, the labels, the title and everything is called the “graph-object”. This is the top-level of every Plotly figure and it is also the name of the Python class, with which we build the plots. Within the graph-object there are two layers: One is the “data” layer with everything that is directly related to the displayed data. That is the data itself, the mode of repesentation in the graph for example the line (in a line-plot) or points (in a scatter-plot) and the styling such as the size or color of the line/points. In plotly, they also call the group of data-related attributes “traces”. Don’t ask me how they came up with it but we have to live with it… We will come back to that later! The second part of the figure is the “layout” layer. It includes everything that makes the graph besides the data itself, for example the axes, the titles on the axes, the title of the graph itself, the legend, colors, maybe a template and so on. In the image below I tried to highlight the areas including the “data” area in red and the “layout” related areas in green: . Lets dive into the code and create a first figure object. Its easy: . # Before we start, lets resample the dwd data down to daily values. # You will create quite some plots and plotting 27 years of 10-minute # data takes a bit of time. df_dwd_daily = df_dwd.resample(\"d\", on=\"data_time\").mean() # First import the graph_objects module from plotly. # We call it \"go\" because that is convention import plotly.graph_objects as go import plotly.io as pio pio.renderers.default = \"browser\" #This will force Plotly to open plots in your default web browser. # Then we create out figure like this: fig = go.Figure() # Check out what happens, when you print this # object with print(fig). You will see the structure # we talked about above! . Well, now we have a graph-object without any data. From printing the figure you can see that the “data” is an empty list. Lets change that and add some data from out dwd-dataset. To do so, we have to add a “trace” (remember how we introduced that above). We do that by calling the .add_trace() method on the figure. In the function the first thing we have to define is, what kind of graph we want to create. Otherwise the empty figure wouldn’t know whether it should become a scatter-plot, a histogram, a line-plot or anything else. We define the type of graph by giving an object of the graph-type we want to the “add_trace()” method. These objects are also included in our “graph-objects” (or “go”). Sounds complicated, but really it is not. Check this out: . # This is the bare figure fig = go.Figure() # Now we will add some data: fig.add_trace( # On fig we call the \"add_trace()\" method go.Scatter( # In the method we provide an object of type \"Scatter\" from \"go\" x = df_dwd_daily.index, # then, in go.Scatter we define, which data should be plotted y = df_dwd_daily[\"tair_2m_mean\"],# on the x- and on the y-axis ) ) # Now print the figure again and look the output # You will see that the \"data\" level now has the x- and y-data in it # Plotly has very nice interactivity. To open the graph # in an interactive browser-window type this: fig.show() # or you save the figure to an image like this: fig.write_image(\"daily_tair_2m_mean.png\") . Above we created a scatter-plot (every data point is a dot in the graph). But if you look at the plot, you’ll note that there is still a lot missing. Most importantly, it does not have axis-labels. We need to add those, so people know what is plotted here! Lets do it. Which part of the figure do you think we need to change to add axis-labels? . SolutionThe \"layout\" bit of the figure So lets see how we can change the layout of the figure! . # to get to the layout of the figure we have two options: # 1. The figure object \"fig\" has the \"layout\" property, # which has an \"xaxis\" property, which again has a \"title\" # property. We can go down this path manually like this: fig.layout.xaxis.title = \"Date\" fig.layout.yaxis.title = \"Tair [F]\" # 2. The second option is to use the \"update_layout() method. # This was was made to make styling more convenient. We can use # it to \"group\" our styling in a single function call. fig.update_layout( xaxis_title=\"Date\", # Note that we use an underscore yaxis_title=\"Tair [F]\" # \"_\" to grab the \"title\" property from \"xaxis\" ) # Now you'll see, that the labels are changed in the figure: fig.show() . This is pretty much the way you can change any attribute that is related to the layout of the figure. The only thing you have to figure out for whatever you want to change in your figure is, where the respective property lies. Is it part of the data or the layout layer? Which sub-layers are there? Sometimes you can figure it out by thinking about it, however you can always refer to the documentation and the hive-mind of the internet. Especially in the beginning you’ll need to google quite a bite, but once you get the hang of it, it is actually quite intuitive. Lets do some more styling. Above we created a scatter-plot. This is a time-series, so maybe a line-plot would be more appropriate… . Exercise 1 . Try to change the style of our plot above to a line-plot. To do so, you need to change the \"mode\" property which is part of the \"data\" layer, or \"trace\". You can change the trace just like we changed the \"layout\" above with a function called \"update_traces(). Challenge: Can you come up with two different ways to change the mode? . Solution! # Option 1: fig.update_traces( mode=\"lines\" ) # Option 2 (which you usually wouldn't use): fig.data[0].mode = \"lines\" # The trick is that we have to write # fig.data[0], because the \"data\" property is # a list! You can see that if you look at the # printed figures \"data\" property, it starts with a \"[\". # The reason is of course that you could plot several # lines within a single plot. This way you could style # them one-by-one. However, generally you use the # \"update_traces()\" method to apply styles that # are used for all plotted data and pass everything # else directly when you create the data with \"add_trace()\" . Exercise 2 . Now lets expand the plot a bit. Add two more lines to the plot, the tair_2m_min and tair_2m_max columns from our dwd data. You can simply add them to the existing plot with the \"add_trace()\" method. When calling add_trace(), try to directly change the mode to \"lines\". When adding the lines, also add the argument \"name\" to the add_trace() method. That defines, how the line will be reprented in the legend. Give appropriate names to the lines. Additionally, try to change the line style of the min and max temperature to \"dashed\". If you want, you can also change the colors of the lines. To do so, change the line_color property. To define the color you can use either a string in the form of \"rgb(0,0,0)\" where you have to replace the zeros with rgb values, or you use one of the pre-defined colors which you can also pass as string. You can find a list of available color-names here: . w3schools list of CSS colors… . Solution! fig = go.Figure() fig.add_trace( go.Scatter( y = df_dwd_daily[\"tair_2m_mean\"], name=\"Tair 2m\", line_color=\"black\" ) ) # after adding the first line we just keep adding # more lines. We can directly change the name, # line_dash and line_color attributes: fig.add_trace( go.Scatter( y = df_dwd_daily[\"tair_2m_min\"], name=\"Tair 2m min\", line_dash=\"dash\", line_color = \"lightblue\" ) ) fig.add_trace( go.Scatter( y = df_dwd_daily[\"tair_2m_max\"], name=\"Tair 2m max\", line_dash=\"dash\", line_color=\"lightcoral\" ) ) fig.update_layout(xaxis_title=\"Date\", yaxis_title=\"T2m [F]\") fig.show() . Great, you are on the best way to becoming a Data-Painting Plotly-Wizard! Of course there are not just simple line and scatter charts. There is a whole world of graphs to explore!. For now, lets look at just one more type of graph, a bar-chart. This is a common type of graph to compare measured amounts (as opposed to discrete values such as a temperature). Such a value would be our rainfall measurement! . Exercise . Go ahead and create a bar chart of the sum of daily rainfall. You can create a bar-chart just like we did with the scatter plots above, only that you call \"go.Bar\" instead of \"go.Scatter\" Remember that when resampling precipitation to daily values, you need to use the sum instead of the mean! . Solution! # First of all we grab daily precipitation data by # resampling with the \"sum\" aggregation function # Here I directly grabbed only the precipitation-column, # but you can also do it another way precip = df_dwd.resample(rule=\"d\", on=\"data_time\").sum()[\"precipitation\"] # Now we create the graph just like before: fig_precip = go.Figure() fig_precip.add_trace( go.Bar( # Here we simply use go.Bar instead of go.Scatter x=precip.index, y=precip # Note: when using a Series instead of a dataframe ) # I dont have to pass the column name, because I ) # only have one column anyways... fig_precip.update_layout( xaxis_title=\"Date\", yaxis_title=\"Rain amount, daily [mm]\" ) fig_precip.show() . Right on, this was quite a deep dive into the Plotly library! But if you followed all the way down here, you are on a very good way to become super proficient in plotting data in python! The skills you got from the exercise above should get you quite far in designing your own figures in the future. If it is all a bit much in the start, don’t worry! As time comes you will do things much faster. For now, keep trying, keep googling, consult the documentation and most importantly: be happy with the progress you make! . Before we finish the visualization exercises I want to show a few more very helpful things. Subplots . Often you want to create not just one but multiple plots in one figure, for example one big figure with a temperature plot on top and a precipitation plot on the bottom. This way readers can easily get an overview of the climate at the station. Creating such a “subplot” in Plotly is super easy! Instead of using go.Figure(), you use a different function to create your top-layer “graph-object”. The function we need is plotly.subplots.make_subplots(). In it we can define the number of rows and columns of figures we want to create with the “rows” and “cols” keywords. Think about the whole figure like a matrix. The figure on the top-left will be row 1, column 1, second on the left row 2, column 1 etc. Then whenever you are adding a new trace, you can define its position with the properties “row” and “col”: . # First we create the subplots graph_object. # To do so we have to import that specific method: from plotly.subplots import make_subplots # Now we create a subplot figure with two rows: fig_subplots = plotly.subplots.make_subplots(rows=2, cols=1) # Now we can start adding traces to the figure: fig_subplots.add_trace( go.Scatter( x=df_dwd_daily.index, y=df_dwd_daily.tair_2m_mean, name=\"Tair mean\", line_color = \"black\" ), row = 1, # here we define, where the figure should be col=1, ) fig_subplots.add_trace( go.Bar( x=precip.index, y=precip, name=\"precip\", marker_color=\"blue\" ), row = 2, # precipitation will be the lower plot col=1 ) fig_subplots.show() . As a little exercise, print the fig_subplots object from above and try to figure out how to change the y-axis titles on the first and the second plot. Solution fig_subplots.update_layout( xaxis2_title=\"Date\", yaxis_title=\"Tair 2m [F]\", yaxis2_title=\"Rain amount, daily [mm]\" ) . Templates . One very nice way to style your figure a bit differently than the default is to use Plotly templates! You can implement them in your figure simply by adding the template in the layout: . fig_subplots.update_layout( template=\"simple-white\" ) fig_subplots.show() . Looks nice right? There is a whole gallery of templates available on the website: Plotly template gallery… . Plotly Express . For now, we will leave it with that. But wait, I was talking about an easier way to create graphs before right? Yes, for “quick and dirty” graphs you can use the awesome plotly.express shortcut. With it you can create a bunch of graphs like the ones we talked about above without all the fuzz of graph_objects etc. All you need to do to create a scatter plot is . import plotly.express as px fig_express = px.scatter(df_dwd_daily, y=\"tair_2m_mean\") fig_express.show() # You can update the fig_express just the same as the output # of go.Figure(), with update_layout and all of its beauty. Plotly express is very well connected with Pandas. This enables you to display all data of a dataframe in one combined, interactive overview char, simply by passing a pandas dataframe into it: . import plotly.express as px fig_express = px.scatter(df_dwd_daily) fig_express.show() . Plotly express includes many other graph types as well. You can find a very nice documentation on the plotly website…. One very last very useful thing I want to mention here is the addition of trendlines in Plotly Express. It is a super handy feature that is only implemented in Plotly Express, not in plain Plotly. I will show you how to do it and how you can grab all information the trendline can give you, but I will not explain the statistics behind it here. It happens very often that you want to plot two variables together, to see whether they are related. In order to explore the relationship, you can fit a linear line to the data and look at the parameters of the line-equation. For example, if I would plot air temperature on the x-axis against the same air-temperature on the y-axis, that would result in a perfect fit, the line would have a slope of exactly 1, the y-axis-intercept would be 0 and the r_square value of the regression would be 1, indicating the perfect fit. In Plotly express you can simply add a trendline to the plot by adding the “trendline” argument to the function call: . import plotly.express as px fig_express = px.scatter(df_dwd_daily, x=\"pressure_air\", y=\"tair_2m_mean\", trendline=\"ols\") results = px.get_trendline_results() print(results) fig_express.show() . But the bare line itself is not too useful, we need to grab the coefficients of the line, so the slope and the intercept of the line-equation. You can grab the results from the figure using px.get_trendline_results(fig_express). Honestly, the actual parameters are a bit hidden inside the deeper objects, but you can extract each one anyways: . import plotly.express as px fig_express = px.scatter(df_dwd_daily, x=\"pressure_air\", y=\"tair_2m_mean\", trendline=\"ols\") results = px.get_trendline_results(fig_express) # To actually access the results of the regression we need to # access the first row of the px_fit_results: ols_results = results.px_fit_results.iloc[0] print(ols_results.summary()) # ols_results is an object of the type \"OLSResults\" # which comes from a different library, \"statsmodels\" (https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLSResults.html) # From it you can grab the coefficients and a bunch of statistical infos: slope = ols_results.params[1] intercept = ols_results.params[0] rsquared = ols_results.rsquared # etc... I won’t go much more into the details because you can easily look up more plotly express functions (and I do encourage you to do so because it is super handy!), but by now you know enough about Plotly to explore that yourself. Why did we go through all the fuzz of handling the traces and layout ourselves? Because express is limited! If you want to customize your plots in some way it does not support out-of-the-box, you will have to dive into the figure-structure sooner or later, and now you know how. Still it is encouraged (even by Plotly themselves) to make use of both: run plotly express for a base figure or for quick data exploration, and then style the figure the way you want with the in-depth methods. ",
    "url": "/New_BAI_DataAnalysis/python_2_data_visualization.html#plotly---the-modern-plotting-library",
    
    "relUrl": "/python_2_data_visualization.html#plotly---the-modern-plotting-library"
  },"17": {
    "doc": "2. DATA HANDLING AND VISUALIZATION",
    "title": "Congratulations",
    "content": "on finishing this plot about plotting plots with Plotly! What a journey. I hope you can take away the knowledge to navigate plotting in Python fairly well from now on! Enjoy your future data-adventures and I wish you happy coding! . ",
    "url": "/New_BAI_DataAnalysis/python_2_data_visualization.html#congratulations",
    
    "relUrl": "/python_2_data_visualization.html#congratulations"
  },"18": {
    "doc": "2. DATA HANDLING AND VISUALIZATION",
    "title": "2. DATA HANDLING AND VISUALIZATION",
    "content": " ",
    "url": "/New_BAI_DataAnalysis/python_2_data_visualization.html",
    
    "relUrl": "/python_2_data_visualization.html"
  },"19": {
    "doc": "3. REGRESSION",
    "title": "Regression",
    "content": " ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#regression",
    
    "relUrl": "/python_3_regression.html#regression"
  },"20": {
    "doc": "3. REGRESSION",
    "title": "Table of Contents",
    "content": ". | What’s Regression All About? | Simple Linear Regression | Multiple Regression | Machine Learning with Random Forests | Gap-filling in Time Series | . Welcome! This tutorial will walk you through regression analysis - one of the most useful tools you’ll encounter for making sense of ecological data. We’ll start from the basics and work our way up to more advanced machine learning methods. Don’t worry if statistics isn’t your strong suit. We’ll take it step by step, and by the end you should feel comfortable applying these techniques to your own data. ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#table-of-contents",
    
    "relUrl": "/python_3_regression.html#table-of-contents"
  },"21": {
    "doc": "3. REGRESSION",
    "title": "1. What’s Regression All About?",
    "content": " ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#1-whats-regression-all-about",
    
    "relUrl": "/python_3_regression.html#1-whats-regression-all-about"
  },"22": {
    "doc": "3. REGRESSION",
    "title": "The Basic Idea",
    "content": "Here’s the thing: as ecologists, we’re constantly trying to figure out what drives the patterns we observe. Why are there more species in some places than others? What makes trees grow faster? How does temperature affect animal behavior? . Regression gives us a way to quantify these relationships. Instead of just saying “warmer temperatures seem to increase growth,” we can say “for every 1°C increase in temperature, tree ring width increases by 0.15 mm.” That’s powerful stuff. At its core, regression asks: how does one thing change when another thing changes? . ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#the-basic-idea",
    
    "relUrl": "/python_3_regression.html#the-basic-idea"
  },"23": {
    "doc": "3. REGRESSION",
    "title": "A Quick Example",
    "content": "Let’s say you’re studying tree growth across a temperature gradient. You measure tree ring widths at different sites: . | Mean Annual Temperature (°C) | Tree Ring Width (mm) | . | 8 | 1.2 | . | 10 | 1.8 | . | 12 | 2.4 | . | 14 | 2.9 | . | 16 | 3.2 | . You can see there’s a pattern - warmer sites have wider rings. But how strong is this relationship? Can we predict growth at a site with 11°C mean temperature? Regression helps us answer these questions. ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#a-quick-example",
    
    "relUrl": "/python_3_regression.html#a-quick-example"
  },"24": {
    "doc": "3. REGRESSION",
    "title": "What Can Regression Achieve?",
    "content": "In ecological research, regression is useful for: . Making predictions - You’ve measured carbon flux at 20 sites, but you want to estimate it across the whole landscape. Regression lets you predict values at unmeasured locations based on environmental variables you can get from satellite data. Understanding relationships - Does nitrogen addition actually increase plant biomass? By how much? Is the effect statistically significant or could it just be noise? . Figuring out what matters - When you have 15 environmental variables that might explain species richness, regression helps you sort out which ones are actually important. Supporting management decisions - If you know how much habitat area affects population size, you can make informed recommendations about reserve design. ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#what-can-regression-achieve",
    
    "relUrl": "/python_3_regression.html#what-can-regression-achieve"
  },"25": {
    "doc": "3. REGRESSION",
    "title": "Terminology",
    "content": "Before we dive in, let’s get our vocabulary straight. Different fields use different terms for the same things, which can be confusing. Target variable . | You can also call it response variable, dependent variable, outcome | The variable you’re trying to predict or explain | We usually call it y | Examples: species richness, biomass, survival rate, carbon flux | . independent variables . | Can also be termed predictors, features, independent variables, explanatory variables | The variable you use to make predictions or explain target variables | We call these x (or x₁, x₂, etc. when there are several) | Examples: temperature, precipitation, soil pH, elevation | . The model: . | This is the mathematical equation that describes how x relates to y | General form: y = f(x) + error | The “error” part is important - it acknowledges that our model won’t be perfect | . Coefficients: . | These are the numbers in our model that define the relationship | In a simple model like y = 3 + 2x, the “3” is the intercept and “2” is the slope | We estimate these from our data | . Residuals: . | The difference between what we observed and what our model predicted | Small residuals = good model fit | Patterns in residuals = something’s wrong with our model | . ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#terminology",
    
    "relUrl": "/python_3_regression.html#terminology"
  },"26": {
    "doc": "3. REGRESSION",
    "title": "How Does Regression Actually Work?",
    "content": "The basic process goes like this: . | Choose a model type. Are you assuming a straight line relationship? A curve? Multiple predictors? . | Fit the model to your data. This means finding the coefficient values that make your predictions as close to the observations as possible. | Check if it worked. Look at how well the model fits, whether the assumptions are met, and whether the results make ecological sense. | . The most common approach for step 2 is called “least squares” - we find the coefficients that minimize the sum of squared differences between observed and predicted values. We square the differences so that positive and negative errors don’t cancel out. ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#how-does-regression-actually-work",
    
    "relUrl": "/python_3_regression.html#how-does-regression-actually-work"
  },"27": {
    "doc": "3. REGRESSION",
    "title": "Evaluating Your Model",
    "content": "How do you know if your model is any good? A few key metrics: . R² (R-squared): This tells you what fraction of the variation in your data is explained by the model. An R² of 0.7 means your model explains 70% of the variance. What’s “good” depends entirely on your system - in controlled experiments 0.9 might be expected, while in field ecology 0.3 might be excellent. $$ R^2 = 1 - \\frac{SS_{res}}{SS_{tot}} = 1 - \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n}(y_i - \\bar{y})^2} $$ Where: . $y_i$ is the observed value . $\\hat{y}_i$ is the predicted value . $\\bar{y}$ is the mean of observed values . $SS_{res}$ is the sum of squared residuals . $SS_{tot}$ is the total sum of squares . RMSE (Root Mean Square Error): This is the average size of your prediction errors, in the same units as your response variable. An RMSE of 2.5°C for a temperature model means your predictions are typically off by about 2.5 degrees. $$ RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2} $$ MAE (Mean Absolute Error): Similar to RMSE but less sensitive to outliers. Useful when you have some weird extreme values in your data. $$ MAE = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y}_i| $$ The difference between RMSE and MAE? RMSE penalizes large errors more heavily because of the squaring. If you have a few really bad predictions, RMSE will be much higher than MAE. This can be useful for detecting outliers or problematic predictions. ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#evaluating-your-model",
    
    "relUrl": "/python_3_regression.html#evaluating-your-model"
  },"28": {
    "doc": "3. REGRESSION",
    "title": "2. Simple Linear Regression",
    "content": "Alright, let’s actually do some regression. We’ll start with the simplest case: one predictor, one response, straight line relationship. ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#2-simple-linear-regression",
    
    "relUrl": "/python_3_regression.html#2-simple-linear-regression"
  },"29": {
    "doc": "3. REGRESSION",
    "title": "The Model",
    "content": "Simple linear regression fits this equation: . ŷ = β₀ + β₁x . Where: . | ŷ (y-hat) is our predicted value of the response variable | β₀ (beta-zero) is the intercept (value of y when x is zero) | β₁ (beta-one) is the slope (how much y changes for each unit increase in x) | x is our predictor variable | . That’s it. We’re just fitting a line through our data points. What’s Actually Happening? . When we fit a regression, we’re looking for the line that minimizes the total squared distance between our observed data points and the line. These distances are called residuals - the difference between what we actually observed and what our model predicted. The full model, including the error, is: . y = β₀ + β₁x + ε . Where ε (epsilon) represents the residual error - all the variation in y that our model doesn’t capture. In a perfect world with a perfect model, ε would be zero. In reality, it never is. Why squared distances? Squaring does two things: (1) it treats positive and negative errors equally (a prediction 10g too high is just as bad as 10g too low), and (2) it penalizes large errors more heavily than small ones. This method is called Ordinary Least Squares (OLS). ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#the-model",
    
    "relUrl": "/python_3_regression.html#the-model"
  },"30": {
    "doc": "3. REGRESSION",
    "title": "Let’s Try It: Penguin Body Mass and Flipper Length",
    "content": "We’ll use the Palmer Penguins dataset - real measurements collected by Dr. Kristen Gorman at Palmer Station, Antarctica. import numpy as np import pandas as pd import plotly.express as px import plotly.graph_objects as go # Load the penguins dataset # You can install it with: pip install palmerpenguins from palmerpenguins import load_penguins penguins = load_penguins() # Take a look at what we have print(\"Dataset shape:\", penguins.shape) print(\"\\nFirst few rows:\") print(penguins.head()) # Check for missing values and drop them for now penguins_clean = penguins.dropna(subset=['flipper_length_mm', 'body_mass_g']) print(f\"\\nComplete cases: {len(penguins_clean)}\") . Now let’s explore the relationship between flipper length and body mass: . # Quick visualization fig = px.scatter(penguins_clean, x='flipper_length_mm', y='body_mass_g', color='species', title='Penguin Body Mass vs Flipper Length') fig.update_layout(template='simple_white', font_size = 36,) fig.show() . You’ll see there’s clearly a positive relationship - longer flippers go with heavier birds. Let’s quantify it. Fitting the Regression . from sklearn.linear_model import LinearRegression # Prepare the data # sklearn expects X to be a 2D array (rows = samples, columns = features) # Even with one feature, we need shape (n_samples, 1), not (n_samples,) # That's why we use [['flipper_length_mm']] (double brackets) instead of ['flipper_length_mm'] X = penguins_clean[['flipper_length_mm']].values y = penguins_clean['body_mass_g'].values # Fit the model model = LinearRegression() model.fit(X, y) print(f\"Slope: {model.coef_[0]:.2f} g per mm\") print(f\"Intercept: {model.intercept_:.2f} g\") print(f\"R-squared: {model.score(X, y):.3f}\") . Visualizing the Fit . # Get predictions for the regression line # We create a sequence of x values spanning our data range # reshape(-1, 1) converts the 1D array to 2D (required by sklearn) X_line = np.linspace(penguins_clean['flipper_length_mm'].min(), penguins_clean['flipper_length_mm'].max(), 100).reshape(-1, 1) y_line = model.predict(X_line) fig = go.Figure() # Data points fig.add_trace(go.Scatter( x=penguins_clean['flipper_length_mm'], y=penguins_clean['body_mass_g'], mode='markers', name='Observations', marker=dict(size=24, opacity=0.6) )) # Regression line fig.add_trace(go.Scatter( x=X_line.flatten(), y=y_line, mode='lines', name='Regression line', line=dict(color='red', width=2) )) fig.update_layout( title='Penguin Body Mass vs Flipper Length', xaxis_title='Flipper Length (mm)', yaxis_title='Body Mass (g)', template='simple_white', font_size = 36 ) fig.show() . ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#lets-try-it-penguin-body-mass-and-flipper-length",
    
    "relUrl": "/python_3_regression.html#lets-try-it-penguin-body-mass-and-flipper-length"
  },"31": {
    "doc": "3. REGRESSION",
    "title": "What Do These Numbers Mean?",
    "content": "With real data, you should get something like: . | Slope ≈ 49.7 g/mm: For every 1 mm increase in flipper length, body mass increases by about 50 grams. This is our β₁. | Intercept ≈ -5781 g: This would be the predicted mass at flipper length = 0, which makes no biological sense (negative mass!), but it’s needed mathematically to position the line correctly within the range of our actual data. This is our β₀. | R² = 0.76 means flipper length explains about 76% of the variation in body mass. The remaining 24% is unexplained variation (our ε). | . Making Predictions . # What's the predicted mass for a penguin with 200mm flippers? new_flipper = np.array([[200]]) # Note: 2D array for sklearn predicted_mass = model.predict(new_flipper) print(f\"Predicted mass for 200mm flipper: {predicted_mass[0]:.0f} g\") # What about 180mm? new_flipper = np.array([[180]]) predicted_mass = model.predict(new_flipper) print(f\"Predicted mass for 180mm flipper: {predicted_mass[0]:.0f} g\") . A Note on Extrapolation . Be careful about predicting outside the range of your training data! Our model was built on penguins with flippers roughly 170-230mm. If you try to predict mass for a 100mm flipper or a 300mm flipper, you’re extrapolating - assuming the linear relationship continues outside the observed range. This is often dangerous because: . | Relationships may be non-linear at extremes | You have no data to validate predictions in that range | Biological constraints may make extrapolations impossible (like our negative-mass intercept) | . Stick to interpolation (predicting within your data range) whenever possible. Try It Yourself . Using the Palmer Penguins dataset, fit a simple regression predicting bill length from bill depth. What do you find? Is the relationship positive or negative? How does R² compare to the flipper-mass relationship? . Solution! from palmerpenguins import load_penguins from sklearn.linear_model import LinearRegression penguins = load_penguins().dropna(subset=['bill_length_mm', 'bill_depth_mm']) X = penguins[['bill_depth_mm']].values y = penguins['bill_length_mm'].values model = LinearRegression() model.fit(X, y) print(f\"Slope: {model.coef_[0]:.3f}\") print(f\"Intercept: {model.intercept_:.3f}\") print(f\"R-squared: {model.score(X, y):.3f}\") # Surprise! You'll find a NEGATIVE relationship and very low R² # This is Simpson's paradox - when you combine the species, # the overall trend is negative, but within each species # the relationship is positive. This is because Gentoo penguins # have long bills but shallow depth, while Adelie have # shorter bills but deeper depth. # Try it by species: for species in penguins['species'].unique(): subset = penguins[penguins['species'] == species] X_sp = subset[['bill_depth_mm']].values y_sp = subset['bill_length_mm'].values model_sp = LinearRegression().fit(X_sp, y_sp) print(f\"{species}: slope = {model_sp.coef_[0]:.2f}, R² = {model_sp.score(X_sp, y_sp):.3f}\") . ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#what-do-these-numbers-mean",
    
    "relUrl": "/python_3_regression.html#what-do-these-numbers-mean"
  },"32": {
    "doc": "3. REGRESSION",
    "title": "Limitation: Simpson’s Paradox",
    "content": "The exercise above reveals something important: simple regression can be misleading when you have groups in your data. This phenomenon is called Simpson’s Paradox - when a trend appears in several groups of data but disappears or reverses when the groups are combined. In our case: . | Within each species: deeper bills → longer bills (positive relationship) | Across all species combined: deeper bills → shorter bills (negative relationship!) | . How can this be? It’s because species differ systematically in both variables. Gentoo penguins have long but shallow bills; Adelie penguins have shorter but deeper bills. When you ignore species, these group differences create an artificial negative trend. The flipper-mass relationship also differs among species - Gentoo penguins are bigger than Adelie and Chinstrap overall. This is one reason we need multiple regression - to account for additional factors that might be confounding our results. ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#limitation-simpsons-paradox",
    
    "relUrl": "/python_3_regression.html#limitation-simpsons-paradox"
  },"33": {
    "doc": "3. REGRESSION",
    "title": "3. Multiple Regression",
    "content": "In the real world, ecological responses depend on many factors simultaneously. Multiple regression lets us include all of them in one model. ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#3-multiple-regression",
    
    "relUrl": "/python_3_regression.html#3-multiple-regression"
  },"34": {
    "doc": "3. REGRESSION",
    "title": "Why Go Multiple?",
    "content": "Looking at the penguin data, body mass depends on more than just flipper length: . | Species differ in overall body size | Males are larger than females | Bill dimensions correlate with mass too | . If we only model mass against flipper length, we’re missing important information. Multiple regression lets us ask: “What’s the effect of flipper length, after accounting for species and sex?” . This is fundamentally different from simple regression. We’re no longer asking “how does y change with x?” but rather “how does y change with x₁, holding x₂, x₃, etc. constant?” . ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#why-go-multiple",
    
    "relUrl": "/python_3_regression.html#why-go-multiple"
  },"35": {
    "doc": "3. REGRESSION",
    "title": "The Model",
    "content": "We extend our simple model to include multiple predictors: . ŷ = β₀ + β₁x₁ + β₂x₂ + β₃x₃ + … . Each coefficient now tells us the partial effect of that variable - its effect while holding the others constant. This is crucial for interpretation and is what makes multiple regression so powerful for teasing apart confounded relationships. ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#the-model-1",
    
    "relUrl": "/python_3_regression.html#the-model-1"
  },"36": {
    "doc": "3. REGRESSION",
    "title": "Example: Predicting Penguin Body Mass",
    "content": "Let’s build a more complete model for penguin body mass. from palmerpenguins import load_penguins from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.preprocessing import LabelEncoder import sklearn.metrics as metrics # Load and prepare data penguins = load_penguins().dropna() # Encode categorical variables (species and sex) as numbers # LabelEncoder converts text categories to integers: 0, 1, 2, etc. le_species = LabelEncoder() le_sex = LabelEncoder() penguins['species_code'] = le_species.fit_transform(penguins['species']) penguins['sex_code'] = le_sex.fit_transform(penguins['sex']) print(\"Species encoding:\", dict(zip(le_species.classes_, range(len(le_species.classes_))))) print(\"Sex encoding:\", dict(zip(le_sex.classes_, range(len(le_sex.classes_))))) # Check what we have print(f\"\\nDataset: {len(penguins)} penguins\") print(penguins[['species', 'sex', 'flipper_length_mm', 'bill_length_mm', 'bill_depth_mm', 'body_mass_g']].head()) . A Note on Encoding Categorical Variables . We just converted species (Adelie, Chinstrap, Gentoo) to numbers (0, 1, 2). This is called label encoding and it has a subtle problem: it implies an ordering. The model might think Gentoo (2) is “more” than Adelie (0) in some way. For binary variables like sex (male/female → 0/1), this is fine - we’re just measuring the difference between two groups. For multi-category variables, a better approach is one-hot encoding (also called dummy variables), where each category gets its own 0/1 column. We’ll keep label encoding here for simplicity, but be aware this is a simplification. In a real analysis, you’d want to use one-hot encoding or a statistical framework that handles categories properly. Check Correlations First . Before building a model, it’s good practice to explore how your variables relate to each other: . # Which variables correlate with body mass? numeric_cols = ['flipper_length_mm', 'bill_length_mm', 'bill_depth_mm', 'body_mass_g', 'species_code', 'sex_code'] print(\"\\nCorrelations with body mass:\") print(penguins[numeric_cols].corr()['body_mass_g'].sort_values(ascending=False)) . Why check correlations? . | It gives you a preview of which variables might be useful predictors | It reveals potential multicollinearity - when predictors are highly correlated with each other | . Multicollinearity is a problem because if two predictors are highly correlated, it becomes hard to separate their individual effects. The model can’t tell if it’s variable A or variable B causing the effect. You’ll get unstable coefficient estimates. We’ll keep an eye on this. Why Split Into Training and Test Sets? . This is a fundamental concept in predictive modeling: . # Prepare features and target X = penguins[['flipper_length_mm', 'bill_length_mm', 'bill_depth_mm', 'species_code', 'sex_code']] y = penguins['body_mass_g'] # Split into training and test sets X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=42 ) print(f\"Training set: {len(X_train)} penguins\") print(f\"Test set: {len(X_test)} penguins\") . Why do we split the data? . If we train our model on all the data and then evaluate it on the same data, we’re essentially asking “how well did you memorize this?” A model could achieve a perfect score by memorizing every data point without learning any real patterns. What we actually want to know is: “how well will this model perform on new, unseen data?” . By holding out a test set that the model never sees during training, we can get an honest estimate of how well the model will generalize to new penguins. | Training set (70%): Used to fit the model | Test set (30%): Used only for evaluation, never for fitting | . The random_state=42 ensures we get the same split every time we run the code (for reproducibility). Fitting the Multiple Regression . # Fit the model model = LinearRegression() model.fit(X_train, y_train) # Look at coefficients print(\"\\nModel coefficients:\") print(f\" Intercept: {model.intercept_:.1f}\") for name, coef in zip(X.columns, model.coef_): print(f\" {name}: {coef:.2f}\") . ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#example-predicting-penguin-body-mass",
    
    "relUrl": "/python_3_regression.html#example-predicting-penguin-body-mass"
  },"37": {
    "doc": "3. REGRESSION",
    "title": "Interpreting the Results",
    "content": "What do these coefficients actually mean? This is where multiple regression differs fundamentally from simple regression. flipper_length_mm: Each additional mm of flipper length adds about 17g to body mass, after controlling for other variables. Note this is smaller than in simple regression (~50g). Why? Because some of that apparent flipper effect was actually due to species differences—Gentoo penguins have both longer flippers AND higher mass. Once we account for species, the “pure” flipper effect is smaller. bill_length_mm: Longer bills are associated with slightly higher mass, holding other variables constant. bill_depth_mm: Deeper bills are associated with higher mass. This makes sense—it’s a measure of overall head size. species_code: The coefficient shows average difference between species (encoded as 0, 1, 2). Interpretation is tricky with encoded categories because we’re treating it as a continuous variable. This is a limitation of our simple encoding approach. sex_code: Males (coded as 1) are heavier than females (coded as 0) by about this many grams, controlling for body measurements. This is the easiest to interpret—it’s the male-female difference in mass after accounting for differences in flipper length, bill size, etc. The Key Insight: “Controlling For” Other Variables . The phrase “controlling for” or “holding constant” is crucial in multiple regression. Here’s what it means: . Imagine you could magically find two penguins that are: . | The same species | The same sex | Have the same bill length | Have the same bill depth | But differ in flipper length by 1mm | . The flipper coefficient tells you how much heavier we’d expect the longer-flippered penguin to be. Of course, we can’t actually find such perfectly matched penguins. Multiple regression does this statistically by partitioning the variation in body mass among all the predictors. How Good Is Our Model? . # Predict on test data (data the model has never seen) y_pred = model.predict(X_test) # Calculate metrics r2 = metrics.r2_score(y_test, y_pred) rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred)) mae = metrics.mean_absolute_error(y_test, y_pred) print(f\"R-squared: {r2:.3f}\") print(f\"RMSE: {rmse:.1f} g\") print(f\"MAE: {mae:.1f} g\") . Understanding the Evaluation Metrics: . | R² (R-squared): Same interpretation as before - proportion of variance explained. But now we’re measuring it on the test set, so it tells us how well the model generalizes. | RMSE (Root Mean Squared Error): The square root of the average squared prediction error. It’s in the same units as your response variable (grams), so you can interpret it directly: “on average, our predictions are off by about RMSE grams.” RMSE penalizes large errors heavily because of the squaring. | MAE (Mean Absolute Error): The average absolute prediction error. Also in grams. MAE treats all errors equally regardless of size. If RMSE is much larger than MAE, you have some predictions with large errors. | . # Plot predicted vs observed fig = px.scatter(x=y_test, y=y_pred, labels={'x': 'Observed Mass (g)', 'y': 'Predicted Mass (g)'}) fig.add_scatter(x=[y_test.min(), y_test.max()], y=[y_test.min(), y_test.max()], mode='lines', name='1:1 line', line=dict(dash='dash', color='red')) fig.update_layout(template='simple_white', title=f'Multiple Regression: R² = {r2:.3f}', font_size = 36) fig.update_traces(marker_size = 24) fig.show() . Interpreting the predicted vs. observed plot: If predictions were perfect, all points would fall exactly on the red 1:1 line. The scatter around that line shows prediction error. Look for patterns - if errors are larger for heavy penguins than light ones, your model might have heteroscedasticity issues. ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#interpreting-the-results",
    
    "relUrl": "/python_3_regression.html#interpreting-the-results"
  },"38": {
    "doc": "3. REGRESSION",
    "title": "Does Adding Variables Help?",
    "content": "Let’s compare models with different numbers of predictors: . results = [] # Just flipper length m1 = LinearRegression().fit(X_train[['flipper_length_mm']], y_train) results.append({ 'Model': 'Flipper only', 'R²': m1.score(X_test[['flipper_length_mm']], y_test) }) # Flipper + species m2 = LinearRegression().fit(X_train[['flipper_length_mm', 'species_code']], y_train) results.append({ 'Model': 'Flipper + Species', 'R²': m2.score(X_test[['flipper_length_mm', 'species_code']], y_test) }) # Flipper + species + sex m3 = LinearRegression().fit(X_train[['flipper_length_mm', 'species_code', 'sex_code']], y_train) results.append({ 'Model': 'Flipper + Species + Sex', 'R²': m3.score(X_test[['flipper_length_mm', 'species_code', 'sex_code']], y_test) }) # All predictors m4 = LinearRegression().fit(X_train, y_train) results.append({ 'Model': 'All predictors', 'R²': m4.score(X_test, y_test) }) print(pd.DataFrame(results)) . You should see R² improve as you add relevant predictors - species and sex both contribute meaningful information about body mass. But be careful! R² will always increase (or stay the same) when you add more predictors to a model, even if those predictors are useless. This is because more parameters give the model more flexibility to fit the training data. This is why we evaluate on a test set - on new data, useless predictors will actually hurt performance by adding noise. This is called overfitting, and it’s a central concern in machine learning. Exercise . Build a multiple regression model predicting bill length from bill depth, flipper length, species, and sex. Which predictors have the strongest effects? Does the bill depth coefficient change from the simple regression? . Solution! from palmerpenguins import load_penguins from sklearn.linear_model import LinearRegression from sklearn.model_selection import train_test_split from sklearn.preprocessing import LabelEncoder penguins = load_penguins().dropna() # Encode categoricals le_species = LabelEncoder() le_sex = LabelEncoder() penguins['species_code'] = le_species.fit_transform(penguins['species']) penguins['sex_code'] = le_sex.fit_transform(penguins['sex']) # Prepare data X = penguins[['bill_depth_mm', 'flipper_length_mm', 'species_code', 'sex_code']] y = penguins['bill_length_mm'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Full model model = LinearRegression() model.fit(X_train, y_train) print(f\"R²: {model.score(X_test, y_test):.3f}\\n\") print(\"Coefficients:\") for name, coef in zip(X.columns, model.coef_): print(f\" {name}: {coef:.3f}\") # Compare to simple regression simple = LinearRegression() simple.fit(penguins[['bill_depth_mm']], penguins['bill_length_mm']) print(f\"\\nSimple regression bill_depth coefficient: {simple.coef_[0]:.3f}\") print(f\"Multiple regression bill_depth coefficient: {model.coef_[0]:.3f}\") # The bill_depth coefficient changes dramatically - in simple regression # it's negative (Simpson's paradox), but in multiple regression # controlling for species, it becomes positive as expected. ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#does-adding-variables-help",
    
    "relUrl": "/python_3_regression.html#does-adding-variables-help"
  },"39": {
    "doc": "3. REGRESSION",
    "title": "Limitations of Multiple Regression",
    "content": "Multiple regression is powerful but has some important limitations: . 1. It assumes linear relationships. The model assumes each predictor has a straight-line relationship with the response. If flipper length and mass have a curved relationship (they often do at extremes), a linear model won’t capture that properly. You might need polynomial terms or transformations. 2. It assumes additive effects. The model says the effect of flipper length is the same for all species - we just shift the intercept for each species. In reality, species might differ in their flipper-mass scaling (different slopes). This would require interaction terms. 3. It assumes independent errors. If you measured the same penguin multiple times, or penguins from the same colony are more similar, you violate the independence assumption. You’d need mixed-effects models for such data. 4. Outliers can have outsized influence. A single unusual data point can dramatically shift your regression line. Always check for influential observations. These limitations bring us to machine learning approaches, which can handle more complexity. ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#limitations-of-multiple-regression",
    
    "relUrl": "/python_3_regression.html#limitations-of-multiple-regression"
  },"40": {
    "doc": "3. REGRESSION",
    "title": "4. Machine Learning with Random Forests",
    "content": "Alright, now we’re getting to the fun stuff. Machine learning sounds fancy, but the basic idea is simple: let the algorithm figure out the patterns in your data, rather than you specifying them in advance. ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#4-machine-learning-with-random-forests",
    
    "relUrl": "/python_3_regression.html#4-machine-learning-with-random-forests"
  },"41": {
    "doc": "3. REGRESSION",
    "title": "Why Machine Learning?",
    "content": "Ecological relationships are often messy: . | Relationships might be non-linear (e.g., growth rates that plateau) | Effects of one variable depend on another (interactions) | There might be thresholds we didn’t anticipate | The functional form might be completely unknown | . Machine learning algorithms can discover these patterns automatically. You don’t have to know the shape of the relationship beforehand. The trade-off: Machine learning models are often less interpretable than linear regression. You might get better predictions but less insight into why. This is sometimes called the “black box” problem. ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#why-machine-learning",
    
    "relUrl": "/python_3_regression.html#why-machine-learning"
  },"42": {
    "doc": "3. REGRESSION",
    "title": "Decision Trees: The Building Block",
    "content": "Before we get to Random Forests, we need to understand decision trees. They’re surprisingly intuitive. The Basic Idea . A decision tree is basically a flowchart of yes/no questions: . Is flipper length &gt; 206mm? ├── Yes → Probably a Gentoo, predict ~5000g └── No → Is bill depth &gt; 18mm? ├── Yes → Probably Adelie, predict ~3700g └── No → Probably Chinstrap, predict ~3500g . The algorithm figures out: . | Which questions to ask (which variable to split on) | What thresholds to use (why 206mm and not 200mm?) | When to stop asking questions | . How Does It Choose Splits? . At each step, the algorithm tries every possible split of every variable and picks the one that creates the most “pure” groups - groups where the response values are most similar to each other. For regression, “purity” is measured by variance. A good split creates child nodes with lower variance in the response than the parent node. The Goal: Reduce Variance . Before any split, a node contains samples with some variance in the target variable. The tree wants to split these samples into two groups where: . | Group 1 (left child): samples are similar to each other | Group 2 (right child): samples are similar to each other | . Even if the two groups have very different means, that’s fine, what matters is that within each group, values are more similar than before. The Algorithm . For every possible split (every feature × every threshold): . | Divide samples into left and right groups based on the split | Calculate the weighted average variance of the two groups | Pick the split that gives the lowest weighted variance | . Mathematically, we want to minimize: . Cost = (n_left / n_total) × MSE_left + (n_right / n_total) × MSE_right . Where MSE (Mean Squared Error) measures variance: . MSE = (1/n) × Σ(yᵢ - ȳ)² . from sklearn.tree import DecisionTreeRegressor, plot_tree import matplotlib.pyplot as plt # Using our penguin data X = penguins[['flipper_length_mm', 'bill_length_mm', 'bill_depth_mm', 'species_code', 'sex_code']] y = penguins['body_mass_g'] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Fit a simple tree # max_depth=3 limits the tree to 3 levels of questions tree = DecisionTreeRegressor(max_depth=3, random_state=42) tree.fit(X_train, y_train) # Visualize it plt.figure(figsize=(16, 8)) plot_tree(tree, feature_names=X.columns, filled=True, rounded=True, fontsize=9) plt.title('Decision Tree for Penguin Body Mass') plt.tight_layout() plt.show() print(f\"Decision Tree R²: {tree.score(X_test, y_test):.3f}\") . Reading the Tree Visualization . Each box in the tree visualization shows: . | The splitting rule (e.g., “flipper_length_mm &lt;= 206.5”) | samples: how many training samples reached this node | value: the predicted value (mean of samples at this node) | squared_error (or mse): the variance of samples at this node | . The colors indicate the predicted value - similar colors mean similar predictions. The Overfitting Problem . Decision trees are easy to interpret - you can literally see the rules. But they have a critical flaw: they tend to overfit. A deep tree can grow until each leaf contains just one sample, essentially memorizing the training data perfectly. But this memorization doesn’t generalize - the model fails on new data because it learned noise, not signal. Try this experiment: . # A tree with no depth limit deep_tree = DecisionTreeRegressor(random_state=42) # No max_depth deep_tree.fit(X_train, y_train) print(f\"Deep tree - Training R²: {deep_tree.score(X_train, y_train):.3f}\") print(f\"Deep tree - Test R²: {deep_tree.score(X_test, y_test):.3f}\") . You’ll likely see near-perfect training R² but worse test R² - classic overfitting! . ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#decision-trees-the-building-block",
    
    "relUrl": "/python_3_regression.html#decision-trees-the-building-block"
  },"43": {
    "doc": "3. REGRESSION",
    "title": "Random Forests: Many Trees Are Better Than One",
    "content": "Random Forests solve the overfitting problem through a clever strategy: build many trees and average their predictions. Individual trees might make mistakes, but their errors tend to cancel out. How It Works . Each tree in the forest is built differently: . | Bootstrap sampling: Each tree is trained on a random sample of the data, drawn with replacement (some observations appear multiple times, others not at all). This is called a “bootstrap sample.” . | Random feature selection: At each split, instead of considering all variables, only a random subset is considered. This prevents all trees from making identical decisions. | . This randomness means individual trees are “weaker” (less accurate) than a fully-grown single tree. But their collective wisdom is stronger and more robust. The magic: When you average many imperfect but different predictions, the random errors cancel out, while the true signal remains. from sklearn.ensemble import RandomForestRegressor # Fit a random forest rf = RandomForestRegressor( n_estimators=100, # number of trees in the forest max_depth=10, # how deep each tree can go min_samples_leaf=5, # minimum samples required at each leaf random_state=42 # for reproducibility ) rf.fit(X_train, y_train) # Evaluate y_pred = rf.predict(X_test) print(f\"Random Forest R²: {metrics.r2_score(y_test, y_pred):.3f}\") print(f\"Random Forest RMSE: {np.sqrt(metrics.mean_squared_error(y_test, y_pred)):.1f} g\") . Key Hyperparameters . Hyperparameters are settings you choose before training (unlike model parameters like coefficients, which are learned from data): . | n_estimators: Number of trees. More trees = more stable predictions, but slower training. 100-500 is usually enough; returns diminish beyond that. | max_depth: Maximum depth of each tree. Shallower trees are simpler and less prone to overfitting. Try 5-20 for most problems. | min_samples_leaf: Minimum samples required at each leaf node. Higher values prevent the tree from creating leaves with just one or two samples, reducing overfitting. | max_features: Number of features to consider at each split. Default is sqrt(n_features) for classification, n_features/3 for regression. Lower values increase randomness between trees. | . Out-of-Bag Error: Free Cross-Validation . Here’s a neat trick: because each tree only sees about 63% of the data (due to bootstrap sampling), the remaining 37% can be used to evaluate that tree. This “out-of-bag” (OOB) error gives you a built-in estimate of test performance without needing a separate test set! . rf_oob = RandomForestRegressor( n_estimators=100, max_depth=10, oob_score=True, # Enable OOB scoring random_state=42 ) rf_oob.fit(X_train, y_train) print(f\"OOB R²: {rf_oob.oob_score_:.3f}\") print(f\"Test R²: {rf_oob.score(X_test, y_test):.3f}\") . The OOB score should be close to your test score - it’s a good sanity check. ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#random-forests-many-trees-are-better-than-one",
    
    "relUrl": "/python_3_regression.html#random-forests-many-trees-are-better-than-one"
  },"44": {
    "doc": "3. REGRESSION",
    "title": "Comparing All Our Methods",
    "content": "Let’s see how everything stacks up on the penguin data: . results = [] # Simple regression simple = LinearRegression() simple.fit(X_train[['flipper_length_mm']], y_train) pred = simple.predict(X_test[['flipper_length_mm']]) results.append({ 'Method': 'Simple regression', 'R²': metrics.r2_score(y_test, pred), 'RMSE': np.sqrt(metrics.mean_squared_error(y_test, pred)) }) # Multiple regression multi = LinearRegression() multi.fit(X_train, y_train) pred = multi.predict(X_test) results.append({ 'Method': 'Multiple regression', 'R²': metrics.r2_score(y_test, pred), 'RMSE': np.sqrt(metrics.mean_squared_error(y_test, pred)) }) # Decision tree tree = DecisionTreeRegressor(max_depth=10, random_state=42) tree.fit(X_train, y_train) pred = tree.predict(X_test) results.append({ 'Method': 'Decision tree', 'R²': metrics.r2_score(y_test, pred), 'RMSE': np.sqrt(metrics.mean_squared_error(y_test, pred)) }) # Random forest rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42) rf.fit(X_train, y_train) pred = rf.predict(X_test) results.append({ 'Method': 'Random forest', 'R²': metrics.r2_score(y_test, pred), 'RMSE': np.sqrt(metrics.mean_squared_error(y_test, pred)) }) print(pd.DataFrame(results).to_string(index=False)) . ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#comparing-all-our-methods",
    
    "relUrl": "/python_3_regression.html#comparing-all-our-methods"
  },"45": {
    "doc": "3. REGRESSION",
    "title": "What’s Driving the Patterns? Feature Importance",
    "content": "One of the nicest things about Random Forests is that they tell you which variables matter most for predictions: . importance = pd.DataFrame({ 'Variable': X.columns, 'Importance': rf.feature_importances_ }).sort_values('Importance', ascending=False) print(\"\\nVariable importance:\") print(importance.to_string(index=False)) # Plot it fig = px.bar(importance, x='Importance', y='Variable', orientation='h', title='What drives penguin body mass?') fig.update_layout(template='simple_white', yaxis={'categoryorder': 'total ascending'}) fig.show() . How Is Feature Importance Calculated? . The default importance measure (called “mean decrease in impurity” or “Gini importance”) is based on how much each feature contributes to reducing prediction error across all trees: . | Every time a feature is used to make a split, it reduces the impurity (variance) somewhat | Sum up these reductions across all splits and all trees | Normalize so importances sum to 1 | . For the penguin data, you’ll probably find that sex and flipper length are the most important predictors - which makes biological sense! . A Caution About Feature Importance . This default importance measure has known biases: . | It favors features with many unique values (continuous &gt; categorical) | It favors features that are correlated with other features | It doesn’t tell you about the direction of the effect | . For more reliable importance estimates, consider permutation importance: randomly shuffle one feature and see how much performance drops. If shuffling a feature hurts predictions a lot, that feature was important. from sklearn.inspection import permutation_importance perm_importance = permutation_importance(rf, X_test, y_test, n_repeats=10, random_state=42) perm_imp_df = pd.DataFrame({ 'Variable': X.columns, 'Importance': perm_importance.importances_mean }).sort_values('Importance', ascending=False) print(\"\\nPermutation importance:\") print(perm_imp_df.to_string(index=False)) . ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#whats-driving-the-patterns-feature-importance",
    
    "relUrl": "/python_3_regression.html#whats-driving-the-patterns-feature-importance"
  },"46": {
    "doc": "3. REGRESSION",
    "title": "When to Use What?",
    "content": "| Method | Use When | Advantages | Disadvantages | . | Simple Regression | One predictor, linear relationship, need interpretability | Simple, coefficients are meaningful | Can’t handle multiple predictors or non-linearity | . | Multiple Regression | Multiple predictors, linear relationships, need to understand effects | Coefficients are interpretable, can control for confounders | Assumes linearity and additivity | . | Decision Tree | Need interpretable rules, non-linear relationships | Easy to explain, handles non-linearity | Prone to overfitting, unstable | . | Random Forest | Prediction is main goal, complex non-linear relationships | Excellent predictive performance, robust | Less interpretable, slower to train | . ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#when-to-use-what",
    
    "relUrl": "/python_3_regression.html#when-to-use-what"
  },"47": {
    "doc": "3. REGRESSION",
    "title": "5. Gap-filling in Time Series",
    "content": "Now let’s apply what we’ve learned to a practical problem: dealing with missing data in ecological time series. ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#5-gap-filling-in-time-series",
    
    "relUrl": "/python_3_regression.html#5-gap-filling-in-time-series"
  },"48": {
    "doc": "3. REGRESSION",
    "title": "The Problem",
    "content": "If you’ve worked with field data, you know this frustration. Your sensor died for a week. The battery ran out during the coldest part of winter. Someone accidentally unplugged the datalogger. Missing data is annoying because: . | You can’t calculate annual totals or means | It messes up time series analyses | Some statistical methods can’t handle NaN values | . ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#the-problem",
    
    "relUrl": "/python_3_regression.html#the-problem"
  },"49": {
    "doc": "3. REGRESSION",
    "title": "Loading Messy Data",
    "content": "We will use some data I have prepared in a way that you might find it in an online data portal. Download the file here To test some things we will work with the air temperature column “tair_2m_mean” here. There are several issues when we have a missing-data-placeholder like that. Try two things: Real data often has placeholder values instead of proper missing data markers. Let’s see an example: . import pandas as pd import numpy as np # Load meteorological data df = pd.read_parquet('./dwd_ahaus_1996_2023_missing_placeholders.parquet') df[\"data_time\"] = pd.to_datetime(df[\"data_time\"]) print(f\"Temperature range: {df['tair_2m_mean'].min():.1f} to {df['tair_2m_mean'].max():.1f}\") . If you see -999.99 as the minimum, that’s a placeholder for missing data - not an actual temperature! We need to fix this: . # Replace placeholder with NaN df.loc[df[\"tair_2m_mean\"] == -999.99, \"tair_2m_mean\"] = np.NaN # Now check print(f\"Missing values: {df['tair_2m_mean'].isna().sum()}\") . ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#loading-messy-data",
    
    "relUrl": "/python_3_regression.html#loading-messy-data"
  },"50": {
    "doc": "3. REGRESSION",
    "title": "Method 1: Linear Interpolation",
    "content": "The simplest approach - just draw a straight line between known values: . # Pandas makes this easy df['temp_interp'] = df['tair_2m_mean'].interpolate(method='linear') . This works fine for short gaps. If temperature was 10°C at noon and 14°C at 2pm, it’s reasonable to guess 12°C at 1pm. But it fails badly for longer gaps. It can’t capture the daily temperature cycle - if you have a 24-hour gap, linear interpolation will give you a flat line right through where the daily max and min should be. ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#method-1-linear-interpolation",
    
    "relUrl": "/python_3_regression.html#method-1-linear-interpolation"
  },"51": {
    "doc": "3. REGRESSION",
    "title": "Method 2: Regression-Based Gap Filling",
    "content": "If we have other variables that were measured continuously, we can use them to estimate the missing temperatures: . from sklearn.linear_model import LinearRegression # Solar radiation and humidity are often available when temperature fails # (different sensors) predictors = ['SWIN', 'rH'] # Get data where everything is present (for training) df_complete = df[['data_time', 'SWIN', 'rH', 'tair_2m_mean']].dropna() X = df_complete[predictors] y = df_complete['tair_2m_mean'] # Fit model model = LinearRegression() model.fit(X, y) print(f\"Gap-filling model R²: {model.score(X, y):.3f}\") . Then we can predict temperature wherever we have radiation and humidity data: . # Find rows where temp is missing but predictors exist mask = df['tair_2m_mean'].isna() &amp; df['SWIN'].notna() &amp; df['rH'].notna() df.loc[mask, 'temp_regression'] = model.predict(df.loc[mask, predictors]) . ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#method-2-regression-based-gap-filling",
    
    "relUrl": "/python_3_regression.html#method-2-regression-based-gap-filling"
  },"52": {
    "doc": "3. REGRESSION",
    "title": "Method 3: Random Forest Gap Filling",
    "content": "For better accuracy, especially with complex patterns, Random Forest often wins: . from sklearn.ensemble import RandomForestRegressor # Use more predictors all_predictors = ['SWIN', 'rH', 'pressure_air', 'wind_speed', 'precipitation'] df_complete = df[all_predictors + ['tair_2m_mean']].dropna() X = df_complete[all_predictors] y = df_complete['tair_2m_mean'] rf = RandomForestRegressor(n_estimators=100, max_depth=15, random_state=42) rf.fit(X, y) print(f\"Random Forest R²: {rf.score(X, y):.3f}\") # Check which predictors matter most for estimating temperature for name, imp in sorted(zip(all_predictors, rf.feature_importances_), key=lambda x: x[1], reverse=True): print(f\" {name}: {imp:.3f}\") . ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#method-3-random-forest-gap-filling",
    
    "relUrl": "/python_3_regression.html#method-3-random-forest-gap-filling"
  },"53": {
    "doc": "3. REGRESSION",
    "title": "Which Method When?",
    "content": "After working with a lot of gap-filled data, here’s what I’ve found: . Short gaps (a few hours): Linear interpolation is usually fine. Temperature doesn’t change that fast. Medium gaps (a day or two): Regression with environmental predictors. This captures the daily cycle if you have radiation data. Long gaps (weeks+): Random Forest or similar, but honestly… consider whether you should be filling such long gaps at all. Sometimes it’s better to acknowledge the data is missing. General advice: . | Always validate your gap-filling on data where you know the truth | Flag gap-filled values in your final dataset | Report the uncertainty or error in your gap-filled values | Don’t over-fill - sometimes missing data should stay missing | . Try It Yourself . Compare linear interpolation vs. Random Forest for filling a 24-hour gap in temperature data. Which method captures the daily cycle better? . Solution! # The key insight is that linear interpolation can't capture # diurnal patterns, while Random Forest (using radiation as # a predictor) can. # For a 24-hour gap: # - Linear interpolation draws a flat line # - Random Forest predicts warm during day, cool at night # (because it learned that high radiation = high temp) # In my experience, Random Forest reduces RMSE by 30-50% # compared to linear interpolation for day-long gaps. # But for gaps under 3-6 hours, the methods are often similar # because temperature hasn't changed much anyway. ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#which-method-when",
    
    "relUrl": "/python_3_regression.html#which-method-when"
  },"54": {
    "doc": "3. REGRESSION",
    "title": "Wrapping Up",
    "content": "We’ve covered a lot of ground here. Let me leave you with the key takeaways: . Simple regression is your starting point. It’s easy to understand, easy to explain, and often good enough for straightforward questions. Multiple regression lets you account for multiple drivers at once. The coefficients tell you the effect of each variable while controlling for the others. Random Forests can capture complex patterns that regression misses. They’re particularly good when you don’t know the shape of the relationships in advance. For gap-filling, match your method to your gap length. Simple interpolation for short gaps, model-based methods for longer ones. Most importantly: always plot your data, check your assumptions, and validate on independent test data. No amount of fancy statistics can fix bad data or inappropriate models. Good luck with your analyses! . ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#wrapping-up",
    
    "relUrl": "/python_3_regression.html#wrapping-up"
  },"55": {
    "doc": "3. REGRESSION",
    "title": "Where to Go From Here",
    "content": "If you want to dig deeper: . | Generalized Additive Models (GAMs) let you fit smooth curves instead of straight lines | Mixed-effects models handle hierarchical data (e.g., measurements nested within sites) | Gradient Boosting (XGBoost) often outperforms Random Forests for prediction | Time series methods (ARIMA, etc.) are specifically designed for temporal data | . But honestly, you can get surprisingly far with just regression and Random Forests. Master these first before moving on to fancier tools. ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html#where-to-go-from-here",
    
    "relUrl": "/python_3_regression.html#where-to-go-from-here"
  },"56": {
    "doc": "3. REGRESSION",
    "title": "3. REGRESSION",
    "content": " ",
    "url": "/New_BAI_DataAnalysis/python_3_regression.html",
    
    "relUrl": "/python_3_regression.html"
  },"57": {
    "doc": "4. FLUX CALCULATION",
    "title": "Flux Calculation",
    "content": "In this tutorial, we’re going to analyze the data you collected on your field trip to the Lüner forest! Your instruments measured raw gas concentrations, but as ecologists, we need to turn that into gas fluxes. Why? Because fluxes represent a rate—the speed at which gases are being exchanged. With CO₂ fluxes, we can estimate crucial metrics like ecosystem respiration (RECO) and net ecosystem exchange (NEE). With fluxes of a potent greenhouse gas like Nitrous Oxide (N₂O), we can understand a key part of the nitrogen cycle. This guide will walk you through the entire process: from cleaning the raw concentration data, to calculating meaningful fluxes, and finally to comparing the results between different land cover types. Notice: In the following sections, we will start using new functions and libraries that we haven’t introduced yet. Don’t worry or feel overwhelmed! This is a normal part of learning to code. For each new tool we use, I will: Briefly explain what it is and why we are using it. Provide a link to its official documentation if you’re curious and want to learn more. Think of it as adding new tools to your data analysis toolbox. We’ll introduce them one at a time, right when we need them. Table of Contents . | Read in and merge data files | Visualizing and cleaning the data | Calculating flux for a single measurement | Automating gas flux calculation | . ",
    "url": "/New_BAI_DataAnalysis/python_4_flux_calculation.html#flux-calculation",
    
    "relUrl": "/python_4_flux_calculation.html#flux-calculation"
  },"58": {
    "doc": "4. FLUX CALCULATION",
    "title": "1.Read in and merge data files",
    "content": "Different from the simple CSV files we might have worked with before, the raw data from the gas analyzer is more complex. When you open the file, you’ll see it contains two parts: A metadata header: This block at the top contains useful information about the measurement (like timezone, device model, etc.), but we don’t need it for our flux calculations. The data block: This is the core data we need, with columns for date, time, and gas concentrations. Our first challenge is to programmatically read only the data block and ignore the metadata. To do this, we’ll need the pandas library for creating our DataFrame and the io library, we need to import them. import pandas as pd import io . Our strategy will be to read the file line-by-line, find the start of the data, and then pass only those lines to pandas. 1.1 Reading and Parsing the File . First, we read the entire file into a single string, and then split that string into a list of individual lines. This gives us the flexibility to find our data “landmarks.” . # Read in raw data as a string with open(\"./BAI_StudyProject_LuentenerWald/raw_data/TG20-01072-2025-08-15T110000.data.txt\") as f: file_content = f.read() # Split the string into a list of lines. # '\\n' is the special character for a newline. lines = file_content.strip().split('\\n') . Next, we need to find the exact line that contains our column headers. Looking at the file, we know this line always starts with the word DATAH. We can write a short command to find the index of that line. # This code searches through our list 'lines' and gets the index of the first line that starts with 'DATAH' header_index = next(i for i, line in enumerate(lines) if line.startswith('DATAH')) # The actual data starts 2 lines after the header line (to skip the \"DATAU\" units line) data_start_index = header_index + 2 # Now we can grab the headers themselves from that line. The values are separated by tabs ('\\t'). headers = lines[header_index].split('\\t') . 1.2 Using io.StringIO to Read Our Cleaned Data . The pd.read_csv() function is built to read from a file. We don’t have a clean file; we have a list of Python strings (lines) that we’ve already processed. So, how do we make pandas read from our list? We use io.StringIO to trick pandas. It takes our cleaned-up data lines and presents them to pandas as if they were a file stored in the computer’s memory. Info: The Python io module helps us manage data streams. io.StringIO specifically allows us to treat a regular text string as a &gt;file. This is incredibly useful when you need to pass text data to a function that expects a file, just like we’re doing &gt;with pd.read_csv(). # Join our data lines back into a single string, separated by newlines data_string = '\\n'.join(lines[data_start_index:]) # Read the data string into a DataFrame df_raw = pd.read_csv( io.StringIO(data_string), # Treat our string as a file sep='\\t', # Tell pandas the data is separated by tabs header=None, # We are providing the headers ourselves, so there isn't one in the data names=headers, # Use the 'headers' list we extracted earlier na_values='nan' # Recognize 'nan' strings as missing values ) . 1.3 Data Formatting . The last step is to tidy up the DataFrame. We will: Remove the useless DATAH column. Combine the separate DATE and TIME columns into a single Timestamp object. This is crucial for time-series analysis. Set this new Timestamp as the DataFrame’s index, which makes plotting and selecting data by time much easier. # Drop the first column which is just the 'DATAH' label if 'DATAH' in df_raw.columns: df_raw = df_raw.drop(columns=['DATAH']) # Combine 'DATE' and 'TIME' into a proper Timestamp and set it as the index if 'DATE' in df_raw.columns and 'TIME' in df_raw.columns: df_raw['Timestamp'] = pd.to_datetime(df_raw['DATE'] + ' ' + df_raw['TIME']) df_raw = df_raw.drop(columns=['DATE', 'TIME']) df_raw = df_raw.set_index('Timestamp') print(\"Data loaded and formatted successfully!\") df_raw.head() . Great! Now, we have successfully read in and formatted our raw data. However, think about our field campaigns. We went out several times and generate a new data file for each trip. If we wanted to analyze all of them, we would have to copy and paste our loading code multiple times. To avoid repetition and make our code cleaner and more reliable, it’s a best practice to wrap a reusable process into a function. Let’s turn our loading and cleaning steps into a function called load_raw_data. Exercise . Try to write this function yourself based on the code snippets we created for data loading! Tip: The function will need to accept one argument: the filepath of the file you want to open. Solution! Note: how it’s the exact same logic as before, just defined within a def block. def load_raw_data(filepath: str) -&gt; pd.DataFrame: \"\"\" Loads raw data from a text file, remove metadata, and returns a DataFrame. Parameters: - filepath (str): The path to the input data file. Returns: - pd.DataFrame: A cleaned DataFrame with a DatetimeIndex. \"\"\" with open(filepath) as f: file_content = f.read() lines = file_content.strip().split('\\n') header_index = next(i for i, line in enumerate(lines) if line.startswith('DATAH')) data_start_index = header_index + 2 headers = lines[header_index].split('\\t') df_raw = pd.read_csv( io.StringIO('\\n'.join(lines[data_start_index:])), sep='\\t', header=None, names=headers, na_values='nan' ) if 'DATAH' in df_raw.columns: df_raw = df_raw.drop(columns=['DATAH']) if 'DATE' in df_raw.columns and 'TIME' in df_raw.columns: df_raw['Timestamp'] = pd.to_datetime(df_raw['DATE'] + ' ' + df_raw['TIME']) df_raw = df_raw.drop(columns=['DATE', 'TIME']) df_raw = df_raw.set_index('Timestamp') print(\"Raw data loaded and cleaned successfully.\") return df_raw . Now that we have our powerful load_raw_data function, we can easily handle data from multiple field trips. Instead of copying code, we can simply call our function in a loop. First, we create a list of all the file paths we want to load. Then, we can loop through this list, call our function for each path, and store the resulting DataFrames in a new list. # First, let's list all the files we want to load. # Make sure the file paths are complete and correct. base_path = \"./BAI_StudyProject_LuentenerWald/raw_data/\" file_names = [ 'TG20-01072-2025-08-15T110000.data.txt', 'TG20-01072-2025-08-16T110000.data.txt' # A hypothetical second file ] # Create the full file paths full_file_paths = [base_path + name for name in file_names] # Create an empty list to hold the loaded DataFrames raw_data_list = [] # Loop through each path, load the data, and append it to our list for path in full_file_paths: df = load_raw_data(path) raw_data_list.append(df) print(f\"\\nSuccessfully loaded {len(raw_data_list)} data files.\") . The loop above is clear and correct. However, a more concise way to write this in Python is with a list comprehension. It achieves the exact same result in a single, readable line: . raw_data_list = [load_raw_data(path) for path in full_file_paths] . For our flux calculations to be accurate, we need more than just gas concentrations. The Ideal Gas Law, which is the basis of the calculation, requires the ambient air temperature and air pressure at the time of each measurement. We will use the same workflow as before: load each file and then combine them. Exercise . You have two Excel files containing air temperature and two files for air pressure. Create lists of the file paths for the temperature and pressure data. Load each Excel file into a pandas DataFrame. Try using a list comprehension as we learned before! . Click here for the solution! # We assume the base path is the same as before base_path = \"./BAI_StudyProject_LuentenerWald/raw_data/\" # --- Load Air Temperature Data --- file_names_Ta = [ 'air_temperature_2025-08-15.xlsx', 'air_temperature_2025-08-16.xlsx' ] full_file_paths_Ta = [base_path + name for name in file_names_Ta] ta_data_list = [pd.read_excel(path) for path in full_file_paths_Ta] print(f\"Successfully loaded {len(ta_data_list)} air temperature files.\") # --- Load Air Pressure Data --- file_names_Pa = [ 'air_pressure_2025-08-15.xlsx', 'air_pressure_2025-08-16.xlsx' ] full_file_paths_Pa = [base_path + name for name in file_names_Pa] pa_data_list = [pd.read_excel(path) for path in full_file_paths_Pa] print(f\"Successfully loaded {len(pa_data_list)} air pressure files.\") . 1.4 Concatenating and Merging All Data . Now that we have all our data loaded, we need to combine it into one master DataFrame for analysis. This involves two steps: Concatenate: Stacking the files of the same type together (e.g., all gas files into one, all temperature files into one). Merge: Joining the different datasets (gas, temperature, and pressure) together based on their common timestamp. Concatenating the Datasets . First, let’s use pd.concat() to combine the lists of DataFrames we created. After combining, we must format the Timestamp column and set it as the index, just as we did before. # --- Concatenate and Clean Gas Data --- df_gas = pd.concat(raw_data_list) # Assumes raw_data_list is from the previous step # --- Concatenate and Clean Temperature Data --- df_Ta = pd.concat(ta_data_list) df_Ta['Timestamp'] = pd.to_datetime(df_Ta['Timestamp']) df_Ta = df_Ta.set_index('Timestamp') # --- Concatenate and Clean Pressure Data --- df_Pa = pd.concat(pa_data_list) df_Pa['Timestamp'] = pd.to_datetime(df_Pa['Timestamp']) df_Pa = df_Pa.set_index('Timestamp') print(\"--- Gas DataFrame Info ---\") df_gas.info() print(\"\\n--- Temperature DataFrame Info ---\") df_Ta.info() print(\"\\n--- Pressure DataFrame Info ---\") df_Pa.info() . Merging Gas and Auxiliary Data . Finally, we need to combine our df_gas, df_Ta, and df_Pa DataFrames. We want to add the temperature and pressure columns to the gas data, matching them by the nearest timestamp. The gas analyzer records data every second, while the weather station might only record every minute. A simple merge would leave many empty rows. The perfect tool for this is pd.merge_asof(). It performs a “nearest-neighbor” merge, which is ideal for combining time-series data with different frequencies. # First, merge the two auxiliary datasets together df_aux = pd.merge_asof(left=df_Ta, right=df_Pa, on='Timestamp', direction='nearest') # Now, merge the gas data with the combined auxiliary data. # We use direction='backward' to find the most recent weather data for each gas measurement. df_raw = pd.merge_asof( left=df_gas, right=df_aux, on='Timestamp', direction='backward' ) print(\"\\n--- Final Merged DataFrame ---\") display(df_raw.head()) df_raw.info() . Brilliant! You now have a single, clean DataFrame called df_final that contains everything you need: the high-frequency gas concentrations and the corresponding temperature and pressure for each measurement point. We are now fully prepared to move on to the flux calculation. ",
    "url": "/New_BAI_DataAnalysis/python_4_flux_calculation.html#1read-in-and-merge-data-files",
    
    "relUrl": "/python_4_flux_calculation.html#1read-in-and-merge-data-files"
  },"59": {
    "doc": "4. FLUX CALCULATION",
    "title": "2. Visualizing and cleaning the data",
    "content": "Now that we have a single, merged DataFrame, our next step is to inspect the data quality. Raw sensor data from the field is almost never perfect. Visualizing it is the best way to diagnose issues like noise, drift, or outliers before we attempt any calculations. For this, we’ll use plotly, a powerful library for creating interactive plots. 2.1 Creating a Reusable Plotting Function with Plotly . Just as we did with data loading, we’ll be plotting our time-series data multiple times. To make this efficient and keep our plots looking consistent, let’s create a dedicated function. This function will take a DataFrame and some plot details as input and generate an interactive plot. Exercise . The plooting function is partly providing in the following, now finish the function! . def plot_time_series(df, y_column, title, mode='lines'): \"\"\" Generates an interactive time-series plot using Plotly. Parameters: - df (pd.DataFrame): DataFrame with a DatetimeIndex. - y_column (str): The name of the column to plot on the y-axis. - title (str): The title for the plot. - mode (str): Plotly mode ('lines', 'markers', or 'lines+markers'). \"\"\" fig = go.Figure() fig.add_trace(...) # Update layout for a clean look fig.update_layout( ... ) fig.show() . Here is the solution! import plotly.graph_objects as go import plotly.io as pio # This setting forces Plotly to open plots in your default web browser, # which can be more stable in some environments. pio.renderers.default = \"browser\" def plot_time_series(df, y_column, title, mode='lines'): \"\"\" Generates an interactive time-series plot using Plotly. This function will automatically try to set a 'Timestamp' column as the index if the existing index is not a datetime type. Parameters: - df (pd.DataFrame): DataFrame to plot. - y_column (str): The name of the column to plot on the y-axis. - title (str): The title for the plot. - mode (str): Plotly mode ('lines', 'markers', or 'lines+markers'). \"\"\" # --- Input Validation and Auto-Correction --- # It's good practice to work on a copy inside a function to avoid # changing the user's original DataFrame unexpectedly. df_plot = df.copy() if not pd.api.types.is_datetime64_any_dtype(df_plot.index): print(\"Note: The DataFrame index is not a DatetimeIndex.\") # Attempt to fix the issue by finding a 'Timestamp' column if 'Timestamp' in df_plot.columns: print(\"--&gt; Found a 'Timestamp' column. Attempting to set it as the index.\") df_plot['Timestamp'] = pd.to_datetime(df_plot['Timestamp']) # CRITICAL: You must re-assign the variable to save the change. df_plot = df_plot.set_index('Timestamp') else: # If we can't fix it automatically, then we raise an error. raise TypeError( \"The DataFrame index is not a DatetimeIndex and a 'Timestamp' column was not found. \" \"Please set a DatetimeIndex before plotting.\" ) # --- Plotting --- # By this point, df_plot is guaranteed to have a valid DatetimeIndex. fig = go.Figure() fig.add_trace(go.Scatter( x=df_plot.index, y=df_plot[y_column], mode=mode, name=y_column )) # Update layout for a clean, professional look fig.update_layout( title=title, xaxis_title='Time', yaxis_title=f'{y_column} Concentration (ppb)', template='plotly_white', title_font=dict(size=24), xaxis=dict(tickfont=dict(size=14), title_font=dict(size=16)), yaxis=dict(tickfont=dict(size=14), title_font=dict(size=16)) ) fig.show() . 2.2 Visualizing the Raw Gas Data . Now, let’s use our new function to look at the raw N₂O data from our combined file. You can zoom and pan on the plot to inspect the noisy areas. # Call our function to plot the raw 'N2O' column plot_time_series(df_final, y_column='N2O', title='Raw N2O Concentration Over Time') . As you can see from the plot, the raw data is very noisy. There are several negative values and some extremely large spikes. These are physically impossible and are likely due to sensor errors or electrical interference. We cannot calculate meaningful fluxes from this data without cleaning it first. 2.3 Filtering with a Quantile Filter . To remove these outliers, we’ll use a simple but effective quantile filter. This method is robust because the extreme values we want to remove have very little influence on the calculation of percentiles. We will calculate the 10th and 90th percentiles of the N₂O concentration and discard any data points that fall outside this range. # Calculate the 10th and 90th percentiles p_10 = df_final.N2O.quantile(0.10) p_90 = df_final.N2O.quantile(0.90) print(f\"Filtering data to keep N2O concentrations between {p_10:.2f} and {p_90:.2f} ppb.\") # Apply the filter to create a new, clean DataFrame # .copy() is used here to avoid a SettingWithCopyWarning from pandas df_filtered = df_final[(df_final.N2O &gt;= p_10) &amp; (df_final.N2O &lt;= p_90)].copy() # Visualize the filtered data using our function again, this time using 'markers' plot_time_series(df_filtered, y_column='N2O', title='Filtered N2O Concentration Over Time', mode='markers') . This looks much better! The noise is gone, and a clear, meaningful pattern has emerged. 2.4 Understanding the Data Pattern . The filtered data shows a repeating pattern which is the signature of the static chamber method: Baseline (Ambient Air): The long, relatively flat periods show the baseline N₂O concentration in the ambient air. Concentration Increase (Chamber Closed): The sections where the concentration rises steadily and linearly are the actual measurements. This occurs when the chamber is placed over the soil, trapping the gases being emitted. The rate of this increase is what we will use to calculate the flux. Sudden Drop (Chamber Opened): The sharp vertical drops occur when a measurement is finished, and the chamber is lifted from the ground, exposing the sensor to ambient air again. Leveling Off: If a chamber is left on the ground for too long, the gas concentration inside can build up, altering the pressure gradient between the soil and the chamber air. This can cause the rate of increase to slow down and “level off.” For this reason, it’s crucial to use only the initial, linear part of the increase for our flux calculation. ",
    "url": "/New_BAI_DataAnalysis/python_4_flux_calculation.html#2-visualizing-and-cleaning-the-data",
    
    "relUrl": "/python_4_flux_calculation.html#2-visualizing-and-cleaning-the-data"
  },"60": {
    "doc": "4. FLUX CALCULATION",
    "title": "3. Calculating flux for a single measurement",
    "content": "After loading and filtering our raw data and getting an overview of the patterns, it’s time to calculate the fluxes. Excited? In this section, we will focus on the data for a single measurement period to understand the process in detail. We’ll break it down into a few key steps: . | Review the flux calculation formula to see what components we need. | Define the metadata (chamber dimensions, etc.) for our specific plot. | Isolate the data for a specific time window and visualize it. | Perform a linear regression on the concentration data to get the rate of change. | Combine all the pieces to calculate the final flux. | . 3.1 The Flux Calculation Formula . First, let’s have a look on the fomula of flux calculation. ​ \\(\\text{Flux Rate (molar)} = \\frac{\\frac{\\Delta C}{t} \\cdot V \\cdot p}{R \\cdot (T_{c} + 273.15) \\cdot A}\\) ​ . Where: . ΔC/t: The rate of change of the gas concentration in ppm/s (this will be the slope from our regression). V: The total volume of the chamber headspace (m³). p: The air pressure in Pascals (Pa) during measurement. R: The ideal gas constant (8.314 J K⁻¹ mol⁻¹). T_c: The air temperature in Celsius (°C). A: The surface area covered by the chamber (m²). To understand this fomula, we need to figure out the meaning of ‘flux’. In the context of climate change, greenhouse gas flux specifically refers to the exchange of greenhouse gases (GHGs) like carbon dioxide (CO₂), methane (CH₄), and nitrous oxide (N₂O) between different parts of the Earth system (https://climate.sustainability-directory.com/term/greenhouse-gas-fluxes/#:~:text=In%20the%20context%20of%20climate,parts%20of%20the%20Earth%20system). Under the context of this analysis, ‘flux’ means gases exchange between soil and our measurement chamber. You might ask, “Doesn’t the rate of concentration change, ΔC/t (in ppb/s), already represent this flux?” Actually, ΔC/t is the raw evidence of a flux, but it is not a standardized, comparable measurement. It only describes what’s happening inside our specific chamber, under the specific conditions of that one measurement. We are not able to compare flux by simply comparing change rate of gas concentration. Under different temperature and pressure, gas molar density vary, the amount of gas molecular can be different even the gas volume is the same. Therefore, we need to utilize Gas Law (PV = nRT) to calculate the amount of molecular. Besides, a chamber covering a large area of soil will naturally capture more gas than one covering a small area. To make the measurement independent of our chamber’s specifications, we must divide by the soil Area (A) it covers. By applying the full formula, We convert our raw observation (ΔC/t) into a robust, standardized unit: micromoles per square meter per second (µmol m⁻² s⁻¹). To better understand the above fomula, it can be arranged into the following: . \\[\\text{Flux Rate (molar)} = ( \\frac{\\Delta C}{t} \\right) \\cdot ( \\frac{p \\cdot V} {R \\cdot (T_C + 273.15)} \\right) \\cdot ( \\frac{1}{A} \\right)\\] Now, it is clear that the fomula only contains three components: Flux = slope * gas_mole * A⁻¹ . Okay, lets create a function of flux calculation based on the fomula for later use. Exercise . The function calculate_flux is provided below but is not complete. It is your task to finish the function based on the formula. # Define key physical constants R = 8.314 # Ideal gas constant (J K⁻¹ mol⁻¹) def calculate_flux(slope_ppb_s, temp_k, pressure_pa, volume, area): \"\"\" Calculates N2O flux. Parameters: - slope_ppb_s (float): Rate of change in ppb/s. - temperature (float): Temperature, assumed to be in Celsius or Kelvin. - pressure (float): Pressure, assumed to be in Pascals (Pa) or hectopascals (hPa). - volume (float): Chamber volume, assumed to be in cubic meters (m³) or Liters (L). - area (float): Chamber area, assumed to be in square meters (m²) or square cm (cm²). \"\"\" # Convert slope from ppb/s to ppm/s for the formula ppm_per_second = ... # Calculate molar density of air (n/V = P/RT) in mol/m³ gas_model = ... # Calculate the flux in µmol m⁻² s⁻¹ # The 1e6 converts from mol to µmol flux = ... return flux . Solution! Here is the completed function: . # Define key physical constants R = 8.314 # Ideal gas constant (J K⁻¹ mol⁻¹) def calculate_flux(slope_ppb_s, temp_k, pressure_pa, volume, area): \"\"\" Calculates N2O flux. Parameters: - slope_ppb_s (float): Rate of change in ppb/s. - temperature (float): Temperature, assumed to be in Celsius or Kelvin. - pressure (float): Pressure, assumed to be in Pascals (Pa) or hectopascals (hPa). - volume (float): Chamber volume, assumed to be in cubic meters (m³) or Liters (L). - area (float): Chamber area, assumed to be in square meters (m²) or square cm (cm²). \"\"\" # Convert slope from ppb/s to ppm/s for the formula ppm_per_second = slope_ppb_s / 1000.0 # Calculate molar density of air (n/V = P/RT) in mol/m³ gas_mole = (pressure_pa * volume)/ (R * temp_k) # Calculate the flux in µmol m⁻² s⁻¹ # The 1e6 converts from mol to µmol flux = ppm_per_second * gas_mole / area * 1e6 return flux . 3.2 Isolating and Visualizing the Measurement Data . Let’s use an example time period of measurement: 2025-08-15 12:04:00 to 2025-08-15 12:10:00. We’ll slice our df_filtered DataFrame to get only the data within this window and then plot it to get a closer look. # Define the start and end times for our measurement window start_time = '2025-08-15 12:04:00' end_time = '2025-08-15 12:09:30' # Select the data for this specific time window measurement_data = df_filtered[(df_filtered.index &gt;= start_time) &amp; (df_filtered.index &lt; end_time)] # Use our plotting function to visualize this specific period plot_time_series( measurement_data, y_column='N2O', title=f'N2O Concentration for Plot {plot_metadata[\"plot_id\"]}', mode='markers' ) . As you can see from the plot, the data in our 5-minute window 2025-08-15 12:04:00 - 2025-08-15 12:09:30 contains more than just the measurement itself. We can identify three distinct phases: . Pre-measurement Baseline: A flat period at the beginning. This is when the sensor was measuring ambient air before the chamber was placed on the collar. The Measurement (Linear Increase): This is the part we want. The chamber is sealed, and N₂O from the soil is accumulating, causing a steady, linear increase in concentration. Post-measurement Drop: The sharp, sudden drop at the end. This occurred when the chamber was lifted, and the sensor was exposed to ambient air again. Our flux calculation relies on the slope (ΔC/t) from a linear regression. If we include the flat baseline or the sharp drop in our regression, the line of best fit will not represent the true rate of accumulation, leading to a highly inaccurate flux calculation. Therefore, To get an accurate flux, visual inspection is necessary to include only the linear increase phase. Zoom in on the interactive Plotly graph. We can see that the clean, linear increase happens approximately between 12:05:30 and 12:09:00. Exercise . Try to slice the dataframe based on your refined time window, and plot it to see our refined result. Solution! Here is the completed function: . # Define the refined, visually inspected time window start_mea = '2025-08-15 12:05:30' end_mea = '2025-08-15 12:09:00' # Create a new DataFrame with data only from this refined window # We use .copy() to create a completely new object for the regression measurement_data = df_filtered[df_filtered.index &gt; start_mea &amp; df_filtered.index &lt; end_mea].copy() # Visualize the refined data to confirm our selection plot_time_series( regression_data, y_column='N2O', title=f'Refined Regression Window for Plot {plot_metadata[\"plot_id\"]}', mode='markers' ) . Great! This plot shows the clear, linear increase in N₂O concentration after the chamber was placed on the collar. This is the exact data we need for our regression. 3.3 Linear Regression to derive the rate of gas concentration change . Now, as we talked before, we will fit a linear line to these data points. The slope of that line is the dC/dt (rate of change) that we need for our flux formula. As we expect the unit of our regression slope to be ppb/s our x-axis needs to be seconds elapsed (it means the seconds passed compared to the start of the measurement) instead of a timestamp. So, our first step is to create a new column, elapsed_seconds. from scipy import stats measurement_data = measurement_data.copy() # Create an 'elapsed_seconds' column for the regression # First, we get the start time of the measurement start_timestamp = measurement_data.index.min() # Then we get the time difference for each time point and the start of measurement, and use function total_seconds convert this time difference into seconds measurement_data['elapsed_seconds'] = (measurement_data.index - start_timestamp).total_seconds() . Then, we are going to actually fit the regression using scipy library. R2 represents the strength of the relationship that we detected. In here, we are going to use r2 = 0.7 as a threshold. If R2 of a regression is lower than 0.7, the change of gas concentration as time is not significant enough to be recognize as a flux (no flux is detected from the data), otherwise a gas flux can be deceted from the data. # Perform the linear regression using SciPy slope, intercept, r_value, p_value, std_err = stats.linregress( x=measurement_data['elapsed_seconds'], y=measurement_data['N2O'] ) # The R-squared value tells us how well the line fits the data (a value &gt; 0.7 is good!) r_squared = r_value**2 print(f\"--- Regression Results ---\") print(f\"Slope (dC/dt): {slope:.4f} ppb/s\") print(f\"R-squared: {r_squared:.4f}\") . 3.4 Visualizing the Fit and Final Calculation . It’s always good practice to visualize the regression line against the data to confirm the fit is good. # --- Visualize the regression line --- fig = go.Figure() # Add the raw data points fig.add_trace(go.Scatter(x=measurement_data['elapsed_seconds'], y=measurement_data['N2O'], mode='markers', name='Raw Data')) # Add the fitted regression line fig.add_trace(go.Scatter(x=measurement_data['elapsed_seconds'], y=intercept + slope * measurement_data['elapsed_seconds'], mode='lines', name='Fitted Line', line=dict(color='red'))) fig.update_layout(title=f'Linear Regression for Plot {plot_metadata[\"plot_id\"]} (R²={r_squared:.2f})', xaxis_title='Elapsed Time (s)', yaxis_title='N2O Concentration (ppb)', template='plotly_white') fig.show() . Good! If the regression is well fitted into our data, we are able to calculate the flux now! Before we call the calculate_flux function, there are still some steps to go. First, we need to get the average chamber air temperature and air pressure during the measurement and convert them into desired unit respectively (K for air temperature and Pa for air pressure). The unit conversion is very important, as when we wrote the calculate_flux function, we assumed units of our inputs. The mismatch of units will introduce systematic errors and leading to inaccuracy. # try to get the average air temperature and air temperature value, don't forget the unit conversion. avg_temp_c = avg_pressure_pa = . Solution! # Get the average temperature and pressure during the measurement avg_temp_c = measurement_data['T_air'].mean() + 273.15 # convert from °C to K avg_pressure_pa = measurement_data['P_air'].mean() * 100 # Assuming pressure is in hPa, convert to Pa . Then, we still need the total volume of the chamber headspace (m³) and the surface area covered by the chamber (m²). As they are independent of time and the same for all plots, we can simply define them as constants. # --- Finally, Calculate the Flux! --- VOLUME = 0.126 # AREA = 0.13 # the radias of the collar ring is 0.2m, so the area is 0.2*0.2*PI . Finally, call our calculate_flux function and we can get the result! . # Now try to call the function using all the inputs we have and print out to check the result. flux_N2O = ... print(...) . Solution! # Now we have all the pieces! Let's call our function. flux_N2O = calculate_flux( slope_ppb_s=slope, temp_k=avg_temp_k, pressure_pa=avg_pressure_pa, volume=VOLUME, area=AREA ) print(f\"\\n--- Final Flux Calculation ---\") print(f\"Average Temperature: {avg_temp_c:.2f} °C\") print(f\"Average Pressure: {avg_pressure_pa:.2f} Pa\") print(f\"Calculated N₂O Flux: {flux_N2O:.5f} µmol m⁻² s⁻¹\") . Brilliant! Now you successfuly turn the raw gas concentration data into gas fulx! . Challenge . Our current calculate_flux function works well, but it has a hidden weakness. It assumes the units of the inputs are correct. For example, it blindly assumes the temp_k argument is already in Kelvin. What if a user accidentally passes in a temperature in Celsius? The function would run without an error but produce a wildly incorrect result. Code that relies on such hidden assumptions is sometimes called “hard-coded.” A much better practice is to write more flexible code that can handle different situations or at least warn the user when something is wrong. Task: upgrade the calculate_flux function to be more robust. It should: . | Add Unit Checks: Check the input values to make a reasonable guess about their units. | Perform Automatic Unit Conversion: If it detects a value in a common but incorrect unit (like Celsius for temperature), it should automatically convert it to the required unit (Kelvin). | Raise Errors: If a value is completely outside a plausible range, it should stop and raise an error with a helpful message. | . Tip: You can determine units by checking the physical range of a variable. For example, for a terrestrial field measurement, if a temperature value is between -50 and 50, it’s almost certainly Celsius. If it’s between 223 and 323, it’s likely already in Kelvin. Solution! # Define key physical constants R = 8.314 # Ideal gas constant (J K⁻¹ mol⁻¹) def calculate_flux(slope_ppb_s, temperature, pressure, volume, area): \"\"\" Calculates N2O flux with extensive unit checks and auto-conversion for all inputs. Parameters: - slope_ppb_s (float): Rate of change in ppb/s. - temperature (float): Temperature, assumed to be in Celsius or Kelvin. - pressure (float): Pressure, assumed to be in Pascals (Pa) or hectopascals (hPa). - volume (float): Chamber volume, assumed to be in cubic meters (m³) or Liters (L). - area (float): Chamber area, assumed to be in square meters (m²) or square cm (cm²). \"\"\" # --- 1. Input Validation and Unit Conversion --- # Check Temperature (Celsius vs. Kelvin) if -50 &lt;= temperature &lt;= 50: print(f\"Note: Temperature ({temperature}) detected as Celsius. Converting to Kelvin.\") temp_k = temperature + 273.15 elif 223 &lt;= temperature &lt;= 323: temp_k = temperature else: raise ValueError(f\"Temperature value ({temperature}) is outside a plausible range.\") # Check Pressure (Pascals vs. hPa) if 800 &lt;= pressure &lt;= 1100: print(f\"Note: Pressure ({pressure}) detected as hPa/mbar. Converting to Pascals.\") pressure_pa = pressure * 100 elif 80000 &lt;= pressure &lt;= 110000: pressure_pa = pressure else: raise ValueError(f\"Pressure value ({pressure}) is outside a plausible range.\") # Check Volume (m³ vs. Liters) if 10000 &lt;= volume &lt;= 2000000 print(f\"Note: Volume ({volume}) detected as cm³. Converting to m³.\") volume_m3 = volume / 1e6 elif 10 &lt;= volume &lt;= 2000: # Plausible range for Liters print(f\"Note: Volume ({volume}) detected as Liters. Converting to m³.\") volume_m3 = volume / 1000.0 elif 0.01 &lt;= volume &lt;= 2: # Plausible range for m³ volume_m3 = volume else: raise ValueError(f\"Volume value ({volume}) is outside a plausible range for m³ or Liters.\") # Check Area (m² vs. cm²) if 100 &lt;= area &lt;= 20000: # Plausible range for cm² print(f\"Note: Area ({area}) detected as cm². Converting to m².\") area_m2 = area / 10000.0 elif 1 &lt;= area &lt;= 200: # Plausible range for dm² area_m2 = area / 100.0 print(f\"Note: Area ({area}) detected as dm². Converting to m².\") elif 0.01 &lt;= area &lt;= 2: # Plausible range for m² area_m2 = area else: raise ValueError(f\"Area value ({area}) is outside a plausible range for m², dm² or cm².\") # --- 2. Core Calculation --- v_over_a = volume_m3 / area_m2 ppm_per_second = slope_ppb_s / 1000.0 molar_density = pressure_pa / (R * temp_k) flux = ppm_per_second * molar_density * v_over_a * 1e6 return flux . Info: Python (raise keywords) is used to raise exceptions or errors. The raise keyword raises an error and stops the control flow of the program. It is used to bring up the current exception in an exception handler (an exception handler indicates the error type) so that it can be handled further up the call stack. The basic way to raise an exception is . raise Exception ('...') # In here, Exception is an exception handler (it is actually a function), which indicate a general exception. It takes a string used to reminder # users what errors happen in here and the potential reasons. In the ‘solution’, we used the handler ValueError to indicate the input value is outside a plausible range, and pass a string showing to users to further explain the error. ",
    "url": "/New_BAI_DataAnalysis/python_4_flux_calculation.html#3-calculating-flux-for-a-single-measurement",
    
    "relUrl": "/python_4_flux_calculation.html#3-calculating-flux-for-a-single-measurement"
  },"61": {
    "doc": "4. FLUX CALCULATION",
    "title": "4. Automating gas flux calculation",
    "content": "4.1 Store and structure measurement info . The first and crucial step of automation is to store the key information (metadata) for each measurement in a structured way that a program can loop through. For this, we will use a Python dictionary. The dictionary keys will be our data “columns” (e.g., ‘plot_id’, ‘land_use’), and the values will be lists containing the data for each plot. Now, there is an issue: we take multiple measurements at the same plot, perhaps on different days or at different times. How can we store this information efficiently? We can store the multiple start and end times for a single plot as a single string, with each timestamp separated by a semicolon (;). Of course, the order of the multiple starttime and endtime for a plot should match. By doing this, we can keep the metadata table concise and still tell our program to perform multiple calculations for that plot. # --- Create Metadata --- measurement_info = { 'plot_id': [1, 2], 'land_use': ['forest', 'forest'], 'start_time': ['2025-08-15 12:06:00; 2025-08-15 12:14:00', '2025-08-15 12:13:00'], 'end_time': ['2025-08-15 12:09:00; 2025-08-15 12:17:00', '2025-08-15 12:18:30'], } metadata_df = pd.DataFrame(measurement_info) . 4.2 Automation Calculation . Now we can build a for loop that iterates through each row (Each row contains infomation for a single plot) of our metadata_df. In here, we are going to use ‘iterrow()’ to iterate through metadata_df. ‘iterrow()’ is a method of data frame object, it generates an iterator object of the DataFrame, allowing us to iterate each row in the DataFrame. Each iteration produces an index object and a row object (a Pandas Series object). Inside the loop, we will split start_time and end_time string for each plot using ‘split()’ method and build a inner loop to iterate all measurements for the plot. results = [] # Create a empty list we can use to store all calculated fluxes. for index, row in metadata_df.iterrows(): start_times = row['start_time'].strip().split(';') # Handle potential multiple times end_times = row['end_time'].strip().split(';') # Handle potential multiple times for start_time, end_time in zip(start_times, end_times): start_time = pd.to_datetime(start_time.strip()) end_time = pd.to_datetime(end_time.strip()) measurement_date = f'{start_time.year}-{start_time.month:02d}-{start_time.day:02d}' . Within the inner loop, we will perform the exact same steps we did manually in the last section. However, there is one key difference in the visual inspection step. In the manual section, we looked at the plot and then assigned our refined start and end times into variables. To keep the program continuing without needing to stop and edit the script each time, we will use the built-in input() function. This will pause the script, show us a plot, and allow us to enter our refined time window directly into the terminal before the program continues. ## step 1: Visual inspection ## # Select the data for this specific time window measurement_data = df_filtered[(df_filtered.index &gt;= start_time) &amp; (df_filtered.index &lt; end_time)] # Plot the raw data for visual inspection plot_time_series(measurement_data, y_column='N2O', title=f'N2O Concentration Over Time}', mode='markers') # Mannually selcect the start and end time for regression start_mea = input(\"Enter the start time for regression (YYYY-MM-DD HH:MM:SS): \").strip() end_mea = input(\"Enter the end time for regression (YYYY-MM-DD HH:MM:SS): \").strip() # Use the original start and end time if no input is given if not start_mea: start_mea = start_time if not end_mea: end_mea = end_time start_time = pd.to_datetime(start_mea) end_time = pd.to_datetime(end_mea) measurement_data = measurement_data[(measurement_data.index &gt;= start_time) &amp; (measurement_data.index &lt;= end_time)] ## step 2: Linear regression ## # Ensure there is enough data to perform a regression if len(measurement_data) &lt; 10: print(f\"Skipping plot {row['plot_id']} on {measurement_date} due to insufficient data.\") continue # Create an 'elapsed_seconds' column for the regression measurement_data['elapsed_seconds'] = (measurement_data.index - start_time).total_seconds() # Perform linear regression: N2O concentration vs. time slope, intercept, r_value, p_value = stats.linregress( x=measurement_data['elapsed_seconds'], y=measurement_data['N2O'] ) # --- Quality Control (QC) --- # We only accept measurements with a good linear fit and a positive slope r_squared = r_value**2 # plot the regression line fig, ax = plt.subplots(layout='constrained', figsize=(10, 5)) ax.scatter(measurement_data['elapsed_seconds'], measurement_data['N2O'], label='N2O Concentration (ppb)') ax.plot(measurement_data['elapsed_seconds'], intercept + slope * measurement_data['elapsed_seconds'], 'r', label='Fitted line') ax.set_xlabel('Elapsed Time (s)') ax.set_ylabel('N2O Concentration (ppb)') ax.set_title(f'Linear Regression for Plot {row[\"plot_id\"]} (R²={r_squared:.2f})') plt.legend() plt.show() if r_squared &lt; 0.70 or p_value &gt; 0.05: flux_umol_m2_s = 0 # Set flux to 0 if QC fails qc_pass = False else: qc_pass = True ## step 3: Flux Calculation Formula ## # This formula converts the rate of change in concentration (slope) to a flux rate. # It corrects for ambient pressure and temperature. temp_k = measurement_data['temperature'].mean() + 273.15 # Convert °C to Kelvin pressure_pa = measurement_data['pressure'].mean() * 100 # Convert hPa to Pascals flux_umol_m2_s = calculate_flux(slope, temp_k, pressure_pa, VOLUME, AREA) . At the end of the iteration, we need to save the results of each calculation. Only the flux value is not enough, we also need to save its metadata (e.g., plot_id, ‘land_use’), which are essential for flux analysis and visualization we are going to do later. # Store the results results.append({ 'plot_id': row['plot_id'], 'land_use': row['land_use'], 'measurement_date': measurement_date, 'slope_ppb_s': slope, 'r_squared': r_squared, 'p_value': p_value, 'qc_pass': qc_pass, 'N2O_flux_umol_m2_s': flux_umol_m2_s }) # Convert the results list to a final DataFrame flux_results_df = pd.DataFrame(results) print(\"\\nFlux calculation complete:\") print(flux_results_df) . 4.3 Flux comparison . # --- Visualization --- plt.figure(figsize=(10, 7)) sns.boxplot(data=flux_results_df, x='land_use', y='N2O_flux_umol_m2_s', palette='viridis') sns.stripplot(data=flux_results_df, x='land_use', y='N2O_flux_umol_m2_s', color='black', size=8, jitter=True, alpha=0.7) plt.title('N₂O Flux by Land Use Type', fontsize=16) plt.xlabel('Land Use', fontsize=12) plt.ylabel('N₂O Flux (µmol m⁻² s⁻¹)', fontsize=12) plt.grid(axis='y', linestyle='--', alpha=0.7) plt.show() . ",
    "url": "/New_BAI_DataAnalysis/python_4_flux_calculation.html#4-automating-gas-flux-calculation",
    
    "relUrl": "/python_4_flux_calculation.html#4-automating-gas-flux-calculation"
  },"62": {
    "doc": "4. FLUX CALCULATION",
    "title": "4. FLUX CALCULATION",
    "content": " ",
    "url": "/New_BAI_DataAnalysis/python_4_flux_calculation.html",
    
    "relUrl": "/python_4_flux_calculation.html"
  }
}
